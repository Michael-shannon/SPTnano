{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16405d86",
   "metadata": {},
   "source": [
    "# SPTnano HPC Parallel Processing\n",
    "\n",
    "SPTnano parallel processing `calculate_time_windowed_metrics` across multiple cores.\n",
    "\n",
    "## Usage:\n",
    "1. **Cluster processing** - HPC usage via SSH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55388b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPTnano package imports\n",
    "import SPTnano as spt\n",
    "\n",
    "# Configuration\n",
    "saved_data = spt.config.SAVED_DATA\n",
    "time_between_frames = spt.config.TIME_BETWEEN_FRAMES\n",
    "\n",
    "# Data processing imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"âœ“ SPTnano loaded successfully\")\n",
    "print(f\"âœ“ Saved data directory: {saved_data}\")\n",
    "print(f\"âœ“ Time between frames: {time_between_frames} s\")\n",
    "\n",
    "# Load instant_df for processing\n",
    "instant_df = pd.read_csv(saved_data + 'instant_df.csv')\n",
    "print(f\"âœ“ Loaded instant_df: {len(instant_df):,} points, {instant_df['unique_id'].nunique():,} tracks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074269b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique unique ids\n",
    "print(instant_df['unique_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458da886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTER PROCESSING CONFIGURATION\n",
    "# UPDATE PATHS\n",
    "\n",
    "# File paths \n",
    "cluster_input_file = '/path/to//full_instant_df.csv'  # Your full dataset on cluster\n",
    "cluster_output_dir = '/path/to//output_directory'     # Where to save results\n",
    "\n",
    "# Processing settings (ADJUST BASED ON CLUSTER RESOURCES!)  \n",
    "cluster_n_jobs = -1        # -1 = use ALL available cores\n",
    "cluster_chunk_size = 4000  # Tracks per chunk \n",
    "\n",
    "# SPTnano parameters (MODIFY IF NEEDED!)\n",
    "cluster_time_between_frames = 0.01  # Time between frames\n",
    "cluster_window_size = 60            # Window size in frames\n",
    "cluster_overlap = 30               # Window overlap in frames\n",
    "cluster_r2_threshold = 0.000001    # RÂ² threshold for curve fitting\n",
    "\n",
    "print(\"Cluster processing configuration:\")\n",
    "print(f\"  Input file: {cluster_input_file}\")\n",
    "print(f\"  Output directory: {cluster_output_dir}\")\n",
    "print(f\"  CPU cores: {'ALL' if cluster_n_jobs == -1 else cluster_n_jobs}\")\n",
    "print(f\"  Chunk size: {cluster_chunk_size} tracks\")\n",
    "print(f\"  Window size: {cluster_window_size} frames\")\n",
    "\n",
    "# UNCOMMENT AND RUN THIS ON CLUSTER:\n",
    "# ============================================\n",
    "#\n",
    "# from SPTnano.HPC import parallel_time_windowed_metrics\n",
    "# import time\n",
    "#\n",
    "# print(\"Starting cluster processing...\")\n",
    "# start_time = time.time()\n",
    "#\n",
    "# cluster_instant_output, cluster_windowed_output = parallel_time_windowed_metrics(\n",
    "#     input_file=cluster_input_file,\n",
    "#     output_dir=cluster_output_dir,\n",
    "#     n_jobs=cluster_n_jobs,\n",
    "#     chunk_size=cluster_chunk_size,\n",
    "#     time_between_frames=cluster_time_between_frames,\n",
    "#     window_size=cluster_window_size,\n",
    "#     overlap=cluster_overlap,\n",
    "#     r2_threshold=cluster_r2_threshold\n",
    "# )\n",
    "#\n",
    "# total_time = time.time() - start_time\n",
    "# print(f\"\\\\nðŸŽ‰ CLUSTER PROCESSING COMPLETED in {total_time:.1f}s\")\n",
    "# print(f\"Results saved to:\")\n",
    "# print(f\"  - Instant DF: {cluster_instant_output}\")\n",
    "# print(f\"  - Windowed DF: {cluster_windowed_output}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"COMMAND LINE ALTERNATIVE:\")\n",
    "print(\"=\"*60)\n",
    "print(\"# Run from terminal on cluster:\")\n",
    "print(f\"python -m SPTnano.HPC \\\\\\\\\")\n",
    "print(f\"    --input_file {cluster_input_file} \\\\\\\\\")\n",
    "print(f\"    --output_dir {cluster_output_dir} \\\\\\\\\")\n",
    "print(f\"    --n_jobs {cluster_n_jobs} \\\\\\\\\")\n",
    "print(f\"    --chunk_size {cluster_chunk_size}\")\n",
    "\n",
    "print(\"\\\\nðŸ’¡ Remember to update the file paths above for your cluster!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4226b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f277938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee34bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoSPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
