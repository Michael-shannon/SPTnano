{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#sptnano","title":"SPTnano","text":"<p>Pipeline for easy single particle detection, linking/tracking, and analysis</p> <p>The structure for this repo was made using a cookiecutter from the Centre for Advanced Research Computing, University College London.</p>"},{"location":"#about","title":"About","text":""},{"location":"#project-team","title":"Project Team","text":"<p>Michael Shannon (m.j.shannon@pm.me)</p>"},{"location":"#research-software-engineering-contact","title":"Research Software Engineering Contact","text":"<p>Michael Shannon (m.j.shannon@pm.me)</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<p><code>SPTnano</code> requires Python 3.10\u20133.12.</p>"},{"location":"#installation","title":"Installation","text":"<p>We recommend installing in a project specific virtual environment created using a environment management tool such as Conda. To install the latest development version of <code>SPTnano</code> using <code>pip</code> in the currently active environment run</p> <pre><code>pip install git+https://github.com/Michael-shannon/SPTnano.git\n</code></pre> <p>Alternatively create a local clone of the repository with</p> <pre><code>git clone https://github.com/Michael-shannon/SPTnano.git\n</code></pre> <p>and then install in editable mode by running</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"#running-locally","title":"Running Locally","text":"<p>How to run the application on your local system.</p>"},{"location":"#running-tests","title":"Running Tests","text":"<p>Tests can be run across all compatible Python versions in isolated environments using <code>tox</code> by running</p> <pre><code>tox\n</code></pre> <p>To run tests manually in a Python environment with <code>pytest</code> installed run</p> <pre><code>pytest tests\n</code></pre> <p>again from the root of the repository.</p>"},{"location":"#building-documentation","title":"Building Documentation","text":"<p>The MkDocs HTML documentation can be built locally by running</p> <pre><code>tox -e docs\n</code></pre> <p>from the root of the repository. The built documentation will be written to <code>site</code>.</p> <p>Alternatively to build and preview the documentation locally, in a Python environment with the optional <code>docs</code> dependencies installed, run</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Initial Research</li> <li> Minimum viable product &lt;-- You are Here</li> <li> Alpha Release</li> <li> Feature-Complete Release</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This work was funded by Cure Huntingtons Disease Initiative (CHDI) and The Rockefeller University.</p>"},{"location":"LICENSE/","title":"License","text":""},{"location":"LICENSE/#mit-license","title":"MIT License","text":"<p>Copyright (c) 2024 Michael Shannon</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"api/","title":"API reference","text":"<p>SPTnano package.</p>"},{"location":"api/#SPTnano.add_microns_and_secs","title":"<code>add_microns_and_secs(df, pixelsize_microns, time_between_frames)</code>","text":"<p>Adds columns to the DataFrame with positions in microns and time in seconds</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def add_microns_and_secs(df, pixelsize_microns, time_between_frames):\n    '''Adds columns to the DataFrame with positions in microns and time in seconds'''\n    #space transformations\n    df['x_um'] = df['x'] * pixelsize_microns\n    df['y_um'] = df['y'] * pixelsize_microns\n\n    df['frame_zeroed'] = df.groupby('particle')['frame'].transform(lambda x: x - x.iloc[0])\n    df['time_s'] = df['frame'] * time_between_frames\n    df['time_s_zeroed'] = df.groupby('particle')['time_s'].transform(lambda x: x - x.iloc[0])\n    return df\n</code></pre>"},{"location":"api/#SPTnano.batch_plot_trajectories","title":"<code>batch_plot_trajectories(master_folder, traj_df, batch=True, filename=None, colorby='particle', mpp=None, label=False, cmap=None)</code>","text":"<p>Batch plot trajectories for all replicates across several conditions.</p>"},{"location":"api/#SPTnano.batch_plot_trajectories--parameters","title":"Parameters","text":"<p>master_folder : str     Path to the master folder containing 'data' and 'saved_data' folders. traj_df : DataFrame     The DataFrame containing trajectory data. batch : bool, optional     If True, plots trajectories for all replicates in batch mode.     If False, plots trajectory for the specified filename. filename : str, optional     Filename of interest when batch is False. colorby : str, optional     Color by 'particle' or 'frame'. mpp : float, optional     Microns per pixel. label : bool, optional     Set to True to write particle ID numbers next to trajectories. cmap : colormap, optional     Colormap to use for coloring tracks.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def batch_plot_trajectories(master_folder, traj_df, batch=True, filename=None, colorby='particle', mpp=None, label=False, cmap=None):\n    \"\"\"\n    Batch plot trajectories for all replicates across several conditions.\n\n    Parameters\n    ----------\n    master_folder : str\n        Path to the master folder containing 'data' and 'saved_data' folders.\n    traj_df : DataFrame\n        The DataFrame containing trajectory data.\n    batch : bool, optional\n        If True, plots trajectories for all replicates in batch mode.\n        If False, plots trajectory for the specified filename.\n    filename : str, optional\n        Filename of interest when batch is False.\n    colorby : str, optional\n        Color by 'particle' or 'frame'.\n    mpp : float, optional\n        Microns per pixel.\n    label : bool, optional\n        Set to True to write particle ID numbers next to trajectories.\n    cmap : colormap, optional\n        Colormap to use for coloring tracks.\n    \"\"\"\n    data_folder = os.path.join(master_folder, 'data')\n    vis_folder = os.path.join(master_folder, 'visualization/trajectories')\n    os.makedirs(vis_folder, exist_ok=True)\n\n    if batch:\n        for condition in os.listdir(data_folder):\n            condition_folder = os.path.join(data_folder, condition)\n            if os.path.isdir(condition_folder):\n                for file in os.listdir(condition_folder):\n                    if file.endswith('.tif'):\n                        filepath = os.path.join(condition_folder, file)\n                        subset_traj_df = traj_df[traj_df['filename'] == file]\n                        if not subset_traj_df.empty:\n                            frames = pims.open(filepath)\n                            frame = frames[0]\n                            fig, ax = plt.subplots()\n                            plot_trajectory(subset_traj_df, colorby=colorby, mpp=mpp, label=label, superimpose=frame, cmap=cmap, ax=ax)\n                            plt.savefig(os.path.join(vis_folder, f'{condition}_{file}.png'))\n                            plt.close(fig)\n    else:\n        if filename is not None:\n            filepath = os.path.join(data_folder, filename)\n            subset_traj_df = traj_df[traj_df['filename'] == filename]\n            if not subset_traj_df.empty:\n                frames = pims.open(filepath)\n                frame = frames[0]\n                fig, ax = plt.subplots()\n                plot_trajectory(subset_traj_df, colorby=colorby, mpp=mpp, label=label, superimpose=frame, cmap=cmap, ax=ax)\n                plt.show()\n        else:\n            print(\"Please provide a filename when batch is set to False.\")\n</code></pre>"},{"location":"api/#SPTnano.example_function","title":"<code>example_function(argument, keyword_argument='default')</code>","text":"<p>Concatenate string arguments - an example function docstring.</p> <p>Parameters:</p> Name Type Description Default <code>argument</code> <code>str</code> <p>An argument.</p> required <code>keyword_argument</code> <code>str</code> <p>A keyword argument with a default value.</p> <code>'default'</code> <p>Returns:</p> Type Description <code>str</code> <p>The concatenation of <code>argument</code> and <code>keyword_argument</code>.</p> Source code in <code>src/SPTnano/__init__.py</code> <pre><code>def example_function(argument: str, keyword_argument: str = \"default\") -&gt; str:\n    \"\"\"\n    Concatenate string arguments - an example function docstring.\n\n    Args:\n        argument: An argument.\n        keyword_argument: A keyword argument with a default value.\n\n    Returns:\n        The concatenation of `argument` and `keyword_argument`.\n\n    \"\"\"\n    return argument + keyword_argument\n</code></pre>"},{"location":"api/#SPTnano.filter_high_speeds","title":"<code>filter_high_speeds(metrics_df, speed_threshold)</code>","text":"<p>Filter based on speed instead - can be relevant if you have different exposure times and different times between frames</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def filter_high_speeds(metrics_df, speed_threshold):\n    '''\n    Filter based on speed instead - can be relevant if you have different exposure times and different times between frames\n    '''\n\n    # Identify unique_ids with any high speeds\n    high_speed_particles = metrics_df[metrics_df['speed_um_s'] &gt; speed_threshold]['unique_id'].unique()\n\n    # Filter out particles with high speeds\n    metrics_df_filtered = metrics_df[~metrics_df['unique_id'].isin(high_speed_particles)].copy()\n    return metrics_df_filtered\n</code></pre>"},{"location":"api/#SPTnano.filter_large_jumps","title":"<code>filter_large_jumps(df, threshold)</code>","text":"<p>Filter out entire particles with any frames showing large jumps in micrometers.</p>"},{"location":"api/#SPTnano.filter_large_jumps--parameters","title":"Parameters","text":"<p>df : DataFrame     DataFrame containing tracking data with a 'segment_len_um' column. threshold : float     Threshold for what constitutes a large jump in micrometers.</p>"},{"location":"api/#SPTnano.filter_large_jumps--returns","title":"Returns","text":"<p>DataFrame     DataFrame with particles having large jumps filtered out.</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def filter_large_jumps(df, threshold):\n    \"\"\"\n    Filter out entire particles with any frames showing large jumps in micrometers.\n\n    Parameters\n    ----------\n    df : DataFrame\n        DataFrame containing tracking data with a 'segment_len_um' column.\n    threshold : float\n        Threshold for what constitutes a large jump in micrometers.\n\n    Returns\n    -------\n    DataFrame\n        DataFrame with particles having large jumps filtered out.\n    \"\"\"\n    # Identify unique_ids with any large jumps\n    large_jump_particles = df[df['segment_len_um'] &gt; threshold]['unique_id'].unique()\n\n    # Filter out particles with large jumps\n    df_filtered = df[~df['unique_id'].isin(large_jump_particles)].copy()\n    # df_filtered.drop(columns=['x_um_prev', 'y_um_prev', 'segment_len_um'], inplace=True)\n    return df_filtered\n</code></pre>"},{"location":"api/#SPTnano.filter_stubs","title":"<code>filter_stubs(df, min_time)</code>","text":"<p>Removes tracks that are shorter than 'min_time' by finding the max duration of each time_s_zeroed column and filtering on that Works across exposure times, because it works on converted seconds, not frames</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def filter_stubs(df, min_time):\n\n    '''\n    Removes tracks that are shorter than 'min_time' by finding the max duration of each time_s_zeroed column and filtering on that\n    Works across exposure times, because it works on converted seconds, not frames\n\n    '''\n    # Calculate the duration of each track by grouping by 'particle' and using the 'time_s' column\n    track_durations = df.groupby('unique_id')['time_s_zeroed'].max() \n    # Identify particles with tracks longer than 0.2 seconds\n    valid_particles = track_durations[track_durations &gt;= min_time].index\n    # Filter the dataframe to include only valid particles\n    filtered_df = df[df['unique_id'].isin(valid_particles)]\n\n    return filtered_df\n</code></pre>"},{"location":"api/#SPTnano.plot_barplots","title":"<code>plot_barplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', meanormedian='mean', talk=False)</code>","text":"<p>Plot bar plots of a specified factor, with bootstrapped confidence intervals.</p>"},{"location":"api/#SPTnano.plot_barplots--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing the data. factor_col : str, optional     The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'. separate_by : str, optional     Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'. palette : str, optional     Color palette for the plot. Default is 'colorblind'. meanormedian : str, optional     Whether to use mean or median for aggregation. Default is 'mean'. talk : bool, optional     Whether to set the figure size to the original large size or a smaller size. Default is False.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_barplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', meanormedian='mean', talk=False):\n    \"\"\"\n    Plot bar plots of a specified factor, with bootstrapped confidence intervals.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing the data.\n    factor_col : str, optional\n        The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'.\n    separate_by : str, optional\n        Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'.\n    palette : str, optional\n        Color palette for the plot. Default is 'colorblind'.\n    meanormedian : str, optional\n        Whether to use mean or median for aggregation. Default is 'mean'.\n    talk : bool, optional\n        Whether to set the figure size to the original large size or a smaller size. Default is False.\n    \"\"\"\n\n    unique_categories = data_df[separate_by].unique() if separate_by else [None]\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    # Set figure size based on the `talk` parameter\n    if talk:\n        fig_size = (20, 12)\n        font_size = 35\n    else:\n        fig_size = (5, 3)\n        font_size = 14\n\n    fig, ax = plt.subplots(figsize=fig_size)\n    sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5, \"font.size\": font_size, \"axes.titlesize\": font_size, \"axes.labelsize\": font_size, \"xtick.labelsize\": font_size, \"ytick.labelsize\": font_size})\n\n    avg_factors_list = []\n    ci_intervals = []\n\n    for i, category in enumerate(unique_categories):\n        subset = data_df if category is None else data_df[data_df[separate_by] == category]\n\n        if meanormedian == 'mean':\n            avg_factors = subset[factor_col].mean()\n            ci_interval = bootstrap_ci_mean(subset[factor_col], num_samples=1000, alpha=0.05)\n        else:\n            avg_factors = subset[factor_col].median()\n            ci_interval = bootstrap_ci_median(subset[factor_col], num_samples=1000, alpha=0.05)\n\n        avg_factors_list.append(avg_factors)\n        ci_intervals.append(ci_interval)\n\n    categories = unique_categories if separate_by else ['Overall']\n    ax.bar(categories, avg_factors_list, yerr=ci_intervals, color=color_palette, capsize=5, edgecolor='black')\n\n    # Remove 'Condition_' prefix from x tick labels\n    new_labels = [label.replace('Condition_', '') for label in categories]\n    if talk:\n        ax.set_xticklabels(new_labels, fontsize=font_size)\n    else:\n        ax.set_xticklabels(new_labels, fontsize=font_size, rotation=90)\n\n\n    ax.set_ylabel(factor_col, fontsize=font_size)\n    ax.tick_params(axis='both', which='major', labelsize=font_size)\n    plt.tight_layout()\n\n    plt.show()\n</code></pre>"},{"location":"api/#SPTnano.plot_histograms","title":"<code>plot_histograms(data_df, feature, bins=100, separate=None, xlimit=None, small_multiples=False, palette='colorblind', use_kde=False, show_plot=True, master_dir=None, tick_interval=5, average='mean', order=None, grid=False)</code>","text":"<p>Plot histograms or KDEs of a specified feature for each category in <code>separate</code>, with consistent binning.</p>"},{"location":"api/#SPTnano.plot_histograms--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing track data with the specified feature and optionally a separating column. feature : str     The feature to plot histograms for. bins : int, optional     Number of bins for the histogram. Default is 100. separate : str, optional     Column to separate the data by. If None, all data will be plotted together. Default is None. xlimit : float, optional     Upper limit for the x-axis. Default is None. small_multiples : bool, optional     Whether to plot each category separately as small multiples. Default is False. palette : str, optional     Color palette for the plot. Default is 'colorblind'. use_kde : bool, optional     Whether to use KDE plot instead of histogram. Default is False. show_plot : bool, optional     Whether to display the plot in the notebook. Default is True. master_dir : str, optional     The directory where the plots folder will be created and the plot will be saved. Default is None. tick_interval : int, optional     Interval for x-axis ticks. Default is 5. average : str, optional     Whether to draw 'mean' or 'median' line on the plot. Default is 'mean'. order : list, optional     Specific order for the conditions. Default is None.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_histograms(data_df, feature, bins=100, separate=None, xlimit=None, small_multiples=False, palette='colorblind', use_kde=False, show_plot=True, master_dir=None, tick_interval=5, average='mean', order=None, grid=False):\n    \"\"\"\n    Plot histograms or KDEs of a specified feature for each category in `separate`, with consistent binning.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing track data with the specified feature and optionally a separating column.\n    feature : str\n        The feature to plot histograms for.\n    bins : int, optional\n        Number of bins for the histogram. Default is 100.\n    separate : str, optional\n        Column to separate the data by. If None, all data will be plotted together. Default is None.\n    xlimit : float, optional\n        Upper limit for the x-axis. Default is None.\n    small_multiples : bool, optional\n        Whether to plot each category separately as small multiples. Default is False.\n    palette : str, optional\n        Color palette for the plot. Default is 'colorblind'.\n    use_kde : bool, optional\n        Whether to use KDE plot instead of histogram. Default is False.\n    show_plot : bool, optional\n        Whether to display the plot in the notebook. Default is True.\n    master_dir : str, optional\n        The directory where the plots folder will be created and the plot will be saved. Default is None.\n    tick_interval : int, optional\n        Interval for x-axis ticks. Default is 5.\n    average : str, optional\n        Whether to draw 'mean' or 'median' line on the plot. Default is 'mean'.\n    order : list, optional\n        Specific order for the conditions. Default is None.\n    \"\"\"\n\n    if master_dir is None:\n        master_dir = config.master  # Use the master directory from config if not provided\n\n    if separate is not None and order is not None:\n        # Ensure the data is ordered according to the specified order\n        data_df[separate] = pd.Categorical(data_df[separate], categories=order, ordered=True)\n\n    textpositionx=  0.6\n    textpositiony= 0.8\n\n    # Use the categories attribute to maintain the specified order\n    if separate is not None:\n        unique_categories = data_df[separate].cat.categories\n    else:\n        unique_categories = [None]\n\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    # Determine global maximum y-value for consistent y-axis limits\n    global_max_y = 0\n\n    if small_multiples and separate is not None:\n        num_categories = len(unique_categories)\n        fig, axes = plt.subplots(num_categories, 1, figsize=(20, 6 * num_categories), sharex=True)\n\n        if num_categories == 1:\n            axes = [axes]  # To handle the case with only one subplot\n\n        for i, category in enumerate(unique_categories):\n            if pd.isna(category):\n                continue\n            subset = data_df[data_df[separate] == category]\n            subsetvalues = subset[feature]\n\n            max_value = subsetvalues.max()\n\n            # Determine bin edges for the entire data range, including negative values\n            min_value = data_df[feature].min()\n            max_value = data_df[feature].max()\n            bin_edges = np.linspace(min_value, max_value, bins + 1)\n            # bin_edges = np.linspace(0, max_value, bins + 1)\n\n            # Plot histogram or KDE\n            if use_kde:\n                sns.kdeplot(subsetvalues, fill=True, ax=axes[i], color=color_palette[i])\n                current_max_y = axes[i].get_ylim()[1]  # Get the current maximum y-value from the plot\n            else:\n                plot = sns.histplot(subsetvalues, bins=bin_edges, kde=False, ax=axes[i], stat=\"percent\", color=color_palette[i])\n                current_max_y = plot.get_ylim()[1]\n\n            # Update global maximum y-value\n            if current_max_y &gt; global_max_y:\n                global_max_y = current_max_y\n\n            # Plot average line\n            if average == 'mean':\n                avg_value = subsetvalues.mean()\n                axes[i].axvline(avg_value, color='black', linestyle='--')\n                axes[i].text(textpositionx, textpositiony, f\"Mean: {avg_value:.2f}\", transform=axes[i].transAxes, fontsize=16)\n            elif average == 'median':\n                avg_value = subsetvalues.median()\n                axes[i].axvline(avg_value, color='black', linestyle='--')\n                axes[i].text(textpositionx, textpositiony, f\"Median: {avg_value:.2f}\", transform=axes[i].transAxes, fontsize=16)\n\n            axes[i].set_title(f'{category}', fontsize=16)\n            axes[i].tick_params(axis='both', which='major', labelsize=16)\n            axes[i].set_xlabel(f'{feature}', fontsize=16)\n            axes[i].set_ylabel('Percentage', fontsize=16)\n\n            if xlimit is not None:\n                x_lower = min_value if min_value &lt; 0 else 0\n                axes[i].set_xlim(x_lower, xlimit)\n            else:\n                axes[i].set_xlim(min_value, max_value)\n\n        # Set common y-axis limits for all subplots\n        for ax in axes:\n            ax.set_ylim(0, global_max_y)\n\n        plt.tight_layout()\n\n    else:\n        plt.figure(figsize=(20, 12))\n        sns.set_context(\"notebook\", rc={\"xtick.labelsize\": 16, \"ytick.labelsize\": 16})\n\n        max_value = data_df[feature].max()\n        bin_edges = np.linspace(0, max_value, bins + 1)\n\n        if separate is None:\n            subsetvalues = data_df[feature]\n\n            # Plot histogram or KDE\n            if use_kde:\n                sns.kdeplot(subsetvalues, fill=True, alpha=0.5, color=color_palette[0])\n            else:\n                sns.histplot(subsetvalues, bins=bin_edges, kde=False, alpha=0.5, stat=\"percent\", color=color_palette[0])\n\n            # Plot average line\n            if average == 'mean':\n                avg_value = subsetvalues.mean()\n                plt.axvline(avg_value, color='r', linestyle='--')\n                plt.text(textpositionx, textpositiony, f\"Overall Mean: {avg_value:.2f}\", transform=plt.gca().transAxes, fontsize=16)\n            elif average == 'median':\n                avg_value = subsetvalues.median()\n                plt.axvline(avg_value, color='b', linestyle='--')\n                plt.text(0.4, 0.6, f\"Overall Median: {avg_value:.2f}\", transform=plt.gca().transAxes, fontsize=16)\n\n        else:\n            for i, category in enumerate(unique_categories):\n                if pd.isna(category):\n                    continue\n                subset = data_df[data_df[separate] == category]\n                subsetvalues = subset[feature]\n\n                # Plot histogram or KDE\n                if use_kde:\n                    sns.kdeplot(subsetvalues, fill=True, label=category, alpha=0.5, color=color_palette[i])\n                else:\n                    sns.histplot(subsetvalues, bins=bin_edges, kde=False, label=category, alpha=0.5, stat=\"percent\", color=color_palette[i])\n\n                # Plot average line\n                if average == 'mean':\n                    avg_value = subsetvalues.mean()\n                    plt.axvline(avg_value, color=color_palette[i], linestyle='--')\n                elif average == 'median':\n                    avg_value = subsetvalues.median()\n                    plt.axvline(avg_value, color=color_palette[i], linestyle='--')\n\n                number_of_tracks = len(subset['unique_id'].unique())\n                shift = i * 0.05\n                plt.text(textpositionx, textpositiony - shift, f\"{category}: {average.capitalize()}: {avg_value:.2f} from {number_of_tracks} tracks\", transform=plt.gca().transAxes, fontsize=16)\n\n        plt.xlabel(f'{feature}', fontsize=16)\n        plt.ylabel('Percentage', fontsize=16)\n        plt.legend(title='', fontsize=16)\n        ax = plt.gca()\n        if xlimit is not None:\n            x_lower = min_value if min_value &lt; 0 else 0\n            axes[i].set_xlim(x_lower, xlimit)\n        else:\n            axes[i].set_xlim(min_value, max_value)\n\n        ax.set_xticks(np.arange(0, max_value + 1, tick_interval))  # Ensure ticks are at integer intervals\n        ax.set_xlim(0, xlimit or max_value)  # Start x-axis at 0\n        if grid:\n            ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)  # Add faint gridlines\n        else:\n            print(\"No grid\")\n\n    # Create directory for plots if it doesn't exist\n    plots_dir = os.path.join(master_dir, 'plots')\n    os.makedirs(plots_dir, exist_ok=True)\n    histodir = os.path.join(plots_dir, 'histograms')\n    os.makedirs(histodir, exist_ok=True)\n\n    # Generate filename\n    kde_text = 'kde' if use_kde else 'histogram'\n    average_text = f'{average}'\n    if small_multiples:\n        multitext = 'small_multiples'\n    else:\n        multitext = 'single_plot'\n\n    filename = f\"{histodir}/{kde_text}_{feature}_{average_text}_{multitext}.png\"\n\n    # Save plot\n    plt.savefig(filename, bbox_inches='tight')\n\n    # Show plot if specified\n    if show_plot:\n        plt.show()\n    else:\n        plt.close()\n</code></pre>"},{"location":"api/#SPTnano.plot_histograms_seconds","title":"<code>plot_histograms_seconds(traj_df, bins=100, coltoseparate='tracker', xlimit=None)</code>","text":"<p>Plot histograms of track lengths in seconds for each tracker, with consistent binning.</p>"},{"location":"api/#SPTnano.plot_histograms_seconds--parameters","title":"Parameters","text":"<p>traj_df : DataFrame     DataFrame containing track data with columns 'tracker', 'unique_id', 'time_s_zeroed', and 'filename'. bins : int, optional     Number of bins for the histogram. Default is 100. coltoseparate : str, optional     Column to separate the data by. Default is 'tracker'. xlimit : float, optional     Upper limit for the x-axis. Default is None.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_histograms_seconds(traj_df, bins=100, coltoseparate='tracker', xlimit=None):\n    \"\"\"\n    Plot histograms of track lengths in seconds for each tracker, with consistent binning.\n\n    Parameters\n    ----------\n    traj_df : DataFrame\n        DataFrame containing track data with columns 'tracker', 'unique_id', 'time_s_zeroed', and 'filename'.\n    bins : int, optional\n        Number of bins for the histogram. Default is 100.\n    coltoseparate : str, optional\n        Column to separate the data by. Default is 'tracker'.\n    xlimit : float, optional\n        Upper limit for the x-axis. Default is None.\n    \"\"\"\n    plt.figure(figsize=(20, 12))\n    size = 10\n    multiplier = 2\n    sns.set_context(\"notebook\", rc={\"xtick.labelsize\": size*multiplier, \"ytick.labelsize\": size*multiplier})\n\n    max_track_length = traj_df.groupby('unique_id')['time_s_zeroed'].max().max()\n    bin_edges = np.linspace(0, max_track_length, bins + 1)\n\n    for i, tracker in enumerate(traj_df[coltoseparate].unique()):\n        subset = traj_df[traj_df[coltoseparate] == tracker]\n        subsetvalues = subset.groupby('unique_id')['time_s_zeroed'].max()\n\n        # Calculate percentage counts\n        counts, _ = np.histogram(subsetvalues, bins=bin_edges)\n        percentage_counts = (counts / counts.sum()) * 100\n\n        # Plot histogram\n        sns.histplot(subsetvalues, bins=bin_edges, kde=True, label=tracker, alpha=0.5, stat=\"percent\")\n\n        subset_mean = subsetvalues.mean()\n        subset_median = subsetvalues.median()\n        subset_number_of_tracks = len(subset['unique_id'].unique())\n        shift = i * 0.05\n        plt.text(0.4, 0.6 - shift, f\"{tracker}: mean: {subset_mean:.2f} seconds from {subset_number_of_tracks} tracks\", transform=plt.gca().transAxes, fontsize=10 * multiplier)\n\n    plt.xlabel('Track length (seconds)', fontsize=size * multiplier)\n    plt.ylabel('Percentage', fontsize=size * multiplier)\n    plt.legend(title='', fontsize=size * multiplier)\n    ax = plt.gca()\n    if xlimit is not None:\n        ax.set_xlim(0, xlimit)\n    else:\n        ax.set_xlim(0, max_track_length)\n    plt.show()\n</code></pre>"},{"location":"api/#SPTnano.plot_time_series","title":"<code>plot_time_series(data_df, factor_col='speed_um_s', absolute=True, separate_by='condition', palette='colorblind', meanormedian='mean', multiplot=False, talk=False, bootstrap=True, show_plot=True, master_dir=None, order=None, grid=True)</code>","text":"<p>Plot time series of a specified factor, with mean/median as a line and confidence intervals as shaded areas.</p>"},{"location":"api/#SPTnano.plot_time_series--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing the time series data. factor_col : str, optional     The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'. absolute : bool, optional     Whether to use absolute time values or time zeroed values. Default is True. separate_by : str, optional     Column to separate the data by, for coloring. If None, all data will be plotted together. Default is None. palette : str, optional     Color palette for the plot. Default is 'colorblind'. meanormedian : str, optional     Whether to use mean or median for aggregation. Default is 'mean'. multiplot : bool, optional     Whether to generate separate small multiple plots for each category. Default is False. talk : bool, optional     Whether to set the figure size to the original large size or a smaller size. Default is False. bootstrap : bool, optional     Whether to use bootstrapping for confidence intervals. Default is True. show_plot : bool, optional     Whether to display the plot in the notebook. Default is True. master_dir : str, optional     The directory where the plots folder will be created and the plot will be saved. Default is None. order : list, optional     Specific order for the conditions. Default is None. grid : bool, optional     Whether to display grid lines. Default is True.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_time_series(data_df, factor_col='speed_um_s', absolute=True, separate_by='condition', palette='colorblind', \n                     meanormedian='mean', multiplot=False, talk=False, bootstrap=True, show_plot=True, \n                     master_dir=None, order=None, grid=True):\n    \"\"\"\n    Plot time series of a specified factor, with mean/median as a line and confidence intervals as shaded areas.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing the time series data.\n    factor_col : str, optional\n        The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'.\n    absolute : bool, optional\n        Whether to use absolute time values or time zeroed values. Default is True.\n    separate_by : str, optional\n        Column to separate the data by, for coloring. If None, all data will be plotted together. Default is None.\n    palette : str, optional\n        Color palette for the plot. Default is 'colorblind'.\n    meanormedian : str, optional\n        Whether to use mean or median for aggregation. Default is 'mean'.\n    multiplot : bool, optional\n        Whether to generate separate small multiple plots for each category. Default is False.\n    talk : bool, optional\n        Whether to set the figure size to the original large size or a smaller size. Default is False.\n    bootstrap : bool, optional\n        Whether to use bootstrapping for confidence intervals. Default is True.\n    show_plot : bool, optional\n        Whether to display the plot in the notebook. Default is True.\n    master_dir : str, optional\n        The directory where the plots folder will be created and the plot will be saved. Default is None.\n    order : list, optional\n        Specific order for the conditions. Default is None.\n    grid : bool, optional\n        Whether to display grid lines. Default is True.\n    \"\"\"\n    xmin = 0.2  # A FIX FOR NOW because really this should be the same as the shortest track (filtered to 0.2 s during filterstubs)\n\n    if master_dir is None:\n        master_dir = config.MASTER  # Use the master directory from config if not provided\n\n    if separate_by is not None and order is not None:\n        # Ensure the data is ordered according to the specified order\n        data_df[separate_by] = pd.Categorical(data_df[separate_by], categories=order, ordered=True)\n\n    if not absolute:\n        time_col = 'time_s_zeroed'\n        max_time_zeroed = data_df['time_s_zeroed'].max()\n        x_label = 'Time zeroed (s)'\n        xmax = max_time_zeroed\n    else:\n        time_col = 'time_s'\n        max_time = data_df['time_s'].max()\n        x_label = 'Time (s)'\n        xmax = max_time\n\n    # Use the categories attribute to maintain the specified order\n    if separate_by is not None:\n        # Convert to categorical if not already\n        if not pd.api.types.is_categorical_dtype(data_df[separate_by]):\n            data_df[separate_by] = pd.Categorical(data_df[separate_by], categories=order, ordered=True)\n        unique_categories = data_df[separate_by].cat.categories\n    else:\n        unique_categories = [None]\n\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    # # Set figure size and font size based on the `talk` parameter\n    # if talk:\n    #     fig_size = (40, 12)\n    #     font_size = 35\n    # else:\n    #     if multiplot and separate_by:\n    #         fig_size = (4, 8 * len(unique_categories))\n    #     else:\n    #         fig_size = (5, 3)\n    #     font_size = 14\n\n# Set figure size and font size based on the `talk` and `multiplot` parameters\n    if talk:\n        base_fig_size = (40, 12)\n        font_size = 35\n    else:\n        base_fig_size = (5, 4)\n        font_size = 14\n\n    # Adjust figure size if multiplot is true\n    if multiplot and separate_by:\n        fig_size = (base_fig_size[0], base_fig_size[1] * len(unique_categories))\n    else:\n        fig_size = base_fig_size\n\n    sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5, \"font.size\": font_size, \"axes.titlesize\": font_size, \n                                    \"axes.labelsize\": font_size, \"xtick.labelsize\": font_size, \"ytick.labelsize\": font_size})\n\n    if multiplot and separate_by:\n        num_categories = len(unique_categories)\n        fig, axes = plt.subplots(num_categories, 1, figsize=fig_size, sharex=True)\n\n        if num_categories == 1:\n            axes = [axes]  # To handle the case with only one subplot\n\n        for i, category in enumerate(unique_categories):\n            if pd.isna(category):\n                continue\n            ax = axes[i] if len(unique_categories) &gt; 1 else axes\n            subset = data_df[data_df[separate_by] == category]\n            times = subset[time_col]\n            factors = subset[factor_col]\n\n            if meanormedian == 'mean':\n                avg_factors = subset.groupby(time_col)[factor_col].mean()\n                ci_func = bootstrap_ci_mean if bootstrap else lambda x: sem(x) * 1.96\n            else:\n                avg_factors = subset.groupby(time_col)[factor_col].median()\n                ci_func = bootstrap_ci_median if bootstrap else lambda x: sem(x) * 1.96\n\n            ci = subset.groupby(time_col)[factor_col].apply(ci_func)\n\n            color = color_palette[i]\n            label = category\n\n            # Exclude the first time point (time zero)\n            valid_indices = avg_factors.index &gt; 0\n\n            ax.plot(avg_factors.index[valid_indices], avg_factors.values[valid_indices], label=label, color=color, linewidth=2.5)\n            ax.fill_between(avg_factors.index[valid_indices], (avg_factors - ci)[valid_indices], (avg_factors + ci)[valid_indices], color=color, alpha=0.3)\n            ax.set_xlabel(x_label, fontsize=font_size)\n            ax.set_ylabel(factor_col, fontsize=font_size, labelpad=20)\n            ax.legend(fontsize=font_size, loc='upper left', bbox_to_anchor=(1, 1))\n            ax.set_xlim(xmin, xmax)\n            if grid:\n                ax.grid(True, which='both', linestyle='--', linewidth=0.5, color='gray', alpha=0.7)\n            ax.spines['top'].set_visible(False)\n            ax.spines['right'].set_visible(False)\n            ax.set_title(f'{category}', fontsize=font_size)\n\n        plt.tight_layout()\n    else:\n        fig, ax = plt.subplots(figsize=fig_size)\n\n        for i, category in enumerate(unique_categories):\n            if pd.isna(category):\n                continue\n            subset = data_df if category is None else data_df[data_df[separate_by] == category]\n            times = subset[time_col]\n            factors = subset[factor_col]\n\n            if meanormedian == 'mean':\n                avg_factors = subset.groupby(time_col)[factor_col].mean()\n                ci_func = bootstrap_ci_mean if bootstrap else lambda x: sem(x) * 1.96\n            else:\n                avg_factors = subset.groupby(time_col)[factor_col].median()\n                ci_func = bootstrap_ci_median if bootstrap else lambda x: sem(x) * 1.96\n\n            ci = subset.groupby(time_col)[factor_col].apply(ci_func)\n\n            color = color_palette[i]\n            label = 'Overall' if category is None else category\n\n            # Exclude the first time point (time zero)\n            valid_indices = avg_factors.index &gt; 0\n\n            ax.plot(avg_factors.index[valid_indices], avg_factors.values[valid_indices], label=label, color=color, linewidth=2.5)\n            ax.fill_between(avg_factors.index[valid_indices], (avg_factors - ci)[valid_indices], (avg_factors + ci)[valid_indices], color=color, alpha=0.3)\n\n        ax.set_xlabel(x_label, fontsize=font_size)\n        ax.set_ylabel(factor_col, fontsize=font_size, labelpad=20)\n        ax.legend(fontsize=font_size, loc='upper left', bbox_to_anchor=(1.05, 1))\n        if grid:\n            ax.grid(True, which='both', linestyle='--', linewidth=0.5, color='gray', alpha=0.7)\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        ax.set_xlim(xmin, xmax)\n        plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout to fit legend\n\n    # Create directory for plots if it doesn't exist\n    plots_dir = os.path.join(master_dir, 'plots')\n    os.makedirs(plots_dir, exist_ok=True)\n\n    # Generate filename\n    time_type = 'absolute' if absolute else 'time_zeroed'\n    bootstrap_text = 'bootstrapped' if bootstrap else 'nonbootstrapped'\n    multiplot_text = 'multiplot' if multiplot else 'singleplot'\n    filename = f\"{plots_dir}/{factor_col}_{time_type}_{meanormedian}_{bootstrap_text}_{multiplot_text}.png\"\n\n    # Save plot\n    plt.savefig(filename, bbox_inches='tight')\n\n    # Show plot if specified\n    if show_plot:\n        plt.show()\n    else:\n        plt.close()\n</code></pre>"},{"location":"api/#SPTnano.plot_trajectory","title":"<code>plot_trajectory(traj, colorby='particle', mpp=None, label=False, superimpose=None, cmap=None, ax=None, t_column=None, pos_columns=None, plot_style={}, **kwargs)</code>","text":"<p>Plot traces of trajectories for each particle. Optionally superimpose it on a frame from the video.</p>"},{"location":"api/#SPTnano.plot_trajectory--parameters","title":"Parameters","text":"<p>traj : DataFrame     The DataFrame should include time and spatial coordinate columns. colorby : {'particle', 'frame'}, optional mpp : float, optional     Microns per pixel. If omitted, the labels will have units of pixels. label : boolean, optional     Set to True to write particle ID numbers next to trajectories. superimpose : ndarray, optional     Background image, default None cmap : colormap, optional     This is only used in colorby='frame' mode. Default = mpl.cm.winter ax : matplotlib axes object, optional     Defaults to current axes t_column : string, optional     DataFrame column name for time coordinate. Default is 'frame'. pos_columns : list of strings, optional     Dataframe column names for spatial coordinates. Default is ['x', 'y']. plot_style : dictionary     Keyword arguments passed through to the <code>Axes.plot(...)</code> command</p>"},{"location":"api/#SPTnano.plot_trajectory--returns","title":"Returns","text":"<p>Axes object</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_trajectory(traj, colorby='particle', mpp=None, label=False,\n                    superimpose=None, cmap=None, ax=None, t_column=None,\n                    pos_columns=None, plot_style={}, **kwargs):\n    \"\"\"\n    Plot traces of trajectories for each particle.\n    Optionally superimpose it on a frame from the video.\n\n    Parameters\n    ----------\n    traj : DataFrame\n        The DataFrame should include time and spatial coordinate columns.\n    colorby : {'particle', 'frame'}, optional\n    mpp : float, optional\n        Microns per pixel. If omitted, the labels will have units of pixels.\n    label : boolean, optional\n        Set to True to write particle ID numbers next to trajectories.\n    superimpose : ndarray, optional\n        Background image, default None\n    cmap : colormap, optional\n        This is only used in colorby='frame' mode. Default = mpl.cm.winter\n    ax : matplotlib axes object, optional\n        Defaults to current axes\n    t_column : string, optional\n        DataFrame column name for time coordinate. Default is 'frame'.\n    pos_columns : list of strings, optional\n        Dataframe column names for spatial coordinates. Default is ['x', 'y'].\n    plot_style : dictionary\n        Keyword arguments passed through to the `Axes.plot(...)` command\n\n    Returns\n    -------\n    Axes object\n    \"\"\"\n    if cmap is None:\n        cmap = plt.cm.winter\n    if t_column is None:\n        t_column = 'frame'\n    if pos_columns is None:\n        pos_columns = ['x', 'y']\n    if len(traj) == 0:\n        raise ValueError(\"DataFrame of trajectories is empty.\")\n\n    _plot_style = dict(linewidth=1)\n    _plot_style.update(**plot_style)\n\n    if ax is None:\n        ax = plt.gca()\n\n    # Axes labels\n    if mpp is None:\n        ax.set_xlabel(f'{pos_columns[0]} [px]')\n        ax.set_ylabel(f'{pos_columns[1]} [px]')\n        mpp = 1.  # for computations of image extent below\n    else:\n        ax.set_xlabel(f'{pos_columns[0]} [\u03bcm]')\n        ax.set_ylabel(f'{pos_columns[1]} [\u03bcm]')\n\n    # Background image\n    if superimpose is not None:\n        ax.imshow(superimpose, cmap=plt.cm.gray,\n                  origin='lower', interpolation='nearest',\n                  vmin=kwargs.get('vmin'), vmax=kwargs.get('vmax'))\n        ax.set_xlim(-0.5 * mpp, (superimpose.shape[1] - 0.5) * mpp)\n        ax.set_ylim(-0.5 * mpp, (superimpose.shape[0] - 0.5) * mpp)\n\n    # Trajectories\n    if colorby == 'particle':\n        # Unstack particles into columns.\n        unstacked = traj.set_index(['particle', t_column])[pos_columns].unstack()\n        for i, trajectory in unstacked.iterrows():\n            ax.plot(mpp * trajectory[pos_columns[0]], mpp * trajectory[pos_columns[1]], **_plot_style)\n    elif colorby == 'frame':\n        # Read http://www.scipy.org/Cookbook/Matplotlib/MulticoloredLine\n        x = traj.set_index([t_column, 'particle'])[pos_columns[0]].unstack()\n        y = traj.set_index([t_column, 'particle'])[pos_columns[1]].unstack()\n        color_numbers = traj[t_column].values / float(traj[t_column].max())\n        for particle in x:\n            points = np.array([x[particle].values, y[particle].values]).T.reshape(-1, 1, 2)\n            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n            lc = LineCollection(segments, cmap=cmap)\n            lc.set_array(color_numbers)\n            ax.add_collection(lc)\n            ax.set_xlim(x.apply(np.min).min(), x.apply(np.max).max())\n            ax.set_ylim(y.apply(np.min).min(), y.apply(np.max).max())\n\n    if label:\n        unstacked = traj.set_index([t_column, 'particle'])[pos_columns].unstack()\n        first_frame = int(traj[t_column].min())\n        coords = unstacked.fillna(method='backfill').stack().loc[first_frame]\n        for particle_id, coord in coords.iterrows():\n            ax.text(*coord.tolist(), s=\"%d\" % particle_id,\n                    horizontalalignment='center',\n                    verticalalignment='center')\n\n    ax.invert_yaxis()\n    return ax\n</code></pre>"},{"location":"api/#SPTnano.plot_violinplots","title":"<code>plot_violinplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', talk=False)</code>","text":"<p>Plot violin plots of a specified factor, with data separated by categories.</p>"},{"location":"api/#SPTnano.plot_violinplots--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing the data. factor_col : str, optional     The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'. separate_by : str, optional     Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'. palette : str, optional     Color palette for the plot. Default is 'colorblind'. talk : bool, optional     Whether to set the figure size to the original large size or a smaller size. Default is False.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_violinplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', talk=False):\n    \"\"\"\n    Plot violin plots of a specified factor, with data separated by categories.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing the data.\n    factor_col : str, optional\n        The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'.\n    separate_by : str, optional\n        Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'.\n    palette : str, optional\n        Color palette for the plot. Default is 'colorblind'.\n    talk : bool, optional\n        Whether to set the figure size to the original large size or a smaller size. Default is False.\n    \"\"\"\n\n    unique_categories = data_df[separate_by].unique() if separate_by else [None]\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    # Set figure size based on the `talk` parameter\n    if talk:\n        fig_size = (20, 12)\n        font_size = 35\n    else:\n        fig_size = (5, 3)\n        font_size = 14\n\n    fig, ax = plt.subplots(figsize=fig_size)\n    sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5, \"font.size\": font_size, \"axes.titlesize\": font_size, \"axes.labelsize\": font_size, \"xtick.labelsize\": font_size, \"ytick.labelsize\": font_size})\n\n    # Plot violin plot\n    sns.violinplot(x=separate_by, y=factor_col, hue=separate_by, data=data_df, palette=color_palette, ax=ax, legend=False, alpha=0.79)\n\n    # Remove 'Condition_' prefix from x tick labels\n    new_labels = [label.replace('Condition_', '') for label in unique_categories]\n    ax.set_xticks(range(len(new_labels)))\n    ax.set_xticklabels(new_labels, fontsize=font_size)\n\n    ax.set_ylabel(factor_col, fontsize=font_size, labelpad=20)\n    ax.set_xlabel(None)\n    ax.tick_params(axis='both', which='major', labelsize=font_size)\n    plt.tight_layout()\n\n    plt.show()\n</code></pre>"},{"location":"api/#SPTnano.features","title":"<code>features</code>","text":""},{"location":"api/#SPTnano.features.ParticleMetrics","title":"<code>ParticleMetrics</code>","text":"Source code in <code>src/SPTnano/features.py</code> <pre><code>class ParticleMetrics:\n    def __init__(self, df, time_between_frames=None, tolerance=None):\n        self.df = df.copy()\n        self.df['Location'] = self.df['filename'].apply(self.extract_location)  # Add Location column\n        self.metrics_df = self.df.copy()\n\n        # Retrieve time_between_frames from config if not provided\n        self.time_between_frames = time_between_frames if time_between_frames is not None else config.TIME_BETWEEN_FRAMES\n\n        self.time_averaged_df = pd.DataFrame(columns=[\n            'x_um_start', 'y_um_start', 'x_um_end', 'y_um_end', \n            'particle', 'condition', 'filename', 'file_id', 'unique_id',\n            'avg_msd', 'n_frames', 'total_time_s', 'Location', \n            'diffusion_coefficient', 'anomalous_exponent', 'motion_class',\n            'avg_speed_um_s', 'avg_acceleration_um_s2', 'avg_jerk_um_s3',\n            'avg_normalized_curvature', 'avg_angle_normalized_curvature'  # Add average normalized curvature and angle normalized curvature\n        ])\n        self.time_windowed_df = pd.DataFrame(columns=[\n            'time_window', 'x_um_start', 'y_um_start', 'x_um_end', 'y_um_end',\n            'particle', 'condition', 'filename', 'file_id', 'unique_id',\n            'avg_msd', 'n_frames', 'total_time_s', 'Location',\n            'diffusion_coefficient', 'anomalous_exponent', 'motion_class',\n            'avg_speed_um_s', 'avg_acceleration_um_s2', 'avg_jerk_um_s3',\n            'avg_normalized_curvature', 'avg_angle_normalized_curvature'  # Add average normalized curvature and angle normalized curvature\n        ])\n\n        self.msd_lagtime_df = pd.DataFrame(columns=[\n            'unique_id', 'time_window', 'lag_time', 'msd', 'diffusion_coefficient', 'anomalous_exponent'\n        ])\n\n                # Calculate tolerance if not provided\n        self.tolerance = tolerance or self.calculate_tolerance()\n\n    def calculate_tolerance(self):\n        # Determine tolerance from existing data if available\n        if len(self.msd_lagtime_df) &gt; 0:\n            alpha_std = self.msd_lagtime_df['anomalous_exponent'].std()\n            return alpha_std / 2  # Use half of the standard deviation\n        else:\n            return 0.1  # Default value if no previous data is available\n\n\n    @staticmethod\n    def extract_location(filename):\n        match = re.match(r'loc-(\\w{2})_', filename)\n        if match:\n            return match.group(1)\n        return 'Unknown'  # Default value if no location is found\n\n    @staticmethod\n    def msd_model(t, D, alpha):\n        return 4 * D * t**alpha\n\n    def calculate_distances(self):\n        \"\"\"\n        Calculate the distances between consecutive frames for each particle in micrometers.\n        \"\"\"\n        self.metrics_df = self.metrics_df.sort_values(by=['unique_id', 'frame'])\n        self.metrics_df[['x_um_prev', 'y_um_prev']] = self.metrics_df.groupby('unique_id')[['x_um', 'y_um']].shift(1)\n        self.metrics_df['segment_len_um'] = np.sqrt(\n            (self.metrics_df['x_um'] - self.metrics_df['x_um_prev'])**2 + \n            (self.metrics_df['y_um'] - self.metrics_df['y_um_prev'])**2\n        )\n        # Fill NaN values with 0\n        self.metrics_df['segment_len_um'] = self.metrics_df['segment_len_um'].fillna(0)\n        return self.metrics_df\n\n    def calculate_speeds(self):\n        \"\"\"\n        Calculate the speed between consecutive frames for each particle in micrometers per second.\n        \"\"\"\n        self.metrics_df[['time_s_prev']] = self.metrics_df.groupby('unique_id')[['time_s']].shift(1)\n        self.metrics_df['delta_time_s'] = self.metrics_df['time_s'] - self.metrics_df['time_s_prev']\n        self.metrics_df['speed_um_s'] = self.metrics_df['segment_len_um'] / self.metrics_df['delta_time_s']\n        # Fill NaN and infinite values with 0\n        self.metrics_df['speed_um_s'] = self.metrics_df['speed_um_s'].replace([np.inf, -np.inf], np.nan).fillna(0)\n        return self.metrics_df\n\n    def calculate_directions(self):\n        \"\"\"\n        Calculate the direction of motion between consecutive frames for each particle in radians.\n        \"\"\"\n        self.metrics_df['direction_rad'] = np.arctan2(\n            self.metrics_df['y_um'] - self.metrics_df['y_um_prev'],\n            self.metrics_df['x_um'] - self.metrics_df['x_um_prev']\n        )\n        # Fill NaN values with 0\n        self.metrics_df['direction_rad'] = self.metrics_df['direction_rad'].fillna(0)\n        return self.metrics_df\n\n    def calculate_accelerations(self):\n        \"\"\"\n        Calculate the acceleration between consecutive frames for each particle in micrometers per second squared.\n        \"\"\"\n        self.metrics_df[['speed_um_s_prev']] = self.metrics_df.groupby('unique_id')[['speed_um_s']].shift(1)\n        self.metrics_df['acceleration_um_s2'] = (self.metrics_df['speed_um_s'] - self.metrics_df['speed_um_s_prev']) / self.metrics_df['delta_time_s']\n        # Fill NaN and infinite values with 0\n        self.metrics_df['acceleration_um_s2'] = self.metrics_df['acceleration_um_s2'].replace([np.inf, -np.inf], np.nan).fillna(0)\n        return self.metrics_df\n\n    def calculate_jerk(self):\n        \"\"\"\n        Calculate the jerk between consecutive frames for each particle in micrometers per second cubed.\n        \"\"\"\n        self.metrics_df[['acceleration_um_s2_prev']] = self.metrics_df.groupby('unique_id')[['acceleration_um_s2']].shift(1)\n        self.metrics_df['jerk_um_s3'] = (self.metrics_df['acceleration_um_s2'] - self.metrics_df['acceleration_um_s2_prev']) / self.metrics_df['delta_time_s']\n        # Fill NaN and infinite values with 0\n        self.metrics_df['jerk_um_s3'] = self.metrics_df['jerk_um_s3'].replace([np.inf, -np.inf], np.nan).fillna(0)\n        return self.metrics_df\n\n    def calculate_normalized_curvature(self):\n        \"\"\"\n        Calculate the curvature normalized by distance between consecutive frames for each particle.\n        \"\"\"\n        self.metrics_df[['direction_rad_prev']] = self.metrics_df.groupby('unique_id')[['direction_rad']].shift(1)\n        self.metrics_df['normalized_curvature'] = (self.metrics_df['direction_rad'] - self.metrics_df['direction_rad_prev']) / self.metrics_df['segment_len_um']\n        # Fill NaN and infinite values with 0\n        self.metrics_df['normalized_curvature'] = self.metrics_df['normalized_curvature'].replace([np.inf, -np.inf], np.nan).fillna(0)\n        # self.metrics_df['normalized_curvature_deg'] = np.degrees(self.metrics_df['normalized_curvature'])\n        return self.metrics_df\n\n    def calculate_angle_normalized_curvature(self):\n        \"\"\"\n        Calculate the curvature (change in direction) normalized to the range [-pi, pi] between consecutive frames for each particle.\n        \"\"\"\n        self.metrics_df[['direction_rad_prev']] = self.metrics_df.groupby('unique_id')[['direction_rad']].shift(1)\n        self.metrics_df['angle_normalized_curvature'] = self.metrics_df['direction_rad'] - self.metrics_df['direction_rad_prev']\n        # Normalize curvature to the range [-pi, pi]\n        self.metrics_df['angle_normalized_curvature'] = (self.metrics_df['angle_normalized_curvature'] + np.pi) % (2 * np.pi) - np.pi\n        # Fill NaN values with 0\n        self.metrics_df['angle_normalized_curvature'] = self.metrics_df['angle_normalized_curvature'].fillna(0)\n        # Convert columns from radians to degrees\n\n        # self.metrics_df['angle_normalized_curvature_deg'] = np.degrees(self.metrics_df['angle_normalized_curvature'])\n        return self.metrics_df\n\n\n    def calculate_persistence_length(self, track_data):\n        \"\"\"\n        Calculate the persistence length for a given track.\n        Parameters:\n        - track_data: DataFrame containing the track data.\n        Returns:\n        - persistence_length: Calculated persistence length for the track.\n        \"\"\"\n        directions = track_data['direction_rad']\n        # Calculate directional correlation: &lt;cos(theta_i - theta_j)&gt;\n        direction_diffs = directions.diff().dropna()\n        correlation = np.cos(direction_diffs).mean()\n            # Check if correlation is greater than zero before calculating log\n        if correlation &gt; 0:\n            persistence_length = -1 / np.log(correlation)\n        else:\n            persistence_length = np.nan  # Assign NaN if the correlation is zero or negative\n\n\n        # persistence_length = -1 / np.log(correlation) if correlation != 0 else np.nan\n        return persistence_length\n\n\n    def calculate_net_displacement(self):\n        \"\"\"\n        Calculate the net displacement from the starting point for each particle in micrometers.\n        \"\"\"\n        self.metrics_df[['x_um_start', 'y_um_start']] = self.metrics_df.groupby('unique_id')[['x_um', 'y_um']].transform('first')\n        self.metrics_df['net_displacement_um'] = np.sqrt(\n            (self.metrics_df['x_um'] - self.metrics_df['x_um_start'])**2 + \n            (self.metrics_df['y_um'] - self.metrics_df['y_um_start'])**2\n        )\n        return self.metrics_df\n\n    def calculate_instantaneous_diffusion_coefficient(self):\n        \"\"\"\n        Calculate the instantaneous diffusion coefficient for each particle in square micrometers per second.\n        \"\"\"\n        self.metrics_df['instant_diff_coeff'] = self.metrics_df['segment_len_um']**2 / (4 * self.metrics_df['delta_time_s'])\n        # Fill NaN and infinite values with 0\n        self.metrics_df['instant_diff_coeff'] = self.metrics_df['instant_diff_coeff'].replace([np.inf, -np.inf], np.nan).fillna(0)\n        return self.metrics_df\n\n    def calculate_instantaneous_velocity(self):\n        \"\"\"\n        Calculate the instantaneous velocity between consecutive frames for each particle in micrometers per second.\n        \"\"\"\n        self.metrics_df['instant_velocity_x_um_s'] = (self.metrics_df['x_um'] - self.metrics_df['x_um_prev']) / self.metrics_df['delta_time_s']\n        self.metrics_df['instant_velocity_y_um_s'] = (self.metrics_df['y_um'] - self.metrics_df['y_um_prev']) / self.metrics_df['delta_time_s']\n        # Fill NaN and infinite values with 0\n        self.metrics_df['instant_velocity_x_um_s'] = self.metrics_df['instant_velocity_x_um_s'].replace([np.inf, -np.inf], np.nan).fillna(0)\n        self.metrics_df['instant_velocity_y_um_s'] = self.metrics_df['instant_velocity_y_um_s'].replace([np.inf, -np.inf], np.nan).fillna(0)\n        return self.metrics_df\n\n    # def calculate_msd_for_track(self, track_data, max_lagtime):\n    #     \"\"\"\n    #     Calculate the MSD for a single track.\n    #     Parameters:\n    #     - track_data: DataFrame containing the track data\n    #     - max_lagtime: maximum number of frames to consider for lag times\n    #     Returns:\n    #     - avg_msd: average MSD for the track\n    #     - D: diffusion coefficient\n    #     - alpha: anomalous exponent\n    #     - motion_class: classified motion type\n    #     \"\"\"\n    #     n_frames = len(track_data)\n    #     msd_values = np.zeros(max_lagtime)\n    #     counts = np.zeros(max_lagtime)\n\n    #     for lag in range(1, max_lagtime + 1):\n    #         if lag &lt; n_frames:\n    #             displacements = (track_data[['x_um', 'y_um']].iloc[lag:].values - track_data[['x_um', 'y_um']].iloc[:-lag].values) ** 2\n    #             squared_displacements = np.sum(displacements, axis=1)\n    #             msd_values[lag - 1] = np.mean(squared_displacements)\n    #             counts[lag - 1] = len(squared_displacements)\n    #         else:\n    #             break\n\n    #     avg_msd = np.mean(msd_values)  # Calculate the average MSD for the track (units: \u03bcm\u00b2)\n\n    #     # Calculate total time in seconds for the track\n    #     total_time_s = (track_data['time_s'].iloc[-1] - track_data['time_s'].iloc[0])\n    #     lag_times = np.arange(1, max_lagtime + 1) * (total_time_s / (n_frames - 1))\n    #     popt, _ = scipy.optimize.curve_fit(self.msd_model, lag_times, msd_values[:max_lagtime])\n    #     D, alpha = popt[0], popt[1]\n\n    #     # Classify the type of motion\n    #     if alpha &lt; 1:\n    #         motion_class = 'subdiffusive'\n    #     elif alpha &gt; 1:\n    #         motion_class = 'superdiffusive'\n    #     else:\n    #         motion_class = 'normal'\n\n    #     return avg_msd, D, alpha, motion_class\n\n    # def calculate_msd_for_track(self, track_data, max_lagtime, store_msd=False, time_window=None, tolerance=None):\n    #     n_frames = len(track_data)\n    #     msd_values = np.zeros(max_lagtime)\n    #     lag_times = np.zeros(max_lagtime)\n\n    #     for lag in range(1, max_lagtime + 1):\n    #         if lag &lt; n_frames:\n    #             displacements = (track_data[['x_um', 'y_um']].iloc[lag:].values - track_data[['x_um', 'y_um']].iloc[:-lag].values) ** 2\n    #             squared_displacements = np.sum(displacements, axis=1)\n    #             msd_values[lag - 1] = np.mean(squared_displacements)\n    #             lag_times[lag - 1] = lag * self.time_between_frames\n    #         else:\n    #             break\n\n    #     avg_msd = np.mean(msd_values)\n\n    #     # Calculate total time in seconds for the track\n    #     total_time_s = (track_data['time_s'].iloc[-1] - track_data['time_s'].iloc[0])\n    #     lag_times = np.arange(1, max_lagtime + 1) * (total_time_s / (n_frames - 1))\n    #     popt, _ = scipy.optimize.curve_fit(self.msd_model, lag_times, msd_values[:max_lagtime])\n    #     D, alpha = popt[0], popt[1]\n\n    #     # Determine tolerance if not provided\n    #     if tolerance is None:\n    #         if len(self.msd_lagtime_df) &gt; 0:\n    #             alpha_std = self.msd_lagtime_df['anomalous_exponent'].std()\n    #             tolerance = alpha_std / 2  # Use half of the standard deviation\n    #         else:\n    #             tolerance = 0.1  # Default value if no previous data is available\n\n    #     # Print the chosen tolerance for this calculation\n    #     print(f\"Chosen tolerance for unique_id {track_data['unique_id'].iloc[0]} and time_window {time_window}: {tolerance}\")\n\n    #     # Classify the type of motion with tolerance\n    #     if alpha &lt; 1 - tolerance:\n    #         motion_class = 'subdiffusive'\n    #     elif alpha &gt; 1 + tolerance:\n    #         motion_class = 'superdiffusive'\n    #     else:\n    #         motion_class = 'normal'\n\n    #     # Store detailed MSD values and lag times if requested\n    #     if store_msd and time_window is not None:\n    #         msd_records = [\n    #             {\n    #                 'unique_id': track_data['unique_id'].iloc[0],\n    #                 'time_window': time_window,\n    #                 'lag_time': lag,\n    #                 'msd': msd,\n    #                 'diffusion_coefficient': D,\n    #                 'anomalous_exponent': alpha\n    #             }\n    #             for lag, msd in zip(lag_times, msd_values)\n    #         ]\n    #         self.msd_lagtime_df = pd.concat([self.msd_lagtime_df, pd.DataFrame(msd_records)], ignore_index=True)\n\n    #     return avg_msd, D, alpha, motion_class\n\n    def calculate_msd_for_track(self, track_data, max_lagtime, store_msd=False, time_window=None):\n        n_frames = len(track_data)\n        msd_values = np.zeros(max_lagtime)\n        lag_times = np.zeros(max_lagtime)\n\n        for lag in range(1, max_lagtime + 1):\n            if lag &lt; n_frames:\n                displacements = (track_data[['x_um', 'y_um']].iloc[lag:].values - track_data[['x_um', 'y_um']].iloc[:-lag].values) ** 2\n                squared_displacements = np.sum(displacements, axis=1)\n                msd_values[lag - 1] = np.mean(squared_displacements)\n                lag_times[lag - 1] = lag * self.time_between_frames\n            else:\n                break\n\n        avg_msd = np.mean(msd_values)\n\n        # Calculate total time in seconds for the track\n        total_time_s = (track_data['time_s'].iloc[-1] - track_data['time_s'].iloc[0])\n        lag_times = np.arange(1, max_lagtime + 1) * (total_time_s / (n_frames - 1))\n        popt, _ = scipy.optimize.curve_fit(self.msd_model, lag_times, msd_values[:max_lagtime])\n        D, alpha = popt[0], popt[1]\n\n        # Print the chosen tolerance for consistency\n        print(f\"Using consistent tolerance: {self.tolerance}\")\n\n        # Classify the type of motion with consistent tolerance\n        if alpha &lt; 1 - self.tolerance:\n            motion_class = 'subdiffusive'\n        elif alpha &gt; 1 + self.tolerance:\n            motion_class = 'superdiffusive'\n        else:\n            motion_class = 'normal'\n\n        # Store detailed MSD values and lag times if requested\n        if store_msd and time_window is not None:\n            msd_records = [\n                {\n                    'unique_id': track_data['unique_id'].iloc[0],\n                    'time_window': time_window,\n                    'lag_time': lag,\n                    'msd': msd,\n                    'diffusion_coefficient': D,\n                    'anomalous_exponent': alpha\n                }\n                for lag, msd in zip(lag_times, msd_values)\n            ]\n            # self.msd_lagtime_df = pd.concat([self.msd_lagtime_df, pd.DataFrame(msd_records)], ignore_index=True)\n            # Only concatenate if msd_records is not empty\n            if msd_records:\n                msd_records_df = pd.DataFrame(msd_records)\n                self.msd_lagtime_df = pd.concat([self.msd_lagtime_df, msd_records_df], ignore_index=True)\n\n        return avg_msd, D, alpha, motion_class\n\n\n\n\n    def produce_time_averaged_df(self, max_lagtime=None):\n        \"\"\"\n        Produce the time-averaged DataFrame.\n        Parameters:\n        - max_lagtime: maximum number of frames to consider for lag times\n        \"\"\"\n        if max_lagtime is None:\n            max_lagtime = self.calculate_default_max_lagtime()\n\n        time_averaged_list = []\n\n        for unique_id, track_data in tqdm(self.metrics_df.groupby('unique_id'), desc=\"Producing Time-Averaged DataFrame\"):\n            avg_msd, D, alpha, motion_class = self.calculate_msd_for_track(track_data, max_lagtime)\n\n            # Calculate total time in seconds for the track\n            total_time_s = (track_data['time_s'].iloc[-1] - track_data['time_s'].iloc[0])\n\n            # Calculate average instantaneous metrics for the track\n            avg_speed = track_data['speed_um_s'].mean()\n            avg_acceleration = track_data['acceleration_um_s2'].mean()\n            avg_jerk = track_data['jerk_um_s3'].mean()\n            avg_norm_curvature = track_data['normalized_curvature'].mean()\n            avg_angle_norm_curvature = track_data['angle_normalized_curvature'].mean()\n\n            # Add track-level summary information to time_averaged_df\n            start_row = track_data.iloc[0]\n            end_row = track_data.iloc[-1]\n            track_summary = pd.DataFrame({\n                'x_um_start': [start_row['x_um']],\n                'y_um_start': [start_row['y_um']],\n                'x_um_end': [end_row['x_um']],\n                'y_um_end': [end_row['y_um']],\n                'particle': [start_row['particle']],\n                'condition': [start_row['condition']],\n                'filename': [start_row['filename']],\n                'file_id': [start_row['file_id']],\n                'unique_id': [unique_id],\n                'avg_msd': [avg_msd],  # Add the average MSD (units: \u03bcm\u00b2)\n                'n_frames': [len(track_data)],  # Add the number of frames\n                'total_time_s': [total_time_s],  # Add the total time in seconds\n                'Location': [start_row['Location']],  # Add the Location\n                'diffusion_coefficient': [D],  # Add the diffusion coefficient\n                'anomalous_exponent': [alpha],  # Add the anomalous exponent\n                'motion_class': [motion_class],  # Add the motion class\n                'avg_speed_um_s': [avg_speed],  # Add average speed\n                'avg_acceleration_um_s2': [avg_acceleration],  # Add average acceleration\n                'avg_jerk_um_s3': [avg_jerk],  # Add average jerk\n                'avg_normalized_curvature': [avg_norm_curvature],  # Add average normalized curvature\n                'avg_angle_normalized_curvature': [avg_angle_norm_curvature],  # Add average angle normalized curvature\n            })\n\n            time_averaged_list.append(track_summary)\n\n        self.time_averaged_df = pd.concat(time_averaged_list).reset_index(drop=True)\n\n    # def calculate_time_windowed_metrics(self, window_size=None, overlap=None):\n    #     \"\"\"\n    #     Calculate metrics for each time window.\n    #     Parameters:\n    #     - window_size: size of the time window in frames\n    #     - overlap: number of overlapping frames between windows\n    #     \"\"\"\n    #     if window_size is None:\n    #         window_size = self.calculate_default_window_size()\n    #     if overlap is None:\n    #         overlap = int(window_size / 2)  # Default overlap is half the window size\n\n    #     # Print the calculated or provided window size and overlap\n    #     print(f\"Using window size: {window_size} frames: please note, tracks shorter than the window size will be skipped\")\n    #     print(f\"Using overlap: {overlap} frames\")\n\n    #     windowed_list = []\n\n    #     for unique_id, track_data in tqdm(self.metrics_df.groupby('unique_id'), desc=\"Calculating Time-Windowed Metrics\"):\n    #         n_frames = len(track_data)\n\n    #         for start in range(0, n_frames - window_size + 1, window_size - overlap):\n    #             end = start + window_size\n    #             window_data = track_data.iloc[start:end]\n\n    #             if len(window_data) &lt; window_size:\n    #                 continue\n\n    #             # Calculate metrics for the window\n    #             avg_msd, D, alpha, motion_class = self.calculate_msd_for_track(window_data, max_lagtime=min(100, int(window_size / 2)))\n\n    #             # Calculate total time in seconds for the window\n    #             total_time_s = (window_data['time_s'].iloc[-1] - window_data['time_s'].iloc[0])\n\n    #             # Calculate average instantaneous metrics for the window\n    #             avg_speed = window_data['speed_um_s'].mean()\n    #             avg_acceleration = window_data['acceleration_um_s2'].mean()\n    #             avg_jerk = window_data['jerk_um_s3'].mean()\n    #             avg_norm_curvature = window_data['normalized_curvature'].mean()\n    #             avg_angle_norm_curvature = window_data['angle_normalized_curvature'].mean()\n    #             # Calculate persistence length for window\n    #             persistence_length = self.calculate_persistence_length(window_data)\n\n\n    #             # Add window-level summary information to time_windowed_df\n    #             start_row = window_data.iloc[0]\n    #             end_row = window_data.iloc[-1]\n    #             window_summary = pd.DataFrame({\n    #                 'time_window': [start // (window_size - overlap)],\n    #                 'x_um_start': [start_row['x_um']],\n    #                 'y_um_start': [start_row['y_um']],\n    #                 'x_um_end': [end_row['x_um']],\n    #                 'y_um_end': [end_row['y_um']],\n    #                 'particle': [start_row['particle']],\n    #                 'condition': [start_row['condition']],\n    #                 'filename': [start_row['filename']],\n    #                 'file_id': [start_row['file_id']],\n    #                 'unique_id': [unique_id],\n    #                 'avg_msd': [avg_msd],  # Add the average MSD (units: \u03bcm\u00b2)\n    #                 'n_frames': [window_size],  # Add the number of frames\n    #                 'total_time_s': [total_time_s],  # Add the total time in seconds\n    #                 'Location': [start_row['Location']],  # Add the Location\n    #                 'diffusion_coefficient': [D],  # Add the diffusion coefficient\n    #                 'anomalous_exponent': [alpha],  # Add the anomalous exponent\n    #                 'motion_class': [motion_class],  # Add the motion class\n    #                 'avg_speed_um_s': [avg_speed],  # Add average speed\n    #                 'avg_acceleration_um_s2': [avg_acceleration],  # Add average acceleration\n    #                 'avg_jerk_um_s3': [avg_jerk],  # Add average jerk\n    #                 'avg_normalized_curvature': [avg_norm_curvature],  # Add average normalized curvature\n    #                 'avg_angle_normalized_curvature': [avg_angle_norm_curvature],  # Add average angle normalized curvature\n    #                 'persistence_length': [persistence_length],  # Add persistence length\n    #             })\n\n    #             windowed_list.append(window_summary)\n\n    #     self.time_windowed_df = pd.concat(windowed_list).reset_index(drop=True)\n\n    #     # Use the instance variable for frame duration\n    #     self.time_windowed_df['time_s'] = self.time_windowed_df['time_window'] * (window_size - overlap) * self.time_between_frames #This added to translate time windows into seconds\n\n    def calculate_time_windowed_metrics(self, window_size=None, overlap=None):\n        if window_size is None:\n            window_size = self.calculate_default_window_size()\n        if overlap is None:\n            overlap = int(window_size / 2)\n\n        windowed_list = []\n\n        for unique_id, track_data in tqdm(self.metrics_df.groupby('unique_id'), desc=\"Calculating Time-Windowed Metrics\"):\n            n_frames = len(track_data)\n\n            for start in range(0, n_frames - window_size + 1, window_size - overlap):\n                end = start + window_size\n                window_data = track_data.iloc[start:end]\n\n                if len(window_data) &lt; window_size:\n                    continue\n\n                # Calculate metrics for the window and store MSD details\n                avg_msd, D, alpha, motion_class = self.calculate_msd_for_track(\n                    window_data, max_lagtime=min(100, int(window_size / 2)), store_msd=True, time_window=start // (window_size - overlap)\n                )\n\n                # Calculate total time in seconds for the window\n                total_time_s = (window_data['time_s'].iloc[-1] - window_data['time_s'].iloc[0])\n\n                # Calculate average instantaneous metrics for the window\n                avg_speed = window_data['speed_um_s'].mean()\n                avg_acceleration = window_data['acceleration_um_s2'].mean()\n                avg_jerk = window_data['jerk_um_s3'].mean()\n                avg_norm_curvature = window_data['normalized_curvature'].mean()\n                avg_angle_norm_curvature = window_data['angle_normalized_curvature'].mean()\n                persistence_length = self.calculate_persistence_length(window_data)\n\n                # Add window-level summary information to time_windowed_df\n                start_row = window_data.iloc[0]\n                end_row = window_data.iloc[-1]\n                window_summary = pd.DataFrame({\n                    'time_window': [start // (window_size - overlap)],\n                    'x_um_start': [start_row['x_um']],\n                    'y_um_start': [start_row['y_um']],\n                    'x_um_end': [end_row['x_um']],\n                    'y_um_end': [end_row['y_um']],\n                    'particle': [start_row['particle']],\n                    'condition': [start_row['condition']],\n                    'filename': [start_row['filename']],\n                    'file_id': [start_row['file_id']],\n                    'unique_id': [unique_id],\n                    'avg_msd': [avg_msd],\n                    'n_frames': [window_size],\n                    'total_time_s': [total_time_s],\n                    'Location': [start_row['Location']],\n                    'diffusion_coefficient': [D],\n                    'anomalous_exponent': [alpha],\n                    'motion_class': [motion_class],\n                    'avg_speed_um_s': [avg_speed],\n                    'avg_acceleration_um_s2': [avg_acceleration],\n                    'avg_jerk_um_s3': [avg_jerk],\n                    'avg_normalized_curvature': [avg_norm_curvature],\n                    'avg_angle_normalized_curvature': [avg_angle_norm_curvature],\n                    'persistence_length': [persistence_length],\n                })\n\n                windowed_list.append(window_summary)\n\n        self.time_windowed_df = pd.concat(windowed_list).reset_index(drop=True)\n\n        # Use the instance variable for frame duration\n        self.time_windowed_df['time_s'] = self.time_windowed_df['time_window'] * (window_size - overlap) * self.time_between_frames\n\n\n    def calculate_metrics_for_window(self, window_data):\n        \"\"\"\n        Calculate metrics for a given window of data.\n        \"\"\"\n        n_frames = len(window_data)\n        max_lagtime = min(100, int(n_frames / 2))  # Example: use half the length of the window or 100, whichever is smaller\n\n        msd_values = np.zeros(max_lagtime)\n\n        for lag in range(1, max_lagtime + 1):\n            if lag &lt; n_frames:\n                displacements = (window_data[['x_um', 'y_um']].iloc[lag:].values - window_data[['x_um', 'y_um']].iloc[:-lag].values) ** 2\n                squared_displacements = np.sum(displacements, axis=1)\n                msd_values[lag - 1] = np.mean(squared_displacements)\n            else:\n                break\n\n        avg_msd = np.mean(msd_values)  # Calculate the average MSD for the window (units: \u03bcm\u00b2)\n\n        # Fit MSD data to determine the type of motion and extract parameters\n        total_time_s = (window_data['time_s'].iloc[-1] - window_data['time_s'].iloc[0])\n        lag_times = np.arange(1, max_lagtime + 1) * (total_time_s / (n_frames - 1))\n        popt, _ = scipy.optimize.curve_fit(self.msd_model, lag_times, msd_values[:max_lagtime])\n        D, alpha = popt[0], popt[1]\n\n        # Classify the type of motion\n        if alpha &lt; 1:\n            motion_class = 'subdiffusive'\n        elif alpha &gt; 1:\n            motion_class = 'superdiffusive'\n        else:\n            motion_class = 'normal'\n\n        return avg_msd, D, alpha, motion_class\n\n    def calculate_default_window_size(self):\n        \"\"\"\n        Calculate the default window size based on the average length of tracks in the dataset.\n        \"\"\"\n        avg_track_length = self.metrics_df.groupby('unique_id').size().mean()\n        default_window_size = int(avg_track_length / 2)  # Example: use half the average length of the tracks\n        return default_window_size\n\n    def calculate_all_features(self, max_lagtime=None, calculate_time_windowed=False):\n        \"\"\"\n        Calculate all features for the particle tracking data.\n        This method will call all individual feature calculation methods.\n        Parameters:\n        - max_lagtime: maximum number of frames to consider for lag times\n        - calculate_time_windowed: boolean flag to indicate if time-windowed metrics should be calculated\n        \"\"\"\n        # Calculate default max lag time if not provided\n        if max_lagtime is None:\n            max_lagtime = self.calculate_default_max_lagtime()\n\n        # Calculate distances between consecutive frames\n        self.calculate_distances()\n\n        # Calculate speeds between consecutive frames\n        self.calculate_speeds()\n\n        # Calculate directions of motion between consecutive frames\n        self.calculate_directions()\n\n        # Calculate accelerations between consecutive frames\n        self.calculate_accelerations()\n\n        # Calculate jerks between consecutive frames\n        self.calculate_jerk()\n\n        # Calculate curvatures between consecutive frames\n        self.calculate_normalized_curvature()\n        self.calculate_angle_normalized_curvature()\n\n        # Calculate instantaneous diffusion coefficient between consecutive frames\n        self.calculate_instantaneous_diffusion_coefficient()\n\n        # Calculate net displacement\n        self.calculate_net_displacement()\n\n        # Calculate MSD for each track and aggregate\n        self.calculate_msd_for_all_tracks(max_lagtime)\n\n        # Calculate instantaneous velocity\n        self.calculate_instantaneous_velocity()\n\n        # Calculate time-windowed metrics if requested\n        if calculate_time_windowed:\n            self.calculate_time_windowed_metrics()\n\n        # Cleanup step to remove temporary columns\n        self.cleanup()\n\n        return self.metrics_df\n\n    def get_time_averaged_df(self):\n        \"\"\"\n        Return the DataFrame with time-averaged metrics.\n        \"\"\"\n        return self.time_averaged_df\n\n    def get_time_windowed_df(self):\n        \"\"\"\n        Return the DataFrame with time-windowed metrics.\n        \"\"\"\n        return self.time_windowed_df\n\n    def calculate_default_max_lagtime(self):\n        \"\"\"\n        Calculate the default maximum lag time based on the shortest track in the dataset.\n        \"\"\"\n        min_track_length = self.metrics_df.groupby('unique_id').size().min()\n        default_max_lagtime = min(100, int(min_track_length / 2))  # Example: use half the length of the shortest track or 100, whichever is smaller\n        return default_max_lagtime\n\n    def calculate_msd_for_all_tracks(self, max_lagtime):\n        \"\"\"\n        Calculate the MSD for all tracks and produce the time-averaged DataFrame.\n        Parameters:\n        - max_lagtime: maximum number of frames to consider for lag times\n        \"\"\"\n        self.produce_time_averaged_df(max_lagtime)\n\n    def cleanup(self):\n        \"\"\"\n        Cleanup the dataframe by dropping unnecessary columns after all features are calculated.\n        \"\"\"\n        self.metrics_df.drop(columns=[\n            'x_um_prev', 'y_um_prev', 'time_s_prev', 'delta_time_s', \n            'speed_um_s_prev', 'acceleration_um_s2_prev', 'direction_rad_prev',\n            'instant_velocity_x_um_s', 'instant_velocity_y_um_s',\n        ], inplace=True)\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_accelerations","title":"<code>calculate_accelerations()</code>","text":"<p>Calculate the acceleration between consecutive frames for each particle in micrometers per second squared.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_accelerations(self):\n    \"\"\"\n    Calculate the acceleration between consecutive frames for each particle in micrometers per second squared.\n    \"\"\"\n    self.metrics_df[['speed_um_s_prev']] = self.metrics_df.groupby('unique_id')[['speed_um_s']].shift(1)\n    self.metrics_df['acceleration_um_s2'] = (self.metrics_df['speed_um_s'] - self.metrics_df['speed_um_s_prev']) / self.metrics_df['delta_time_s']\n    # Fill NaN and infinite values with 0\n    self.metrics_df['acceleration_um_s2'] = self.metrics_df['acceleration_um_s2'].replace([np.inf, -np.inf], np.nan).fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_all_features","title":"<code>calculate_all_features(max_lagtime=None, calculate_time_windowed=False)</code>","text":"<p>Calculate all features for the particle tracking data. This method will call all individual feature calculation methods. Parameters: - max_lagtime: maximum number of frames to consider for lag times - calculate_time_windowed: boolean flag to indicate if time-windowed metrics should be calculated</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_all_features(self, max_lagtime=None, calculate_time_windowed=False):\n    \"\"\"\n    Calculate all features for the particle tracking data.\n    This method will call all individual feature calculation methods.\n    Parameters:\n    - max_lagtime: maximum number of frames to consider for lag times\n    - calculate_time_windowed: boolean flag to indicate if time-windowed metrics should be calculated\n    \"\"\"\n    # Calculate default max lag time if not provided\n    if max_lagtime is None:\n        max_lagtime = self.calculate_default_max_lagtime()\n\n    # Calculate distances between consecutive frames\n    self.calculate_distances()\n\n    # Calculate speeds between consecutive frames\n    self.calculate_speeds()\n\n    # Calculate directions of motion between consecutive frames\n    self.calculate_directions()\n\n    # Calculate accelerations between consecutive frames\n    self.calculate_accelerations()\n\n    # Calculate jerks between consecutive frames\n    self.calculate_jerk()\n\n    # Calculate curvatures between consecutive frames\n    self.calculate_normalized_curvature()\n    self.calculate_angle_normalized_curvature()\n\n    # Calculate instantaneous diffusion coefficient between consecutive frames\n    self.calculate_instantaneous_diffusion_coefficient()\n\n    # Calculate net displacement\n    self.calculate_net_displacement()\n\n    # Calculate MSD for each track and aggregate\n    self.calculate_msd_for_all_tracks(max_lagtime)\n\n    # Calculate instantaneous velocity\n    self.calculate_instantaneous_velocity()\n\n    # Calculate time-windowed metrics if requested\n    if calculate_time_windowed:\n        self.calculate_time_windowed_metrics()\n\n    # Cleanup step to remove temporary columns\n    self.cleanup()\n\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_angle_normalized_curvature","title":"<code>calculate_angle_normalized_curvature()</code>","text":"<p>Calculate the curvature (change in direction) normalized to the range [-pi, pi] between consecutive frames for each particle.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_angle_normalized_curvature(self):\n    \"\"\"\n    Calculate the curvature (change in direction) normalized to the range [-pi, pi] between consecutive frames for each particle.\n    \"\"\"\n    self.metrics_df[['direction_rad_prev']] = self.metrics_df.groupby('unique_id')[['direction_rad']].shift(1)\n    self.metrics_df['angle_normalized_curvature'] = self.metrics_df['direction_rad'] - self.metrics_df['direction_rad_prev']\n    # Normalize curvature to the range [-pi, pi]\n    self.metrics_df['angle_normalized_curvature'] = (self.metrics_df['angle_normalized_curvature'] + np.pi) % (2 * np.pi) - np.pi\n    # Fill NaN values with 0\n    self.metrics_df['angle_normalized_curvature'] = self.metrics_df['angle_normalized_curvature'].fillna(0)\n    # Convert columns from radians to degrees\n\n    # self.metrics_df['angle_normalized_curvature_deg'] = np.degrees(self.metrics_df['angle_normalized_curvature'])\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_default_max_lagtime","title":"<code>calculate_default_max_lagtime()</code>","text":"<p>Calculate the default maximum lag time based on the shortest track in the dataset.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_default_max_lagtime(self):\n    \"\"\"\n    Calculate the default maximum lag time based on the shortest track in the dataset.\n    \"\"\"\n    min_track_length = self.metrics_df.groupby('unique_id').size().min()\n    default_max_lagtime = min(100, int(min_track_length / 2))  # Example: use half the length of the shortest track or 100, whichever is smaller\n    return default_max_lagtime\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_default_window_size","title":"<code>calculate_default_window_size()</code>","text":"<p>Calculate the default window size based on the average length of tracks in the dataset.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_default_window_size(self):\n    \"\"\"\n    Calculate the default window size based on the average length of tracks in the dataset.\n    \"\"\"\n    avg_track_length = self.metrics_df.groupby('unique_id').size().mean()\n    default_window_size = int(avg_track_length / 2)  # Example: use half the average length of the tracks\n    return default_window_size\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_directions","title":"<code>calculate_directions()</code>","text":"<p>Calculate the direction of motion between consecutive frames for each particle in radians.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_directions(self):\n    \"\"\"\n    Calculate the direction of motion between consecutive frames for each particle in radians.\n    \"\"\"\n    self.metrics_df['direction_rad'] = np.arctan2(\n        self.metrics_df['y_um'] - self.metrics_df['y_um_prev'],\n        self.metrics_df['x_um'] - self.metrics_df['x_um_prev']\n    )\n    # Fill NaN values with 0\n    self.metrics_df['direction_rad'] = self.metrics_df['direction_rad'].fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_distances","title":"<code>calculate_distances()</code>","text":"<p>Calculate the distances between consecutive frames for each particle in micrometers.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_distances(self):\n    \"\"\"\n    Calculate the distances between consecutive frames for each particle in micrometers.\n    \"\"\"\n    self.metrics_df = self.metrics_df.sort_values(by=['unique_id', 'frame'])\n    self.metrics_df[['x_um_prev', 'y_um_prev']] = self.metrics_df.groupby('unique_id')[['x_um', 'y_um']].shift(1)\n    self.metrics_df['segment_len_um'] = np.sqrt(\n        (self.metrics_df['x_um'] - self.metrics_df['x_um_prev'])**2 + \n        (self.metrics_df['y_um'] - self.metrics_df['y_um_prev'])**2\n    )\n    # Fill NaN values with 0\n    self.metrics_df['segment_len_um'] = self.metrics_df['segment_len_um'].fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_instantaneous_diffusion_coefficient","title":"<code>calculate_instantaneous_diffusion_coefficient()</code>","text":"<p>Calculate the instantaneous diffusion coefficient for each particle in square micrometers per second.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_instantaneous_diffusion_coefficient(self):\n    \"\"\"\n    Calculate the instantaneous diffusion coefficient for each particle in square micrometers per second.\n    \"\"\"\n    self.metrics_df['instant_diff_coeff'] = self.metrics_df['segment_len_um']**2 / (4 * self.metrics_df['delta_time_s'])\n    # Fill NaN and infinite values with 0\n    self.metrics_df['instant_diff_coeff'] = self.metrics_df['instant_diff_coeff'].replace([np.inf, -np.inf], np.nan).fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_instantaneous_velocity","title":"<code>calculate_instantaneous_velocity()</code>","text":"<p>Calculate the instantaneous velocity between consecutive frames for each particle in micrometers per second.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_instantaneous_velocity(self):\n    \"\"\"\n    Calculate the instantaneous velocity between consecutive frames for each particle in micrometers per second.\n    \"\"\"\n    self.metrics_df['instant_velocity_x_um_s'] = (self.metrics_df['x_um'] - self.metrics_df['x_um_prev']) / self.metrics_df['delta_time_s']\n    self.metrics_df['instant_velocity_y_um_s'] = (self.metrics_df['y_um'] - self.metrics_df['y_um_prev']) / self.metrics_df['delta_time_s']\n    # Fill NaN and infinite values with 0\n    self.metrics_df['instant_velocity_x_um_s'] = self.metrics_df['instant_velocity_x_um_s'].replace([np.inf, -np.inf], np.nan).fillna(0)\n    self.metrics_df['instant_velocity_y_um_s'] = self.metrics_df['instant_velocity_y_um_s'].replace([np.inf, -np.inf], np.nan).fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_jerk","title":"<code>calculate_jerk()</code>","text":"<p>Calculate the jerk between consecutive frames for each particle in micrometers per second cubed.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_jerk(self):\n    \"\"\"\n    Calculate the jerk between consecutive frames for each particle in micrometers per second cubed.\n    \"\"\"\n    self.metrics_df[['acceleration_um_s2_prev']] = self.metrics_df.groupby('unique_id')[['acceleration_um_s2']].shift(1)\n    self.metrics_df['jerk_um_s3'] = (self.metrics_df['acceleration_um_s2'] - self.metrics_df['acceleration_um_s2_prev']) / self.metrics_df['delta_time_s']\n    # Fill NaN and infinite values with 0\n    self.metrics_df['jerk_um_s3'] = self.metrics_df['jerk_um_s3'].replace([np.inf, -np.inf], np.nan).fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_metrics_for_window","title":"<code>calculate_metrics_for_window(window_data)</code>","text":"<p>Calculate metrics for a given window of data.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_metrics_for_window(self, window_data):\n    \"\"\"\n    Calculate metrics for a given window of data.\n    \"\"\"\n    n_frames = len(window_data)\n    max_lagtime = min(100, int(n_frames / 2))  # Example: use half the length of the window or 100, whichever is smaller\n\n    msd_values = np.zeros(max_lagtime)\n\n    for lag in range(1, max_lagtime + 1):\n        if lag &lt; n_frames:\n            displacements = (window_data[['x_um', 'y_um']].iloc[lag:].values - window_data[['x_um', 'y_um']].iloc[:-lag].values) ** 2\n            squared_displacements = np.sum(displacements, axis=1)\n            msd_values[lag - 1] = np.mean(squared_displacements)\n        else:\n            break\n\n    avg_msd = np.mean(msd_values)  # Calculate the average MSD for the window (units: \u03bcm\u00b2)\n\n    # Fit MSD data to determine the type of motion and extract parameters\n    total_time_s = (window_data['time_s'].iloc[-1] - window_data['time_s'].iloc[0])\n    lag_times = np.arange(1, max_lagtime + 1) * (total_time_s / (n_frames - 1))\n    popt, _ = scipy.optimize.curve_fit(self.msd_model, lag_times, msd_values[:max_lagtime])\n    D, alpha = popt[0], popt[1]\n\n    # Classify the type of motion\n    if alpha &lt; 1:\n        motion_class = 'subdiffusive'\n    elif alpha &gt; 1:\n        motion_class = 'superdiffusive'\n    else:\n        motion_class = 'normal'\n\n    return avg_msd, D, alpha, motion_class\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_msd_for_all_tracks","title":"<code>calculate_msd_for_all_tracks(max_lagtime)</code>","text":"<p>Calculate the MSD for all tracks and produce the time-averaged DataFrame. Parameters: - max_lagtime: maximum number of frames to consider for lag times</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_msd_for_all_tracks(self, max_lagtime):\n    \"\"\"\n    Calculate the MSD for all tracks and produce the time-averaged DataFrame.\n    Parameters:\n    - max_lagtime: maximum number of frames to consider for lag times\n    \"\"\"\n    self.produce_time_averaged_df(max_lagtime)\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_net_displacement","title":"<code>calculate_net_displacement()</code>","text":"<p>Calculate the net displacement from the starting point for each particle in micrometers.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_net_displacement(self):\n    \"\"\"\n    Calculate the net displacement from the starting point for each particle in micrometers.\n    \"\"\"\n    self.metrics_df[['x_um_start', 'y_um_start']] = self.metrics_df.groupby('unique_id')[['x_um', 'y_um']].transform('first')\n    self.metrics_df['net_displacement_um'] = np.sqrt(\n        (self.metrics_df['x_um'] - self.metrics_df['x_um_start'])**2 + \n        (self.metrics_df['y_um'] - self.metrics_df['y_um_start'])**2\n    )\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_normalized_curvature","title":"<code>calculate_normalized_curvature()</code>","text":"<p>Calculate the curvature normalized by distance between consecutive frames for each particle.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_normalized_curvature(self):\n    \"\"\"\n    Calculate the curvature normalized by distance between consecutive frames for each particle.\n    \"\"\"\n    self.metrics_df[['direction_rad_prev']] = self.metrics_df.groupby('unique_id')[['direction_rad']].shift(1)\n    self.metrics_df['normalized_curvature'] = (self.metrics_df['direction_rad'] - self.metrics_df['direction_rad_prev']) / self.metrics_df['segment_len_um']\n    # Fill NaN and infinite values with 0\n    self.metrics_df['normalized_curvature'] = self.metrics_df['normalized_curvature'].replace([np.inf, -np.inf], np.nan).fillna(0)\n    # self.metrics_df['normalized_curvature_deg'] = np.degrees(self.metrics_df['normalized_curvature'])\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_persistence_length","title":"<code>calculate_persistence_length(track_data)</code>","text":"<p>Calculate the persistence length for a given track. Parameters: - track_data: DataFrame containing the track data. Returns: - persistence_length: Calculated persistence length for the track.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_persistence_length(self, track_data):\n    \"\"\"\n    Calculate the persistence length for a given track.\n    Parameters:\n    - track_data: DataFrame containing the track data.\n    Returns:\n    - persistence_length: Calculated persistence length for the track.\n    \"\"\"\n    directions = track_data['direction_rad']\n    # Calculate directional correlation: &lt;cos(theta_i - theta_j)&gt;\n    direction_diffs = directions.diff().dropna()\n    correlation = np.cos(direction_diffs).mean()\n        # Check if correlation is greater than zero before calculating log\n    if correlation &gt; 0:\n        persistence_length = -1 / np.log(correlation)\n    else:\n        persistence_length = np.nan  # Assign NaN if the correlation is zero or negative\n\n\n    # persistence_length = -1 / np.log(correlation) if correlation != 0 else np.nan\n    return persistence_length\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_speeds","title":"<code>calculate_speeds()</code>","text":"<p>Calculate the speed between consecutive frames for each particle in micrometers per second.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_speeds(self):\n    \"\"\"\n    Calculate the speed between consecutive frames for each particle in micrometers per second.\n    \"\"\"\n    self.metrics_df[['time_s_prev']] = self.metrics_df.groupby('unique_id')[['time_s']].shift(1)\n    self.metrics_df['delta_time_s'] = self.metrics_df['time_s'] - self.metrics_df['time_s_prev']\n    self.metrics_df['speed_um_s'] = self.metrics_df['segment_len_um'] / self.metrics_df['delta_time_s']\n    # Fill NaN and infinite values with 0\n    self.metrics_df['speed_um_s'] = self.metrics_df['speed_um_s'].replace([np.inf, -np.inf], np.nan).fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.cleanup","title":"<code>cleanup()</code>","text":"<p>Cleanup the dataframe by dropping unnecessary columns after all features are calculated.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def cleanup(self):\n    \"\"\"\n    Cleanup the dataframe by dropping unnecessary columns after all features are calculated.\n    \"\"\"\n    self.metrics_df.drop(columns=[\n        'x_um_prev', 'y_um_prev', 'time_s_prev', 'delta_time_s', \n        'speed_um_s_prev', 'acceleration_um_s2_prev', 'direction_rad_prev',\n        'instant_velocity_x_um_s', 'instant_velocity_y_um_s',\n    ], inplace=True)\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.get_time_averaged_df","title":"<code>get_time_averaged_df()</code>","text":"<p>Return the DataFrame with time-averaged metrics.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def get_time_averaged_df(self):\n    \"\"\"\n    Return the DataFrame with time-averaged metrics.\n    \"\"\"\n    return self.time_averaged_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.get_time_windowed_df","title":"<code>get_time_windowed_df()</code>","text":"<p>Return the DataFrame with time-windowed metrics.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def get_time_windowed_df(self):\n    \"\"\"\n    Return the DataFrame with time-windowed metrics.\n    \"\"\"\n    return self.time_windowed_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.produce_time_averaged_df","title":"<code>produce_time_averaged_df(max_lagtime=None)</code>","text":"<p>Produce the time-averaged DataFrame. Parameters: - max_lagtime: maximum number of frames to consider for lag times</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def produce_time_averaged_df(self, max_lagtime=None):\n    \"\"\"\n    Produce the time-averaged DataFrame.\n    Parameters:\n    - max_lagtime: maximum number of frames to consider for lag times\n    \"\"\"\n    if max_lagtime is None:\n        max_lagtime = self.calculate_default_max_lagtime()\n\n    time_averaged_list = []\n\n    for unique_id, track_data in tqdm(self.metrics_df.groupby('unique_id'), desc=\"Producing Time-Averaged DataFrame\"):\n        avg_msd, D, alpha, motion_class = self.calculate_msd_for_track(track_data, max_lagtime)\n\n        # Calculate total time in seconds for the track\n        total_time_s = (track_data['time_s'].iloc[-1] - track_data['time_s'].iloc[0])\n\n        # Calculate average instantaneous metrics for the track\n        avg_speed = track_data['speed_um_s'].mean()\n        avg_acceleration = track_data['acceleration_um_s2'].mean()\n        avg_jerk = track_data['jerk_um_s3'].mean()\n        avg_norm_curvature = track_data['normalized_curvature'].mean()\n        avg_angle_norm_curvature = track_data['angle_normalized_curvature'].mean()\n\n        # Add track-level summary information to time_averaged_df\n        start_row = track_data.iloc[0]\n        end_row = track_data.iloc[-1]\n        track_summary = pd.DataFrame({\n            'x_um_start': [start_row['x_um']],\n            'y_um_start': [start_row['y_um']],\n            'x_um_end': [end_row['x_um']],\n            'y_um_end': [end_row['y_um']],\n            'particle': [start_row['particle']],\n            'condition': [start_row['condition']],\n            'filename': [start_row['filename']],\n            'file_id': [start_row['file_id']],\n            'unique_id': [unique_id],\n            'avg_msd': [avg_msd],  # Add the average MSD (units: \u03bcm\u00b2)\n            'n_frames': [len(track_data)],  # Add the number of frames\n            'total_time_s': [total_time_s],  # Add the total time in seconds\n            'Location': [start_row['Location']],  # Add the Location\n            'diffusion_coefficient': [D],  # Add the diffusion coefficient\n            'anomalous_exponent': [alpha],  # Add the anomalous exponent\n            'motion_class': [motion_class],  # Add the motion class\n            'avg_speed_um_s': [avg_speed],  # Add average speed\n            'avg_acceleration_um_s2': [avg_acceleration],  # Add average acceleration\n            'avg_jerk_um_s3': [avg_jerk],  # Add average jerk\n            'avg_normalized_curvature': [avg_norm_curvature],  # Add average normalized curvature\n            'avg_angle_normalized_curvature': [avg_angle_norm_curvature],  # Add average angle normalized curvature\n        })\n\n        time_averaged_list.append(track_summary)\n\n    self.time_averaged_df = pd.concat(time_averaged_list).reset_index(drop=True)\n</code></pre>"},{"location":"api/#SPTnano.helper_scripts","title":"<code>helper_scripts</code>","text":""},{"location":"api/#SPTnano.helper_scripts.add_microns_and_secs","title":"<code>add_microns_and_secs(df, pixelsize_microns, time_between_frames)</code>","text":"<p>Adds columns to the DataFrame with positions in microns and time in seconds</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def add_microns_and_secs(df, pixelsize_microns, time_between_frames):\n    '''Adds columns to the DataFrame with positions in microns and time in seconds'''\n    #space transformations\n    df['x_um'] = df['x'] * pixelsize_microns\n    df['y_um'] = df['y'] * pixelsize_microns\n\n    df['frame_zeroed'] = df.groupby('particle')['frame'].transform(lambda x: x - x.iloc[0])\n    df['time_s'] = df['frame'] * time_between_frames\n    df['time_s_zeroed'] = df.groupby('particle')['time_s'].transform(lambda x: x - x.iloc[0])\n    return df\n</code></pre>"},{"location":"api/#SPTnano.helper_scripts.filter_high_speeds","title":"<code>filter_high_speeds(metrics_df, speed_threshold)</code>","text":"<p>Filter based on speed instead - can be relevant if you have different exposure times and different times between frames</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def filter_high_speeds(metrics_df, speed_threshold):\n    '''\n    Filter based on speed instead - can be relevant if you have different exposure times and different times between frames\n    '''\n\n    # Identify unique_ids with any high speeds\n    high_speed_particles = metrics_df[metrics_df['speed_um_s'] &gt; speed_threshold]['unique_id'].unique()\n\n    # Filter out particles with high speeds\n    metrics_df_filtered = metrics_df[~metrics_df['unique_id'].isin(high_speed_particles)].copy()\n    return metrics_df_filtered\n</code></pre>"},{"location":"api/#SPTnano.helper_scripts.filter_large_jumps","title":"<code>filter_large_jumps(df, threshold)</code>","text":"<p>Filter out entire particles with any frames showing large jumps in micrometers.</p>"},{"location":"api/#SPTnano.helper_scripts.filter_large_jumps--parameters","title":"Parameters","text":"<p>df : DataFrame     DataFrame containing tracking data with a 'segment_len_um' column. threshold : float     Threshold for what constitutes a large jump in micrometers.</p>"},{"location":"api/#SPTnano.helper_scripts.filter_large_jumps--returns","title":"Returns","text":"<p>DataFrame     DataFrame with particles having large jumps filtered out.</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def filter_large_jumps(df, threshold):\n    \"\"\"\n    Filter out entire particles with any frames showing large jumps in micrometers.\n\n    Parameters\n    ----------\n    df : DataFrame\n        DataFrame containing tracking data with a 'segment_len_um' column.\n    threshold : float\n        Threshold for what constitutes a large jump in micrometers.\n\n    Returns\n    -------\n    DataFrame\n        DataFrame with particles having large jumps filtered out.\n    \"\"\"\n    # Identify unique_ids with any large jumps\n    large_jump_particles = df[df['segment_len_um'] &gt; threshold]['unique_id'].unique()\n\n    # Filter out particles with large jumps\n    df_filtered = df[~df['unique_id'].isin(large_jump_particles)].copy()\n    # df_filtered.drop(columns=['x_um_prev', 'y_um_prev', 'segment_len_um'], inplace=True)\n    return df_filtered\n</code></pre>"},{"location":"api/#SPTnano.helper_scripts.filter_stubs","title":"<code>filter_stubs(df, min_time)</code>","text":"<p>Removes tracks that are shorter than 'min_time' by finding the max duration of each time_s_zeroed column and filtering on that Works across exposure times, because it works on converted seconds, not frames</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def filter_stubs(df, min_time):\n\n    '''\n    Removes tracks that are shorter than 'min_time' by finding the max duration of each time_s_zeroed column and filtering on that\n    Works across exposure times, because it works on converted seconds, not frames\n\n    '''\n    # Calculate the duration of each track by grouping by 'particle' and using the 'time_s' column\n    track_durations = df.groupby('unique_id')['time_s_zeroed'].max() \n    # Identify particles with tracks longer than 0.2 seconds\n    valid_particles = track_durations[track_durations &gt;= min_time].index\n    # Filter the dataframe to include only valid particles\n    filtered_df = df[df['unique_id'].isin(valid_particles)]\n\n    return filtered_df\n</code></pre>"},{"location":"api/#SPTnano.visualization","title":"<code>visualization</code>","text":""},{"location":"api/#SPTnano.visualization.batch_plot_trajectories","title":"<code>batch_plot_trajectories(master_folder, traj_df, batch=True, filename=None, colorby='particle', mpp=None, label=False, cmap=None)</code>","text":"<p>Batch plot trajectories for all replicates across several conditions.</p>"},{"location":"api/#SPTnano.visualization.batch_plot_trajectories--parameters","title":"Parameters","text":"<p>master_folder : str     Path to the master folder containing 'data' and 'saved_data' folders. traj_df : DataFrame     The DataFrame containing trajectory data. batch : bool, optional     If True, plots trajectories for all replicates in batch mode.     If False, plots trajectory for the specified filename. filename : str, optional     Filename of interest when batch is False. colorby : str, optional     Color by 'particle' or 'frame'. mpp : float, optional     Microns per pixel. label : bool, optional     Set to True to write particle ID numbers next to trajectories. cmap : colormap, optional     Colormap to use for coloring tracks.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def batch_plot_trajectories(master_folder, traj_df, batch=True, filename=None, colorby='particle', mpp=None, label=False, cmap=None):\n    \"\"\"\n    Batch plot trajectories for all replicates across several conditions.\n\n    Parameters\n    ----------\n    master_folder : str\n        Path to the master folder containing 'data' and 'saved_data' folders.\n    traj_df : DataFrame\n        The DataFrame containing trajectory data.\n    batch : bool, optional\n        If True, plots trajectories for all replicates in batch mode.\n        If False, plots trajectory for the specified filename.\n    filename : str, optional\n        Filename of interest when batch is False.\n    colorby : str, optional\n        Color by 'particle' or 'frame'.\n    mpp : float, optional\n        Microns per pixel.\n    label : bool, optional\n        Set to True to write particle ID numbers next to trajectories.\n    cmap : colormap, optional\n        Colormap to use for coloring tracks.\n    \"\"\"\n    data_folder = os.path.join(master_folder, 'data')\n    vis_folder = os.path.join(master_folder, 'visualization/trajectories')\n    os.makedirs(vis_folder, exist_ok=True)\n\n    if batch:\n        for condition in os.listdir(data_folder):\n            condition_folder = os.path.join(data_folder, condition)\n            if os.path.isdir(condition_folder):\n                for file in os.listdir(condition_folder):\n                    if file.endswith('.tif'):\n                        filepath = os.path.join(condition_folder, file)\n                        subset_traj_df = traj_df[traj_df['filename'] == file]\n                        if not subset_traj_df.empty:\n                            frames = pims.open(filepath)\n                            frame = frames[0]\n                            fig, ax = plt.subplots()\n                            plot_trajectory(subset_traj_df, colorby=colorby, mpp=mpp, label=label, superimpose=frame, cmap=cmap, ax=ax)\n                            plt.savefig(os.path.join(vis_folder, f'{condition}_{file}.png'))\n                            plt.close(fig)\n    else:\n        if filename is not None:\n            filepath = os.path.join(data_folder, filename)\n            subset_traj_df = traj_df[traj_df['filename'] == filename]\n            if not subset_traj_df.empty:\n                frames = pims.open(filepath)\n                frame = frames[0]\n                fig, ax = plt.subplots()\n                plot_trajectory(subset_traj_df, colorby=colorby, mpp=mpp, label=label, superimpose=frame, cmap=cmap, ax=ax)\n                plt.show()\n        else:\n            print(\"Please provide a filename when batch is set to False.\")\n</code></pre>"},{"location":"api/#SPTnano.visualization.plot_barplots","title":"<code>plot_barplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', meanormedian='mean', talk=False)</code>","text":"<p>Plot bar plots of a specified factor, with bootstrapped confidence intervals.</p>"},{"location":"api/#SPTnano.visualization.plot_barplots--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing the data. factor_col : str, optional     The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'. separate_by : str, optional     Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'. palette : str, optional     Color palette for the plot. Default is 'colorblind'. meanormedian : str, optional     Whether to use mean or median for aggregation. Default is 'mean'. talk : bool, optional     Whether to set the figure size to the original large size or a smaller size. Default is False.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_barplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', meanormedian='mean', talk=False):\n    \"\"\"\n    Plot bar plots of a specified factor, with bootstrapped confidence intervals.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing the data.\n    factor_col : str, optional\n        The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'.\n    separate_by : str, optional\n        Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'.\n    palette : str, optional\n        Color palette for the plot. Default is 'colorblind'.\n    meanormedian : str, optional\n        Whether to use mean or median for aggregation. Default is 'mean'.\n    talk : bool, optional\n        Whether to set the figure size to the original large size or a smaller size. Default is False.\n    \"\"\"\n\n    unique_categories = data_df[separate_by].unique() if separate_by else [None]\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    # Set figure size based on the `talk` parameter\n    if talk:\n        fig_size = (20, 12)\n        font_size = 35\n    else:\n        fig_size = (5, 3)\n        font_size = 14\n\n    fig, ax = plt.subplots(figsize=fig_size)\n    sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5, \"font.size\": font_size, \"axes.titlesize\": font_size, \"axes.labelsize\": font_size, \"xtick.labelsize\": font_size, \"ytick.labelsize\": font_size})\n\n    avg_factors_list = []\n    ci_intervals = []\n\n    for i, category in enumerate(unique_categories):\n        subset = data_df if category is None else data_df[data_df[separate_by] == category]\n\n        if meanormedian == 'mean':\n            avg_factors = subset[factor_col].mean()\n            ci_interval = bootstrap_ci_mean(subset[factor_col], num_samples=1000, alpha=0.05)\n        else:\n            avg_factors = subset[factor_col].median()\n            ci_interval = bootstrap_ci_median(subset[factor_col], num_samples=1000, alpha=0.05)\n\n        avg_factors_list.append(avg_factors)\n        ci_intervals.append(ci_interval)\n\n    categories = unique_categories if separate_by else ['Overall']\n    ax.bar(categories, avg_factors_list, yerr=ci_intervals, color=color_palette, capsize=5, edgecolor='black')\n\n    # Remove 'Condition_' prefix from x tick labels\n    new_labels = [label.replace('Condition_', '') for label in categories]\n    if talk:\n        ax.set_xticklabels(new_labels, fontsize=font_size)\n    else:\n        ax.set_xticklabels(new_labels, fontsize=font_size, rotation=90)\n\n\n    ax.set_ylabel(factor_col, fontsize=font_size)\n    ax.tick_params(axis='both', which='major', labelsize=font_size)\n    plt.tight_layout()\n\n    plt.show()\n</code></pre>"},{"location":"api/#SPTnano.visualization.plot_histograms","title":"<code>plot_histograms(data_df, feature, bins=100, separate=None, xlimit=None, small_multiples=False, palette='colorblind', use_kde=False, show_plot=True, master_dir=None, tick_interval=5, average='mean', order=None, grid=False)</code>","text":"<p>Plot histograms or KDEs of a specified feature for each category in <code>separate</code>, with consistent binning.</p>"},{"location":"api/#SPTnano.visualization.plot_histograms--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing track data with the specified feature and optionally a separating column. feature : str     The feature to plot histograms for. bins : int, optional     Number of bins for the histogram. Default is 100. separate : str, optional     Column to separate the data by. If None, all data will be plotted together. Default is None. xlimit : float, optional     Upper limit for the x-axis. Default is None. small_multiples : bool, optional     Whether to plot each category separately as small multiples. Default is False. palette : str, optional     Color palette for the plot. Default is 'colorblind'. use_kde : bool, optional     Whether to use KDE plot instead of histogram. Default is False. show_plot : bool, optional     Whether to display the plot in the notebook. Default is True. master_dir : str, optional     The directory where the plots folder will be created and the plot will be saved. Default is None. tick_interval : int, optional     Interval for x-axis ticks. Default is 5. average : str, optional     Whether to draw 'mean' or 'median' line on the plot. Default is 'mean'. order : list, optional     Specific order for the conditions. Default is None.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_histograms(data_df, feature, bins=100, separate=None, xlimit=None, small_multiples=False, palette='colorblind', use_kde=False, show_plot=True, master_dir=None, tick_interval=5, average='mean', order=None, grid=False):\n    \"\"\"\n    Plot histograms or KDEs of a specified feature for each category in `separate`, with consistent binning.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing track data with the specified feature and optionally a separating column.\n    feature : str\n        The feature to plot histograms for.\n    bins : int, optional\n        Number of bins for the histogram. Default is 100.\n    separate : str, optional\n        Column to separate the data by. If None, all data will be plotted together. Default is None.\n    xlimit : float, optional\n        Upper limit for the x-axis. Default is None.\n    small_multiples : bool, optional\n        Whether to plot each category separately as small multiples. Default is False.\n    palette : str, optional\n        Color palette for the plot. Default is 'colorblind'.\n    use_kde : bool, optional\n        Whether to use KDE plot instead of histogram. Default is False.\n    show_plot : bool, optional\n        Whether to display the plot in the notebook. Default is True.\n    master_dir : str, optional\n        The directory where the plots folder will be created and the plot will be saved. Default is None.\n    tick_interval : int, optional\n        Interval for x-axis ticks. Default is 5.\n    average : str, optional\n        Whether to draw 'mean' or 'median' line on the plot. Default is 'mean'.\n    order : list, optional\n        Specific order for the conditions. Default is None.\n    \"\"\"\n\n    if master_dir is None:\n        master_dir = config.master  # Use the master directory from config if not provided\n\n    if separate is not None and order is not None:\n        # Ensure the data is ordered according to the specified order\n        data_df[separate] = pd.Categorical(data_df[separate], categories=order, ordered=True)\n\n    textpositionx=  0.6\n    textpositiony= 0.8\n\n    # Use the categories attribute to maintain the specified order\n    if separate is not None:\n        unique_categories = data_df[separate].cat.categories\n    else:\n        unique_categories = [None]\n\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    # Determine global maximum y-value for consistent y-axis limits\n    global_max_y = 0\n\n    if small_multiples and separate is not None:\n        num_categories = len(unique_categories)\n        fig, axes = plt.subplots(num_categories, 1, figsize=(20, 6 * num_categories), sharex=True)\n\n        if num_categories == 1:\n            axes = [axes]  # To handle the case with only one subplot\n\n        for i, category in enumerate(unique_categories):\n            if pd.isna(category):\n                continue\n            subset = data_df[data_df[separate] == category]\n            subsetvalues = subset[feature]\n\n            max_value = subsetvalues.max()\n\n            # Determine bin edges for the entire data range, including negative values\n            min_value = data_df[feature].min()\n            max_value = data_df[feature].max()\n            bin_edges = np.linspace(min_value, max_value, bins + 1)\n            # bin_edges = np.linspace(0, max_value, bins + 1)\n\n            # Plot histogram or KDE\n            if use_kde:\n                sns.kdeplot(subsetvalues, fill=True, ax=axes[i], color=color_palette[i])\n                current_max_y = axes[i].get_ylim()[1]  # Get the current maximum y-value from the plot\n            else:\n                plot = sns.histplot(subsetvalues, bins=bin_edges, kde=False, ax=axes[i], stat=\"percent\", color=color_palette[i])\n                current_max_y = plot.get_ylim()[1]\n\n            # Update global maximum y-value\n            if current_max_y &gt; global_max_y:\n                global_max_y = current_max_y\n\n            # Plot average line\n            if average == 'mean':\n                avg_value = subsetvalues.mean()\n                axes[i].axvline(avg_value, color='black', linestyle='--')\n                axes[i].text(textpositionx, textpositiony, f\"Mean: {avg_value:.2f}\", transform=axes[i].transAxes, fontsize=16)\n            elif average == 'median':\n                avg_value = subsetvalues.median()\n                axes[i].axvline(avg_value, color='black', linestyle='--')\n                axes[i].text(textpositionx, textpositiony, f\"Median: {avg_value:.2f}\", transform=axes[i].transAxes, fontsize=16)\n\n            axes[i].set_title(f'{category}', fontsize=16)\n            axes[i].tick_params(axis='both', which='major', labelsize=16)\n            axes[i].set_xlabel(f'{feature}', fontsize=16)\n            axes[i].set_ylabel('Percentage', fontsize=16)\n\n            if xlimit is not None:\n                x_lower = min_value if min_value &lt; 0 else 0\n                axes[i].set_xlim(x_lower, xlimit)\n            else:\n                axes[i].set_xlim(min_value, max_value)\n\n        # Set common y-axis limits for all subplots\n        for ax in axes:\n            ax.set_ylim(0, global_max_y)\n\n        plt.tight_layout()\n\n    else:\n        plt.figure(figsize=(20, 12))\n        sns.set_context(\"notebook\", rc={\"xtick.labelsize\": 16, \"ytick.labelsize\": 16})\n\n        max_value = data_df[feature].max()\n        bin_edges = np.linspace(0, max_value, bins + 1)\n\n        if separate is None:\n            subsetvalues = data_df[feature]\n\n            # Plot histogram or KDE\n            if use_kde:\n                sns.kdeplot(subsetvalues, fill=True, alpha=0.5, color=color_palette[0])\n            else:\n                sns.histplot(subsetvalues, bins=bin_edges, kde=False, alpha=0.5, stat=\"percent\", color=color_palette[0])\n\n            # Plot average line\n            if average == 'mean':\n                avg_value = subsetvalues.mean()\n                plt.axvline(avg_value, color='r', linestyle='--')\n                plt.text(textpositionx, textpositiony, f\"Overall Mean: {avg_value:.2f}\", transform=plt.gca().transAxes, fontsize=16)\n            elif average == 'median':\n                avg_value = subsetvalues.median()\n                plt.axvline(avg_value, color='b', linestyle='--')\n                plt.text(0.4, 0.6, f\"Overall Median: {avg_value:.2f}\", transform=plt.gca().transAxes, fontsize=16)\n\n        else:\n            for i, category in enumerate(unique_categories):\n                if pd.isna(category):\n                    continue\n                subset = data_df[data_df[separate] == category]\n                subsetvalues = subset[feature]\n\n                # Plot histogram or KDE\n                if use_kde:\n                    sns.kdeplot(subsetvalues, fill=True, label=category, alpha=0.5, color=color_palette[i])\n                else:\n                    sns.histplot(subsetvalues, bins=bin_edges, kde=False, label=category, alpha=0.5, stat=\"percent\", color=color_palette[i])\n\n                # Plot average line\n                if average == 'mean':\n                    avg_value = subsetvalues.mean()\n                    plt.axvline(avg_value, color=color_palette[i], linestyle='--')\n                elif average == 'median':\n                    avg_value = subsetvalues.median()\n                    plt.axvline(avg_value, color=color_palette[i], linestyle='--')\n\n                number_of_tracks = len(subset['unique_id'].unique())\n                shift = i * 0.05\n                plt.text(textpositionx, textpositiony - shift, f\"{category}: {average.capitalize()}: {avg_value:.2f} from {number_of_tracks} tracks\", transform=plt.gca().transAxes, fontsize=16)\n\n        plt.xlabel(f'{feature}', fontsize=16)\n        plt.ylabel('Percentage', fontsize=16)\n        plt.legend(title='', fontsize=16)\n        ax = plt.gca()\n        if xlimit is not None:\n            x_lower = min_value if min_value &lt; 0 else 0\n            axes[i].set_xlim(x_lower, xlimit)\n        else:\n            axes[i].set_xlim(min_value, max_value)\n\n        ax.set_xticks(np.arange(0, max_value + 1, tick_interval))  # Ensure ticks are at integer intervals\n        ax.set_xlim(0, xlimit or max_value)  # Start x-axis at 0\n        if grid:\n            ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)  # Add faint gridlines\n        else:\n            print(\"No grid\")\n\n    # Create directory for plots if it doesn't exist\n    plots_dir = os.path.join(master_dir, 'plots')\n    os.makedirs(plots_dir, exist_ok=True)\n    histodir = os.path.join(plots_dir, 'histograms')\n    os.makedirs(histodir, exist_ok=True)\n\n    # Generate filename\n    kde_text = 'kde' if use_kde else 'histogram'\n    average_text = f'{average}'\n    if small_multiples:\n        multitext = 'small_multiples'\n    else:\n        multitext = 'single_plot'\n\n    filename = f\"{histodir}/{kde_text}_{feature}_{average_text}_{multitext}.png\"\n\n    # Save plot\n    plt.savefig(filename, bbox_inches='tight')\n\n    # Show plot if specified\n    if show_plot:\n        plt.show()\n    else:\n        plt.close()\n</code></pre>"},{"location":"api/#SPTnano.visualization.plot_histograms_seconds","title":"<code>plot_histograms_seconds(traj_df, bins=100, coltoseparate='tracker', xlimit=None)</code>","text":"<p>Plot histograms of track lengths in seconds for each tracker, with consistent binning.</p>"},{"location":"api/#SPTnano.visualization.plot_histograms_seconds--parameters","title":"Parameters","text":"<p>traj_df : DataFrame     DataFrame containing track data with columns 'tracker', 'unique_id', 'time_s_zeroed', and 'filename'. bins : int, optional     Number of bins for the histogram. Default is 100. coltoseparate : str, optional     Column to separate the data by. Default is 'tracker'. xlimit : float, optional     Upper limit for the x-axis. Default is None.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_histograms_seconds(traj_df, bins=100, coltoseparate='tracker', xlimit=None):\n    \"\"\"\n    Plot histograms of track lengths in seconds for each tracker, with consistent binning.\n\n    Parameters\n    ----------\n    traj_df : DataFrame\n        DataFrame containing track data with columns 'tracker', 'unique_id', 'time_s_zeroed', and 'filename'.\n    bins : int, optional\n        Number of bins for the histogram. Default is 100.\n    coltoseparate : str, optional\n        Column to separate the data by. Default is 'tracker'.\n    xlimit : float, optional\n        Upper limit for the x-axis. Default is None.\n    \"\"\"\n    plt.figure(figsize=(20, 12))\n    size = 10\n    multiplier = 2\n    sns.set_context(\"notebook\", rc={\"xtick.labelsize\": size*multiplier, \"ytick.labelsize\": size*multiplier})\n\n    max_track_length = traj_df.groupby('unique_id')['time_s_zeroed'].max().max()\n    bin_edges = np.linspace(0, max_track_length, bins + 1)\n\n    for i, tracker in enumerate(traj_df[coltoseparate].unique()):\n        subset = traj_df[traj_df[coltoseparate] == tracker]\n        subsetvalues = subset.groupby('unique_id')['time_s_zeroed'].max()\n\n        # Calculate percentage counts\n        counts, _ = np.histogram(subsetvalues, bins=bin_edges)\n        percentage_counts = (counts / counts.sum()) * 100\n\n        # Plot histogram\n        sns.histplot(subsetvalues, bins=bin_edges, kde=True, label=tracker, alpha=0.5, stat=\"percent\")\n\n        subset_mean = subsetvalues.mean()\n        subset_median = subsetvalues.median()\n        subset_number_of_tracks = len(subset['unique_id'].unique())\n        shift = i * 0.05\n        plt.text(0.4, 0.6 - shift, f\"{tracker}: mean: {subset_mean:.2f} seconds from {subset_number_of_tracks} tracks\", transform=plt.gca().transAxes, fontsize=10 * multiplier)\n\n    plt.xlabel('Track length (seconds)', fontsize=size * multiplier)\n    plt.ylabel('Percentage', fontsize=size * multiplier)\n    plt.legend(title='', fontsize=size * multiplier)\n    ax = plt.gca()\n    if xlimit is not None:\n        ax.set_xlim(0, xlimit)\n    else:\n        ax.set_xlim(0, max_track_length)\n    plt.show()\n</code></pre>"},{"location":"api/#SPTnano.visualization.plot_time_series","title":"<code>plot_time_series(data_df, factor_col='speed_um_s', absolute=True, separate_by='condition', palette='colorblind', meanormedian='mean', multiplot=False, talk=False, bootstrap=True, show_plot=True, master_dir=None, order=None, grid=True)</code>","text":"<p>Plot time series of a specified factor, with mean/median as a line and confidence intervals as shaded areas.</p>"},{"location":"api/#SPTnano.visualization.plot_time_series--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing the time series data. factor_col : str, optional     The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'. absolute : bool, optional     Whether to use absolute time values or time zeroed values. Default is True. separate_by : str, optional     Column to separate the data by, for coloring. If None, all data will be plotted together. Default is None. palette : str, optional     Color palette for the plot. Default is 'colorblind'. meanormedian : str, optional     Whether to use mean or median for aggregation. Default is 'mean'. multiplot : bool, optional     Whether to generate separate small multiple plots for each category. Default is False. talk : bool, optional     Whether to set the figure size to the original large size or a smaller size. Default is False. bootstrap : bool, optional     Whether to use bootstrapping for confidence intervals. Default is True. show_plot : bool, optional     Whether to display the plot in the notebook. Default is True. master_dir : str, optional     The directory where the plots folder will be created and the plot will be saved. Default is None. order : list, optional     Specific order for the conditions. Default is None. grid : bool, optional     Whether to display grid lines. Default is True.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_time_series(data_df, factor_col='speed_um_s', absolute=True, separate_by='condition', palette='colorblind', \n                     meanormedian='mean', multiplot=False, talk=False, bootstrap=True, show_plot=True, \n                     master_dir=None, order=None, grid=True):\n    \"\"\"\n    Plot time series of a specified factor, with mean/median as a line and confidence intervals as shaded areas.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing the time series data.\n    factor_col : str, optional\n        The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'.\n    absolute : bool, optional\n        Whether to use absolute time values or time zeroed values. Default is True.\n    separate_by : str, optional\n        Column to separate the data by, for coloring. If None, all data will be plotted together. Default is None.\n    palette : str, optional\n        Color palette for the plot. Default is 'colorblind'.\n    meanormedian : str, optional\n        Whether to use mean or median for aggregation. Default is 'mean'.\n    multiplot : bool, optional\n        Whether to generate separate small multiple plots for each category. Default is False.\n    talk : bool, optional\n        Whether to set the figure size to the original large size or a smaller size. Default is False.\n    bootstrap : bool, optional\n        Whether to use bootstrapping for confidence intervals. Default is True.\n    show_plot : bool, optional\n        Whether to display the plot in the notebook. Default is True.\n    master_dir : str, optional\n        The directory where the plots folder will be created and the plot will be saved. Default is None.\n    order : list, optional\n        Specific order for the conditions. Default is None.\n    grid : bool, optional\n        Whether to display grid lines. Default is True.\n    \"\"\"\n    xmin = 0.2  # A FIX FOR NOW because really this should be the same as the shortest track (filtered to 0.2 s during filterstubs)\n\n    if master_dir is None:\n        master_dir = config.MASTER  # Use the master directory from config if not provided\n\n    if separate_by is not None and order is not None:\n        # Ensure the data is ordered according to the specified order\n        data_df[separate_by] = pd.Categorical(data_df[separate_by], categories=order, ordered=True)\n\n    if not absolute:\n        time_col = 'time_s_zeroed'\n        max_time_zeroed = data_df['time_s_zeroed'].max()\n        x_label = 'Time zeroed (s)'\n        xmax = max_time_zeroed\n    else:\n        time_col = 'time_s'\n        max_time = data_df['time_s'].max()\n        x_label = 'Time (s)'\n        xmax = max_time\n\n    # Use the categories attribute to maintain the specified order\n    if separate_by is not None:\n        # Convert to categorical if not already\n        if not pd.api.types.is_categorical_dtype(data_df[separate_by]):\n            data_df[separate_by] = pd.Categorical(data_df[separate_by], categories=order, ordered=True)\n        unique_categories = data_df[separate_by].cat.categories\n    else:\n        unique_categories = [None]\n\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    # # Set figure size and font size based on the `talk` parameter\n    # if talk:\n    #     fig_size = (40, 12)\n    #     font_size = 35\n    # else:\n    #     if multiplot and separate_by:\n    #         fig_size = (4, 8 * len(unique_categories))\n    #     else:\n    #         fig_size = (5, 3)\n    #     font_size = 14\n\n# Set figure size and font size based on the `talk` and `multiplot` parameters\n    if talk:\n        base_fig_size = (40, 12)\n        font_size = 35\n    else:\n        base_fig_size = (5, 4)\n        font_size = 14\n\n    # Adjust figure size if multiplot is true\n    if multiplot and separate_by:\n        fig_size = (base_fig_size[0], base_fig_size[1] * len(unique_categories))\n    else:\n        fig_size = base_fig_size\n\n    sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5, \"font.size\": font_size, \"axes.titlesize\": font_size, \n                                    \"axes.labelsize\": font_size, \"xtick.labelsize\": font_size, \"ytick.labelsize\": font_size})\n\n    if multiplot and separate_by:\n        num_categories = len(unique_categories)\n        fig, axes = plt.subplots(num_categories, 1, figsize=fig_size, sharex=True)\n\n        if num_categories == 1:\n            axes = [axes]  # To handle the case with only one subplot\n\n        for i, category in enumerate(unique_categories):\n            if pd.isna(category):\n                continue\n            ax = axes[i] if len(unique_categories) &gt; 1 else axes\n            subset = data_df[data_df[separate_by] == category]\n            times = subset[time_col]\n            factors = subset[factor_col]\n\n            if meanormedian == 'mean':\n                avg_factors = subset.groupby(time_col)[factor_col].mean()\n                ci_func = bootstrap_ci_mean if bootstrap else lambda x: sem(x) * 1.96\n            else:\n                avg_factors = subset.groupby(time_col)[factor_col].median()\n                ci_func = bootstrap_ci_median if bootstrap else lambda x: sem(x) * 1.96\n\n            ci = subset.groupby(time_col)[factor_col].apply(ci_func)\n\n            color = color_palette[i]\n            label = category\n\n            # Exclude the first time point (time zero)\n            valid_indices = avg_factors.index &gt; 0\n\n            ax.plot(avg_factors.index[valid_indices], avg_factors.values[valid_indices], label=label, color=color, linewidth=2.5)\n            ax.fill_between(avg_factors.index[valid_indices], (avg_factors - ci)[valid_indices], (avg_factors + ci)[valid_indices], color=color, alpha=0.3)\n            ax.set_xlabel(x_label, fontsize=font_size)\n            ax.set_ylabel(factor_col, fontsize=font_size, labelpad=20)\n            ax.legend(fontsize=font_size, loc='upper left', bbox_to_anchor=(1, 1))\n            ax.set_xlim(xmin, xmax)\n            if grid:\n                ax.grid(True, which='both', linestyle='--', linewidth=0.5, color='gray', alpha=0.7)\n            ax.spines['top'].set_visible(False)\n            ax.spines['right'].set_visible(False)\n            ax.set_title(f'{category}', fontsize=font_size)\n\n        plt.tight_layout()\n    else:\n        fig, ax = plt.subplots(figsize=fig_size)\n\n        for i, category in enumerate(unique_categories):\n            if pd.isna(category):\n                continue\n            subset = data_df if category is None else data_df[data_df[separate_by] == category]\n            times = subset[time_col]\n            factors = subset[factor_col]\n\n            if meanormedian == 'mean':\n                avg_factors = subset.groupby(time_col)[factor_col].mean()\n                ci_func = bootstrap_ci_mean if bootstrap else lambda x: sem(x) * 1.96\n            else:\n                avg_factors = subset.groupby(time_col)[factor_col].median()\n                ci_func = bootstrap_ci_median if bootstrap else lambda x: sem(x) * 1.96\n\n            ci = subset.groupby(time_col)[factor_col].apply(ci_func)\n\n            color = color_palette[i]\n            label = 'Overall' if category is None else category\n\n            # Exclude the first time point (time zero)\n            valid_indices = avg_factors.index &gt; 0\n\n            ax.plot(avg_factors.index[valid_indices], avg_factors.values[valid_indices], label=label, color=color, linewidth=2.5)\n            ax.fill_between(avg_factors.index[valid_indices], (avg_factors - ci)[valid_indices], (avg_factors + ci)[valid_indices], color=color, alpha=0.3)\n\n        ax.set_xlabel(x_label, fontsize=font_size)\n        ax.set_ylabel(factor_col, fontsize=font_size, labelpad=20)\n        ax.legend(fontsize=font_size, loc='upper left', bbox_to_anchor=(1.05, 1))\n        if grid:\n            ax.grid(True, which='both', linestyle='--', linewidth=0.5, color='gray', alpha=0.7)\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        ax.set_xlim(xmin, xmax)\n        plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout to fit legend\n\n    # Create directory for plots if it doesn't exist\n    plots_dir = os.path.join(master_dir, 'plots')\n    os.makedirs(plots_dir, exist_ok=True)\n\n    # Generate filename\n    time_type = 'absolute' if absolute else 'time_zeroed'\n    bootstrap_text = 'bootstrapped' if bootstrap else 'nonbootstrapped'\n    multiplot_text = 'multiplot' if multiplot else 'singleplot'\n    filename = f\"{plots_dir}/{factor_col}_{time_type}_{meanormedian}_{bootstrap_text}_{multiplot_text}.png\"\n\n    # Save plot\n    plt.savefig(filename, bbox_inches='tight')\n\n    # Show plot if specified\n    if show_plot:\n        plt.show()\n    else:\n        plt.close()\n</code></pre>"},{"location":"api/#SPTnano.visualization.plot_trajectory","title":"<code>plot_trajectory(traj, colorby='particle', mpp=None, label=False, superimpose=None, cmap=None, ax=None, t_column=None, pos_columns=None, plot_style={}, **kwargs)</code>","text":"<p>Plot traces of trajectories for each particle. Optionally superimpose it on a frame from the video.</p>"},{"location":"api/#SPTnano.visualization.plot_trajectory--parameters","title":"Parameters","text":"<p>traj : DataFrame     The DataFrame should include time and spatial coordinate columns. colorby : {'particle', 'frame'}, optional mpp : float, optional     Microns per pixel. If omitted, the labels will have units of pixels. label : boolean, optional     Set to True to write particle ID numbers next to trajectories. superimpose : ndarray, optional     Background image, default None cmap : colormap, optional     This is only used in colorby='frame' mode. Default = mpl.cm.winter ax : matplotlib axes object, optional     Defaults to current axes t_column : string, optional     DataFrame column name for time coordinate. Default is 'frame'. pos_columns : list of strings, optional     Dataframe column names for spatial coordinates. Default is ['x', 'y']. plot_style : dictionary     Keyword arguments passed through to the <code>Axes.plot(...)</code> command</p>"},{"location":"api/#SPTnano.visualization.plot_trajectory--returns","title":"Returns","text":"<p>Axes object</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_trajectory(traj, colorby='particle', mpp=None, label=False,\n                    superimpose=None, cmap=None, ax=None, t_column=None,\n                    pos_columns=None, plot_style={}, **kwargs):\n    \"\"\"\n    Plot traces of trajectories for each particle.\n    Optionally superimpose it on a frame from the video.\n\n    Parameters\n    ----------\n    traj : DataFrame\n        The DataFrame should include time and spatial coordinate columns.\n    colorby : {'particle', 'frame'}, optional\n    mpp : float, optional\n        Microns per pixel. If omitted, the labels will have units of pixels.\n    label : boolean, optional\n        Set to True to write particle ID numbers next to trajectories.\n    superimpose : ndarray, optional\n        Background image, default None\n    cmap : colormap, optional\n        This is only used in colorby='frame' mode. Default = mpl.cm.winter\n    ax : matplotlib axes object, optional\n        Defaults to current axes\n    t_column : string, optional\n        DataFrame column name for time coordinate. Default is 'frame'.\n    pos_columns : list of strings, optional\n        Dataframe column names for spatial coordinates. Default is ['x', 'y'].\n    plot_style : dictionary\n        Keyword arguments passed through to the `Axes.plot(...)` command\n\n    Returns\n    -------\n    Axes object\n    \"\"\"\n    if cmap is None:\n        cmap = plt.cm.winter\n    if t_column is None:\n        t_column = 'frame'\n    if pos_columns is None:\n        pos_columns = ['x', 'y']\n    if len(traj) == 0:\n        raise ValueError(\"DataFrame of trajectories is empty.\")\n\n    _plot_style = dict(linewidth=1)\n    _plot_style.update(**plot_style)\n\n    if ax is None:\n        ax = plt.gca()\n\n    # Axes labels\n    if mpp is None:\n        ax.set_xlabel(f'{pos_columns[0]} [px]')\n        ax.set_ylabel(f'{pos_columns[1]} [px]')\n        mpp = 1.  # for computations of image extent below\n    else:\n        ax.set_xlabel(f'{pos_columns[0]} [\u03bcm]')\n        ax.set_ylabel(f'{pos_columns[1]} [\u03bcm]')\n\n    # Background image\n    if superimpose is not None:\n        ax.imshow(superimpose, cmap=plt.cm.gray,\n                  origin='lower', interpolation='nearest',\n                  vmin=kwargs.get('vmin'), vmax=kwargs.get('vmax'))\n        ax.set_xlim(-0.5 * mpp, (superimpose.shape[1] - 0.5) * mpp)\n        ax.set_ylim(-0.5 * mpp, (superimpose.shape[0] - 0.5) * mpp)\n\n    # Trajectories\n    if colorby == 'particle':\n        # Unstack particles into columns.\n        unstacked = traj.set_index(['particle', t_column])[pos_columns].unstack()\n        for i, trajectory in unstacked.iterrows():\n            ax.plot(mpp * trajectory[pos_columns[0]], mpp * trajectory[pos_columns[1]], **_plot_style)\n    elif colorby == 'frame':\n        # Read http://www.scipy.org/Cookbook/Matplotlib/MulticoloredLine\n        x = traj.set_index([t_column, 'particle'])[pos_columns[0]].unstack()\n        y = traj.set_index([t_column, 'particle'])[pos_columns[1]].unstack()\n        color_numbers = traj[t_column].values / float(traj[t_column].max())\n        for particle in x:\n            points = np.array([x[particle].values, y[particle].values]).T.reshape(-1, 1, 2)\n            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n            lc = LineCollection(segments, cmap=cmap)\n            lc.set_array(color_numbers)\n            ax.add_collection(lc)\n            ax.set_xlim(x.apply(np.min).min(), x.apply(np.max).max())\n            ax.set_ylim(y.apply(np.min).min(), y.apply(np.max).max())\n\n    if label:\n        unstacked = traj.set_index([t_column, 'particle'])[pos_columns].unstack()\n        first_frame = int(traj[t_column].min())\n        coords = unstacked.fillna(method='backfill').stack().loc[first_frame]\n        for particle_id, coord in coords.iterrows():\n            ax.text(*coord.tolist(), s=\"%d\" % particle_id,\n                    horizontalalignment='center',\n                    verticalalignment='center')\n\n    ax.invert_yaxis()\n    return ax\n</code></pre>"},{"location":"api/#SPTnano.visualization.plot_violinplots","title":"<code>plot_violinplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', talk=False)</code>","text":"<p>Plot violin plots of a specified factor, with data separated by categories.</p>"},{"location":"api/#SPTnano.visualization.plot_violinplots--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing the data. factor_col : str, optional     The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'. separate_by : str, optional     Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'. palette : str, optional     Color palette for the plot. Default is 'colorblind'. talk : bool, optional     Whether to set the figure size to the original large size or a smaller size. Default is False.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_violinplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', talk=False):\n    \"\"\"\n    Plot violin plots of a specified factor, with data separated by categories.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing the data.\n    factor_col : str, optional\n        The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'.\n    separate_by : str, optional\n        Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'.\n    palette : str, optional\n        Color palette for the plot. Default is 'colorblind'.\n    talk : bool, optional\n        Whether to set the figure size to the original large size or a smaller size. Default is False.\n    \"\"\"\n\n    unique_categories = data_df[separate_by].unique() if separate_by else [None]\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    # Set figure size based on the `talk` parameter\n    if talk:\n        fig_size = (20, 12)\n        font_size = 35\n    else:\n        fig_size = (5, 3)\n        font_size = 14\n\n    fig, ax = plt.subplots(figsize=fig_size)\n    sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5, \"font.size\": font_size, \"axes.titlesize\": font_size, \"axes.labelsize\": font_size, \"xtick.labelsize\": font_size, \"ytick.labelsize\": font_size})\n\n    # Plot violin plot\n    sns.violinplot(x=separate_by, y=factor_col, hue=separate_by, data=data_df, palette=color_palette, ax=ax, legend=False, alpha=0.79)\n\n    # Remove 'Condition_' prefix from x tick labels\n    new_labels = [label.replace('Condition_', '') for label in unique_categories]\n    ax.set_xticks(range(len(new_labels)))\n    ax.set_xticklabels(new_labels, fontsize=font_size)\n\n    ax.set_ylabel(factor_col, fontsize=font_size, labelpad=20)\n    ax.set_xlabel(None)\n    ax.tick_params(axis='both', which='major', labelsize=font_size)\n    plt.tight_layout()\n\n    plt.show()\n</code></pre>"}]}