{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#sptnano","title":"SPTnano","text":"<p>Pipeline for easy single particle detection, linking/tracking, and analysis</p> <p>The structure for this repo was made using a cookiecutter from the Centre for Advanced Research Computing, University College London.</p>"},{"location":"#about","title":"About","text":""},{"location":"#project-team","title":"Project Team","text":"<p>Michael Shannon (m.j.shannon@pm.me)</p>"},{"location":"#research-software-engineering-contact","title":"Research Software Engineering Contact","text":"<p>Michael Shannon (m.j.shannon@pm.me)</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<p><code>SPTnano</code> requires Python 3.10\u20133.12.</p>"},{"location":"#installation","title":"Installation","text":"<p>We recommend installing in a project specific virtual environment created using a environment management tool such as Conda. To install the latest development version of <code>SPTnano</code> using <code>pip</code> in the currently active environment run</p> <pre><code>pip install git+https://github.com/Michael-shannon/SPTnano.git\n</code></pre> <p>Alternatively create a local clone of the repository with</p> <pre><code>git clone https://github.com/Michael-shannon/SPTnano.git\n</code></pre> <p>and then install in editable mode by running</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"#running-locally","title":"Running Locally","text":"<p>How to run the application on your local system.</p>"},{"location":"#running-tests","title":"Running Tests","text":"<p>Tests can be run across all compatible Python versions in isolated environments using <code>tox</code> by running</p> <pre><code>tox\n</code></pre> <p>To run tests manually in a Python environment with <code>pytest</code> installed run</p> <pre><code>pytest tests\n</code></pre> <p>again from the root of the repository.</p>"},{"location":"#building-documentation","title":"Building Documentation","text":"<p>The MkDocs HTML documentation can be built locally by running</p> <pre><code>tox -e docs\n</code></pre> <p>from the root of the repository. The built documentation will be written to <code>site</code>.</p> <p>Alternatively to build and preview the documentation locally, in a Python environment with the optional <code>docs</code> dependencies installed, run</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Initial Research</li> <li> Minimum viable product &lt;-- You are Here</li> <li> Alpha Release</li> <li> Feature-Complete Release</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This work was funded by Cure Huntingtons Disease Initiative (CHDI) and The Rockefeller University.</p>"},{"location":"LICENSE/","title":"License","text":""},{"location":"LICENSE/#mit-license","title":"MIT License","text":"<p>Copyright (c) 2024 Michael Shannon</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"api/","title":"API reference","text":"<p>SPTnano package.</p>"},{"location":"api/#SPTnano.add_microns_and_secs","title":"<code>add_microns_and_secs(df, pixelsize_microns, time_between_frames)</code>","text":"<p>Adds columns to the DataFrame with positions in microns and time in seconds</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def add_microns_and_secs(df, pixelsize_microns, time_between_frames):\n    '''Adds columns to the DataFrame with positions in microns and time in seconds'''\n    #space transformations\n    df['x_um'] = df['x'] * pixelsize_microns\n    df['y_um'] = df['y'] * pixelsize_microns\n\n    df['frame_zeroed'] = df.groupby('particle')['frame'].transform(lambda x: x - x.iloc[0])\n    df['time_s'] = df['frame'] * time_between_frames\n    df['time_s_zeroed'] = df.groupby('particle')['time_s'].transform(lambda x: x - x.iloc[0])\n    return df\n</code></pre>"},{"location":"api/#SPTnano.batch_plot_trajectories","title":"<code>batch_plot_trajectories(master_folder, traj_df, batch=True, filename=None, colorby='particle', mpp=None, label=False, cmap=None)</code>","text":"<p>Batch plot trajectories for all replicates across several conditions.</p>"},{"location":"api/#SPTnano.batch_plot_trajectories--parameters","title":"Parameters","text":"<p>master_folder : str     Path to the master folder containing 'data' and 'saved_data' folders. traj_df : DataFrame     The DataFrame containing trajectory data. batch : bool, optional     If True, plots trajectories for all replicates in batch mode.     If False, plots trajectory for the specified filename. filename : str, optional     Filename of interest when batch is False. colorby : str, optional     Color by 'particle' or 'frame'. mpp : float, optional     Microns per pixel. label : bool, optional     Set to True to write particle ID numbers next to trajectories. cmap : colormap, optional     Colormap to use for coloring tracks.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def batch_plot_trajectories(master_folder, traj_df, batch=True, filename=None, colorby='particle', mpp=None, label=False, cmap=None):\n    \"\"\"\n    Batch plot trajectories for all replicates across several conditions.\n\n    Parameters\n    ----------\n    master_folder : str\n        Path to the master folder containing 'data' and 'saved_data' folders.\n    traj_df : DataFrame\n        The DataFrame containing trajectory data.\n    batch : bool, optional\n        If True, plots trajectories for all replicates in batch mode.\n        If False, plots trajectory for the specified filename.\n    filename : str, optional\n        Filename of interest when batch is False.\n    colorby : str, optional\n        Color by 'particle' or 'frame'.\n    mpp : float, optional\n        Microns per pixel.\n    label : bool, optional\n        Set to True to write particle ID numbers next to trajectories.\n    cmap : colormap, optional\n        Colormap to use for coloring tracks.\n    \"\"\"\n    data_folder = os.path.join(master_folder, 'data')\n    vis_folder = os.path.join(master_folder, 'visualization')\n    os.makedirs(vis_folder, exist_ok=True)\n\n    if batch:\n        for condition in os.listdir(data_folder):\n            condition_folder = os.path.join(data_folder, condition)\n            if os.path.isdir(condition_folder):\n                for file in os.listdir(condition_folder):\n                    if file.endswith('.tif'):\n                        filepath = os.path.join(condition_folder, file)\n                        subset_traj_df = traj_df[traj_df['filename'] == file]\n                        if not subset_traj_df.empty:\n                            frames = pims.open(filepath)\n                            frame = frames[0]\n                            fig, ax = plt.subplots()\n                            plot_trajectory(subset_traj_df, colorby=colorby, mpp=mpp, label=label, superimpose=frame, cmap=cmap, ax=ax)\n                            plt.savefig(os.path.join(vis_folder, f'{condition}_{file}.png'))\n                            plt.close(fig)\n    else:\n        if filename is not None:\n            filepath = os.path.join(data_folder, filename)\n            subset_traj_df = traj_df[traj_df['filename'] == filename]\n            if not subset_traj_df.empty:\n                frames = pims.open(filepath)\n                frame = frames[0]\n                fig, ax = plt.subplots()\n                plot_trajectory(subset_traj_df, colorby=colorby, mpp=mpp, label=label, superimpose=frame, cmap=cmap, ax=ax)\n                plt.show()\n        else:\n            print(\"Please provide a filename when batch is set to False.\")\n</code></pre>"},{"location":"api/#SPTnano.example_function","title":"<code>example_function(argument, keyword_argument='default')</code>","text":"<p>Concatenate string arguments - an example function docstring.</p> <p>Parameters:</p> Name Type Description Default <code>argument</code> <code>str</code> <p>An argument.</p> required <code>keyword_argument</code> <code>str</code> <p>A keyword argument with a default value.</p> <code>'default'</code> <p>Returns:</p> Type Description <code>str</code> <p>The concatenation of <code>argument</code> and <code>keyword_argument</code>.</p> Source code in <code>src/SPTnano/__init__.py</code> <pre><code>def example_function(argument: str, keyword_argument: str = \"default\") -&gt; str:\n    \"\"\"\n    Concatenate string arguments - an example function docstring.\n\n    Args:\n        argument: An argument.\n        keyword_argument: A keyword argument with a default value.\n\n    Returns:\n        The concatenation of `argument` and `keyword_argument`.\n\n    \"\"\"\n    return argument + keyword_argument\n</code></pre>"},{"location":"api/#SPTnano.filter_high_speeds","title":"<code>filter_high_speeds(metrics_df, speed_threshold)</code>","text":"<p>Filter based on speed instead - can be relevant if you have different exposure times and different times between frames</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def filter_high_speeds(metrics_df, speed_threshold):\n    '''\n    Filter based on speed instead - can be relevant if you have different exposure times and different times between frames\n    '''\n\n    # Identify unique_ids with any high speeds\n    high_speed_particles = metrics_df[metrics_df['speed_um_s'] &gt; speed_threshold]['unique_id'].unique()\n\n    # Filter out particles with high speeds\n    metrics_df_filtered = metrics_df[~metrics_df['unique_id'].isin(high_speed_particles)].copy()\n    return metrics_df_filtered\n</code></pre>"},{"location":"api/#SPTnano.filter_large_jumps","title":"<code>filter_large_jumps(df, threshold)</code>","text":"<p>Filter out entire particles with any frames showing large jumps in micrometers.</p>"},{"location":"api/#SPTnano.filter_large_jumps--parameters","title":"Parameters","text":"<p>df : DataFrame     DataFrame containing tracking data with a 'segment_len_um' column. threshold : float     Threshold for what constitutes a large jump in micrometers.</p>"},{"location":"api/#SPTnano.filter_large_jumps--returns","title":"Returns","text":"<p>DataFrame     DataFrame with particles having large jumps filtered out.</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def filter_large_jumps(df, threshold):\n    \"\"\"\n    Filter out entire particles with any frames showing large jumps in micrometers.\n\n    Parameters\n    ----------\n    df : DataFrame\n        DataFrame containing tracking data with a 'segment_len_um' column.\n    threshold : float\n        Threshold for what constitutes a large jump in micrometers.\n\n    Returns\n    -------\n    DataFrame\n        DataFrame with particles having large jumps filtered out.\n    \"\"\"\n    # Identify unique_ids with any large jumps\n    large_jump_particles = df[df['segment_len_um'] &gt; threshold]['unique_id'].unique()\n\n    # Filter out particles with large jumps\n    df_filtered = df[~df['unique_id'].isin(large_jump_particles)].copy()\n    # df_filtered.drop(columns=['x_um_prev', 'y_um_prev', 'segment_len_um'], inplace=True)\n    return df_filtered\n</code></pre>"},{"location":"api/#SPTnano.filter_stubs","title":"<code>filter_stubs(df, min_time)</code>","text":"<p>Removes tracks that are shorter than 'min_time' by finding the max duration of each time_s_zeroed column and filtering on that Works across exposure times, because it works on converted seconds, not frames</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def filter_stubs(df, min_time):\n\n    '''\n    Removes tracks that are shorter than 'min_time' by finding the max duration of each time_s_zeroed column and filtering on that\n    Works across exposure times, because it works on converted seconds, not frames\n\n    '''\n    # Calculate the duration of each track by grouping by 'particle' and using the 'time_s' column\n    track_durations = df.groupby('unique_id')['time_s_zeroed'].max() \n    # Identify particles with tracks longer than 0.2 seconds\n    valid_particles = track_durations[track_durations &gt;= min_time].index\n    # Filter the dataframe to include only valid particles\n    filtered_df = df[df['unique_id'].isin(valid_particles)]\n\n    return filtered_df\n</code></pre>"},{"location":"api/#SPTnano.plot_barplots","title":"<code>plot_barplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', meanormedian='mean', talk=False)</code>","text":"<p>Plot bar plots of a specified factor, with bootstrapped confidence intervals.</p>"},{"location":"api/#SPTnano.plot_barplots--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing the data. factor_col : str, optional     The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'. separate_by : str, optional     Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'. palette : str, optional     Color palette for the plot. Default is 'colorblind'. meanormedian : str, optional     Whether to use mean or median for aggregation. Default is 'mean'. talk : bool, optional     Whether to set the figure size to the original large size or a smaller size. Default is False.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_barplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', meanormedian='mean', talk=False):\n    \"\"\"\n    Plot bar plots of a specified factor, with bootstrapped confidence intervals.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing the data.\n    factor_col : str, optional\n        The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'.\n    separate_by : str, optional\n        Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'.\n    palette : str, optional\n        Color palette for the plot. Default is 'colorblind'.\n    meanormedian : str, optional\n        Whether to use mean or median for aggregation. Default is 'mean'.\n    talk : bool, optional\n        Whether to set the figure size to the original large size or a smaller size. Default is False.\n    \"\"\"\n\n    unique_categories = data_df[separate_by].unique() if separate_by else [None]\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    # Set figure size based on the `talk` parameter\n    if talk:\n        fig_size = (20, 12)\n        font_size = 35\n    else:\n        fig_size = (5, 3)\n        font_size = 14\n\n    fig, ax = plt.subplots(figsize=fig_size)\n    sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5, \"font.size\": font_size, \"axes.titlesize\": font_size, \"axes.labelsize\": font_size, \"xtick.labelsize\": font_size, \"ytick.labelsize\": font_size})\n\n    avg_factors_list = []\n    ci_intervals = []\n\n    for i, category in enumerate(unique_categories):\n        subset = data_df if category is None else data_df[data_df[separate_by] == category]\n\n        if meanormedian == 'mean':\n            avg_factors = subset[factor_col].mean()\n            ci_interval = bootstrap_ci_mean(subset[factor_col], num_samples=1000, alpha=0.05)\n        else:\n            avg_factors = subset[factor_col].median()\n            ci_interval = bootstrap_ci_median(subset[factor_col], num_samples=1000, alpha=0.05)\n\n        avg_factors_list.append(avg_factors)\n        ci_intervals.append(ci_interval)\n\n    categories = unique_categories if separate_by else ['Overall']\n    ax.bar(categories, avg_factors_list, yerr=ci_intervals, color=color_palette, capsize=5, edgecolor='black')\n\n    # Remove 'Condition_' prefix from x tick labels\n    new_labels = [label.replace('Condition_', '') for label in categories]\n    if talk:\n        ax.set_xticklabels(new_labels, fontsize=font_size)\n    else:\n        ax.set_xticklabels(new_labels, fontsize=font_size, rotation=90)\n\n\n    ax.set_ylabel(factor_col, fontsize=font_size)\n    ax.tick_params(axis='both', which='major', labelsize=font_size)\n    plt.tight_layout()\n\n    plt.show()\n</code></pre>"},{"location":"api/#SPTnano.plot_histograms","title":"<code>plot_histograms(data_df, feature, bins=100, separate=None, xlimit=None, small_multiples=False, palette='colorblind', use_kde=False)</code>","text":"<p>Plot histograms of a specified feature for each category in coltoseparate, with consistent binning.</p>"},{"location":"api/#SPTnano.plot_histograms--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing track data with the specified feature and optionally a separating column. feature : str     The feature to plot histograms for. bins : int, optional     Number of bins for the histogram. Default is 100. separate : str, optional     Column to separate the data by. If None, all data will be plotted together. Default is None. xlimit : float, optional     Upper limit for the x-axis. Default is None. small_multiples : bool, optional     Whether to plot each category separately as small multiples. Default is False. use_kde : bool, optional     Whether to use KDE plot instead of histogram. Default is False.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_histograms(data_df, feature, bins=100, separate=None, xlimit=None, small_multiples=False, palette='colorblind', use_kde=False):\n    \"\"\"\n    Plot histograms of a specified feature for each category in coltoseparate, with consistent binning.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing track data with the specified feature and optionally a separating column.\n    feature : str\n        The feature to plot histograms for.\n    bins : int, optional\n        Number of bins for the histogram. Default is 100.\n    separate : str, optional\n        Column to separate the data by. If None, all data will be plotted together. Default is None.\n    xlimit : float, optional\n        Upper limit for the x-axis. Default is None.\n    small_multiples : bool, optional\n        Whether to plot each category separately as small multiples. Default is False.\n    use_kde : bool, optional\n        Whether to use KDE plot instead of histogram. Default is False.\n    \"\"\"\n    unique_categories = data_df[separate].unique() if separate else [None]\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    if small_multiples and separate is not None:\n        num_categories = len(unique_categories)\n        fig, axes = plt.subplots(num_categories, 1, figsize=(20, 6 * num_categories), sharex=True)\n\n        if num_categories == 1:\n            axes = [axes]  # To handle the case with only one subplot\n\n        for i, category in enumerate(unique_categories):\n            subset = data_df[data_df[separate] == category]\n            subsetvalues = subset[feature]\n\n            max_value = subsetvalues.max()\n            bin_edges = np.linspace(0, max_value, bins + 1)\n\n            # Plot histogram or KDE\n            if use_kde:\n                sns.kdeplot(subsetvalues, fill=True, ax=axes[i], color=color_palette[i])\n            else:\n                sns.histplot(subsetvalues, bins=bin_edges, kde=True, ax=axes[i], stat=\"percent\", color=color_palette[i])\n\n            axes[i].set_title(f'{category}', fontsize=14)\n\n            mean_value = subsetvalues.mean()\n            median_value = subsetvalues.median()\n            number_of_tracks = len(subset['unique_id'].unique())\n            axes[i].text(0.4, 0.6, f\"Mean: {mean_value:.2f}, Median: {median_value:.2f}, Tracks: {number_of_tracks}\", transform=axes[i].transAxes, fontsize=10)\n\n            if xlimit is not None:\n                axes[i].set_xlim(0, xlimit)\n            else:\n                axes[i].set_xlim(0, max_value)\n\n        plt.xlabel(f'{feature}', fontsize=12)\n        plt.tight_layout()\n        plt.show()\n\n    else:\n        plt.figure(figsize=(20, 12))\n        size = 10\n        multiplier = 2\n        sns.set_context(\"notebook\", rc={\"xtick.labelsize\": size * multiplier, \"ytick.labelsize\": size * multiplier})\n\n        max_value = data_df[feature].max()\n        bin_edges = np.linspace(0, max_value, bins + 1)\n\n        if separate is None:\n            subsetvalues = data_df[feature]\n\n            # Plot histogram or KDE\n            if use_kde:\n                sns.kdeplot(subsetvalues, fill=True, alpha=0.5, color=color_palette[0])\n            else:\n                sns.histplot(subsetvalues, bins=bin_edges, kde=True, alpha=0.5, stat=\"percent\", color=color_palette[0])\n\n            mean_value = subsetvalues.mean()\n            median_value = subsetvalues.median()\n            plt.text(0.4, 0.6, f\"Overall: mean: {mean_value:.2f}, median: {median_value:.2f}\", transform=plt.gca().transAxes, fontsize=10 * multiplier)\n\n        else:\n            for i, category in enumerate(unique_categories):\n                subset = data_df[data_df[separate] == category]\n                subsetvalues = subset[feature]\n\n                # Plot histogram or KDE\n                if use_kde:\n                    sns.kdeplot(subsetvalues, fill=True, label=category, alpha=0.5, color=color_palette[i])\n                else:\n                    sns.histplot(subsetvalues, bins=bin_edges, kde=True, label=category, alpha=0.5, stat=\"percent\", color=color_palette[i])\n\n                mean_value = subsetvalues.mean()\n                median_value = subsetvalues.median()\n                number_of_tracks = len(subset['unique_id'].unique())\n                shift = i * 0.05\n                plt.text(0.4, 0.6 - shift, f\"{category}: mean: {mean_value:.2f} from {number_of_tracks} tracks\", transform=plt.gca().transAxes, fontsize=10 * multiplier)\n\n        plt.xlabel(f'{feature}', fontsize=size * multiplier)\n        plt.ylabel('Percentage', fontsize=size * multiplier)\n        plt.legend(title='', fontsize=size * multiplier)\n        ax = plt.gca()\n        if xlimit is not None:\n            ax.set_xlim(0, xlimit)\n        else:\n            ax.set_xlim(0, max_value)\n        plt.show()\n</code></pre>"},{"location":"api/#SPTnano.plot_histograms_seconds","title":"<code>plot_histograms_seconds(traj_df, bins=100, coltoseparate='tracker', xlimit=None)</code>","text":"<p>Plot histograms of track lengths in seconds for each tracker, with consistent binning.</p>"},{"location":"api/#SPTnano.plot_histograms_seconds--parameters","title":"Parameters","text":"<p>traj_df : DataFrame     DataFrame containing track data with columns 'tracker', 'unique_id', 'time_s_zeroed', and 'filename'. bins : int, optional     Number of bins for the histogram. Default is 100. coltoseparate : str, optional     Column to separate the data by. Default is 'tracker'. xlimit : float, optional     Upper limit for the x-axis. Default is None.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_histograms_seconds(traj_df, bins=100, coltoseparate='tracker', xlimit=None):\n    \"\"\"\n    Plot histograms of track lengths in seconds for each tracker, with consistent binning.\n\n    Parameters\n    ----------\n    traj_df : DataFrame\n        DataFrame containing track data with columns 'tracker', 'unique_id', 'time_s_zeroed', and 'filename'.\n    bins : int, optional\n        Number of bins for the histogram. Default is 100.\n    coltoseparate : str, optional\n        Column to separate the data by. Default is 'tracker'.\n    xlimit : float, optional\n        Upper limit for the x-axis. Default is None.\n    \"\"\"\n    plt.figure(figsize=(20, 12))\n    size = 10\n    multiplier = 2\n    sns.set_context(\"notebook\", rc={\"xtick.labelsize\": size*multiplier, \"ytick.labelsize\": size*multiplier})\n\n    max_track_length = traj_df.groupby('unique_id')['time_s_zeroed'].max().max()\n    bin_edges = np.linspace(0, max_track_length, bins + 1)\n\n    for i, tracker in enumerate(traj_df[coltoseparate].unique()):\n        subset = traj_df[traj_df[coltoseparate] == tracker]\n        subsetvalues = subset.groupby('unique_id')['time_s_zeroed'].max()\n\n        # Calculate percentage counts\n        counts, _ = np.histogram(subsetvalues, bins=bin_edges)\n        percentage_counts = (counts / counts.sum()) * 100\n\n        # Plot histogram\n        sns.histplot(subsetvalues, bins=bin_edges, kde=True, label=tracker, alpha=0.5, stat=\"percent\")\n\n        subset_mean = subsetvalues.mean()\n        subset_median = subsetvalues.median()\n        subset_number_of_tracks = len(subset['unique_id'].unique())\n        shift = i * 0.05\n        plt.text(0.4, 0.6 - shift, f\"{tracker}: mean: {subset_mean:.2f} seconds from {subset_number_of_tracks} tracks\", transform=plt.gca().transAxes, fontsize=10 * multiplier)\n\n    plt.xlabel('Track length (seconds)', fontsize=size * multiplier)\n    plt.ylabel('Percentage', fontsize=size * multiplier)\n    plt.legend(title='', fontsize=size * multiplier)\n    ax = plt.gca()\n    if xlimit is not None:\n        ax.set_xlim(0, xlimit)\n    else:\n        ax.set_xlim(0, max_track_length)\n    plt.show()\n</code></pre>"},{"location":"api/#SPTnano.plot_time_series","title":"<code>plot_time_series(data_df, factor_col='speed_um_s', absolute=True, separate_by='condition', palette='colorblind', meanormedian='mean', multiplot=False, talk=False)</code>","text":"<p>Plot time series of a specified factor, with mean as a thick line and confidence intervals as shaded areas.</p>"},{"location":"api/#SPTnano.plot_time_series--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing the time series data. factor_col : str, optional     The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'. absolute : bool, optional     Whether to use absolute time values or time zeroed values. Default is True. separate_by : str, optional     Column to separate the data by, for coloring. If None, all data will be plotted together. Default is None. palette : str, optional     Color palette for the plot. Default is 'colorblind'. meanormedian : str, optional     Whether to use mean or median for aggregation. Default is 'mean'. multiplot : bool, optional     Whether to generate separate small multiple plots for each category. Default is False. talk : bool, optional     Whether to set the figure size to the original large size or a smaller size. Default is False.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_time_series(data_df, factor_col='speed_um_s', absolute=True, separate_by='condition', palette='colorblind', meanormedian='mean', multiplot=False, talk=False):\n    \"\"\"\n    Plot time series of a specified factor, with mean as a thick line and confidence intervals as shaded areas.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing the time series data.\n    factor_col : str, optional\n        The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'.\n    absolute : bool, optional\n        Whether to use absolute time values or time zeroed values. Default is True.\n    separate_by : str, optional\n        Column to separate the data by, for coloring. If None, all data will be plotted together. Default is None.\n    palette : str, optional\n        Color palette for the plot. Default is 'colorblind'.\n    meanormedian : str, optional\n        Whether to use mean or median for aggregation. Default is 'mean'.\n    multiplot : bool, optional\n        Whether to generate separate small multiple plots for each category. Default is False.\n    talk : bool, optional\n        Whether to set the figure size to the original large size or a smaller size. Default is False.\n    \"\"\"\n\n    if not absolute:\n        time_col = 'time_s_zeroed'\n        x_label = 'Time zeroed (s)'\n    else:\n        time_col = 'time_s'\n        x_label = 'Time (s)'\n\n    unique_categories = data_df[separate_by].unique() if separate_by else [None]\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    # Set figure size and font size based on the `talk` parameter\n    if talk:\n        fig_size = (40, 12)\n        font_size = 35\n    else:\n        if multiplot and separate_by:\n            fig_size = (10, 5 * len(unique_categories))\n        else:\n            fig_size = (5, 3)\n        font_size = 14\n\n    sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5, \"font.size\": font_size, \"axes.titlesize\": font_size, \"axes.labelsize\": font_size, \"xtick.labelsize\": font_size, \"ytick.labelsize\": font_size})\n\n    if multiplot and separate_by:\n        fig, axes = plt.subplots(len(unique_categories), 1, figsize=fig_size, sharex=True)\n\n        for i, category in enumerate(unique_categories):\n            ax = axes[i] if len(unique_categories) &gt; 1 else axes\n            subset = data_df[data_df[separate_by] == category]\n            times = subset[time_col]\n            factors = subset[factor_col]\n\n            if meanormedian == 'mean':\n                avg_factors = subset.groupby(time_col)[factor_col].mean()\n                ci = subset.groupby(time_col)[factor_col].apply(lambda x: bootstrap_ci_mean(x, num_samples=1000, alpha=0.05))\n            else:\n                avg_factors = subset.groupby(time_col)[factor_col].median()\n                ci = subset.groupby(time_col)[factor_col].apply(lambda x: bootstrap_ci_median(x, num_samples=1000, alpha=0.05))\n\n            color = color_palette[i]\n            label = category\n\n            ax.plot(avg_factors.index, avg_factors.values, label=label, color=color, linewidth=0.5)\n            ax.fill_between(avg_factors.index, avg_factors - ci, avg_factors + ci, color=color, alpha=0.3)\n            ax.set_xlabel(x_label, fontsize=font_size)\n            ax.set_ylabel(factor_col, fontsize=font_size, labelpad=20)\n            ax.legend(title=separate_by, fontsize=font_size, loc='upper left', bbox_to_anchor=(1, 1))\n            ax.set_title(f'Time Series of {factor_col} - {category}', fontsize=font_size)\n\n        plt.tight_layout()\n    else:\n        fig, ax = plt.subplots(figsize=fig_size)\n\n        for i, category in enumerate(unique_categories):\n            subset = data_df if category is None else data_df[data_df[separate_by] == category]\n            times = subset[time_col]\n            factors = subset[factor_col]\n\n            if meanormedian == 'mean':\n                avg_factors = subset.groupby(time_col)[factor_col].mean()\n                ci = subset.groupby(time_col)[factor_col].apply(lambda x: bootstrap_ci_mean(x, num_samples=1000, alpha=0.05))\n            else:\n                avg_factors = subset.groupby(time_col)[factor_col].median()\n                ci = subset.groupby(time_col)[factor_col].apply(lambda x: bootstrap_ci_median(x, num_samples=1000, alpha=0.05))\n\n            color = color_palette[i]\n            label = 'Overall' if category is None else category\n\n            ax.plot(avg_factors.index, avg_factors.values, label=label, color=color, linewidth=0.5)\n            ax.fill_between(avg_factors.index, avg_factors - ci, avg_factors + ci, color=color, alpha=0.3)\n\n        ax.set_xlabel(x_label, fontsize=font_size)\n        ax.set_ylabel(factor_col, fontsize=font_size, labelpad=20)\n        ax.legend(title=separate_by, fontsize=font_size, loc='upper left', bbox_to_anchor=(1, 1))\n        ax.set_title(f'Time Series of {factor_col}', fontsize=font_size)\n        plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout to fit legend\n\n    plt.show()\n</code></pre>"},{"location":"api/#SPTnano.plot_trajectory","title":"<code>plot_trajectory(traj, colorby='particle', mpp=None, label=False, superimpose=None, cmap=None, ax=None, t_column=None, pos_columns=None, plot_style={}, **kwargs)</code>","text":"<p>Plot traces of trajectories for each particle. Optionally superimpose it on a frame from the video.</p>"},{"location":"api/#SPTnano.plot_trajectory--parameters","title":"Parameters","text":"<p>traj : DataFrame     The DataFrame should include time and spatial coordinate columns. colorby : {'particle', 'frame'}, optional mpp : float, optional     Microns per pixel. If omitted, the labels will have units of pixels. label : boolean, optional     Set to True to write particle ID numbers next to trajectories. superimpose : ndarray, optional     Background image, default None cmap : colormap, optional     This is only used in colorby='frame' mode. Default = mpl.cm.winter ax : matplotlib axes object, optional     Defaults to current axes t_column : string, optional     DataFrame column name for time coordinate. Default is 'frame'. pos_columns : list of strings, optional     Dataframe column names for spatial coordinates. Default is ['x', 'y']. plot_style : dictionary     Keyword arguments passed through to the <code>Axes.plot(...)</code> command</p>"},{"location":"api/#SPTnano.plot_trajectory--returns","title":"Returns","text":"<p>Axes object</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_trajectory(traj, colorby='particle', mpp=None, label=False,\n                    superimpose=None, cmap=None, ax=None, t_column=None,\n                    pos_columns=None, plot_style={}, **kwargs):\n    \"\"\"\n    Plot traces of trajectories for each particle.\n    Optionally superimpose it on a frame from the video.\n\n    Parameters\n    ----------\n    traj : DataFrame\n        The DataFrame should include time and spatial coordinate columns.\n    colorby : {'particle', 'frame'}, optional\n    mpp : float, optional\n        Microns per pixel. If omitted, the labels will have units of pixels.\n    label : boolean, optional\n        Set to True to write particle ID numbers next to trajectories.\n    superimpose : ndarray, optional\n        Background image, default None\n    cmap : colormap, optional\n        This is only used in colorby='frame' mode. Default = mpl.cm.winter\n    ax : matplotlib axes object, optional\n        Defaults to current axes\n    t_column : string, optional\n        DataFrame column name for time coordinate. Default is 'frame'.\n    pos_columns : list of strings, optional\n        Dataframe column names for spatial coordinates. Default is ['x', 'y'].\n    plot_style : dictionary\n        Keyword arguments passed through to the `Axes.plot(...)` command\n\n    Returns\n    -------\n    Axes object\n    \"\"\"\n    if cmap is None:\n        cmap = plt.cm.winter\n    if t_column is None:\n        t_column = 'frame'\n    if pos_columns is None:\n        pos_columns = ['x', 'y']\n    if len(traj) == 0:\n        raise ValueError(\"DataFrame of trajectories is empty.\")\n\n    _plot_style = dict(linewidth=1)\n    _plot_style.update(**plot_style)\n\n    if ax is None:\n        ax = plt.gca()\n\n    # Axes labels\n    if mpp is None:\n        ax.set_xlabel(f'{pos_columns[0]} [px]')\n        ax.set_ylabel(f'{pos_columns[1]} [px]')\n        mpp = 1.  # for computations of image extent below\n    else:\n        ax.set_xlabel(f'{pos_columns[0]} [\u03bcm]')\n        ax.set_ylabel(f'{pos_columns[1]} [\u03bcm]')\n\n    # Background image\n    if superimpose is not None:\n        ax.imshow(superimpose, cmap=plt.cm.gray,\n                  origin='lower', interpolation='nearest',\n                  vmin=kwargs.get('vmin'), vmax=kwargs.get('vmax'))\n        ax.set_xlim(-0.5 * mpp, (superimpose.shape[1] - 0.5) * mpp)\n        ax.set_ylim(-0.5 * mpp, (superimpose.shape[0] - 0.5) * mpp)\n\n    # Trajectories\n    if colorby == 'particle':\n        # Unstack particles into columns.\n        unstacked = traj.set_index(['particle', t_column])[pos_columns].unstack()\n        for i, trajectory in unstacked.iterrows():\n            ax.plot(mpp * trajectory[pos_columns[0]], mpp * trajectory[pos_columns[1]], **_plot_style)\n    elif colorby == 'frame':\n        # Read http://www.scipy.org/Cookbook/Matplotlib/MulticoloredLine\n        x = traj.set_index([t_column, 'particle'])[pos_columns[0]].unstack()\n        y = traj.set_index([t_column, 'particle'])[pos_columns[1]].unstack()\n        color_numbers = traj[t_column].values / float(traj[t_column].max())\n        for particle in x:\n            points = np.array([x[particle].values, y[particle].values]).T.reshape(-1, 1, 2)\n            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n            lc = LineCollection(segments, cmap=cmap)\n            lc.set_array(color_numbers)\n            ax.add_collection(lc)\n            ax.set_xlim(x.apply(np.min).min(), x.apply(np.max).max())\n            ax.set_ylim(y.apply(np.min).min(), y.apply(np.max).max())\n\n    if label:\n        unstacked = traj.set_index([t_column, 'particle'])[pos_columns].unstack()\n        first_frame = int(traj[t_column].min())\n        coords = unstacked.fillna(method='backfill').stack().loc[first_frame]\n        for particle_id, coord in coords.iterrows():\n            ax.text(*coord.tolist(), s=\"%d\" % particle_id,\n                    horizontalalignment='center',\n                    verticalalignment='center')\n\n    ax.invert_yaxis()\n    return ax\n</code></pre>"},{"location":"api/#SPTnano.plot_violinplots","title":"<code>plot_violinplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', talk=False)</code>","text":"<p>Plot violin plots of a specified factor, with data separated by categories.</p>"},{"location":"api/#SPTnano.plot_violinplots--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing the data. factor_col : str, optional     The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'. separate_by : str, optional     Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'. palette : str, optional     Color palette for the plot. Default is 'colorblind'. talk : bool, optional     Whether to set the figure size to the original large size or a smaller size. Default is False.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_violinplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', talk=False):\n    \"\"\"\n    Plot violin plots of a specified factor, with data separated by categories.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing the data.\n    factor_col : str, optional\n        The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'.\n    separate_by : str, optional\n        Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'.\n    palette : str, optional\n        Color palette for the plot. Default is 'colorblind'.\n    talk : bool, optional\n        Whether to set the figure size to the original large size or a smaller size. Default is False.\n    \"\"\"\n\n    unique_categories = data_df[separate_by].unique() if separate_by else [None]\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    # Set figure size based on the `talk` parameter\n    if talk:\n        fig_size = (20, 12)\n        font_size = 35\n    else:\n        fig_size = (5, 3)\n        font_size = 14\n\n    fig, ax = plt.subplots(figsize=fig_size)\n    sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5, \"font.size\": font_size, \"axes.titlesize\": font_size, \"axes.labelsize\": font_size, \"xtick.labelsize\": font_size, \"ytick.labelsize\": font_size})\n\n    # Plot violin plot\n    sns.violinplot(x=separate_by, y=factor_col, hue=separate_by, data=data_df, palette=color_palette, ax=ax, legend=False, alpha=0.79)\n\n    # Remove 'Condition_' prefix from x tick labels\n    new_labels = [label.replace('Condition_', '') for label in unique_categories]\n    ax.set_xticks(range(len(new_labels)))\n    ax.set_xticklabels(new_labels, fontsize=font_size)\n\n    ax.set_ylabel(factor_col, fontsize=font_size, labelpad=20)\n    ax.set_xlabel(None)\n    ax.tick_params(axis='both', which='major', labelsize=font_size)\n    plt.tight_layout()\n\n    plt.show()\n</code></pre>"},{"location":"api/#SPTnano.features","title":"<code>features</code>","text":""},{"location":"api/#SPTnano.features.ParticleMetrics","title":"<code>ParticleMetrics</code>","text":"Source code in <code>src/SPTnano/features.py</code> <pre><code>class ParticleMetrics:\n    def __init__(self, df):\n        self.df = df\n        self.df['Location'] = self.df['filename'].apply(self.extract_location)  # Add Location column\n        self.metrics_df = self.df.copy()\n        self.time_averaged_df = pd.DataFrame(columns=[\n            'x_um_start', 'y_um_start', 'x_um_end', 'y_um_end', \n            'particle', 'condition', 'filename', 'file_id', 'unique_id',\n            'avg_msd', 'n_frames', 'total_time_s', 'Location', \n            'diffusion_coefficient', 'anomalous_exponent', 'motion_class'  # Add columns for diffusion coefficient, anomalous exponent, and motion class\n        ])\n\n    @staticmethod\n    def extract_location(filename):\n        match = re.match(r'loc-(\\w{2})_', filename)\n        if match:\n            return match.group(1)\n        return 'Unknown'  # Default value if no location is found\n\n    def calculate_distances(self):\n        \"\"\"\n        Calculate the distances between consecutive frames for each particle in micrometers.\n        \"\"\"\n        self.metrics_df = self.metrics_df.sort_values(by=['unique_id', 'frame'])\n        self.metrics_df[['x_um_prev', 'y_um_prev']] = self.metrics_df.groupby('unique_id')[['x_um', 'y_um']].shift(1)\n        self.metrics_df['segment_len_um'] = np.sqrt(\n            (self.metrics_df['x_um'] - self.metrics_df['x_um_prev'])**2 + \n            (self.metrics_df['y_um'] - self.metrics_df['y_um_prev'])**2\n        )\n        # Fill NaN values with 0\n        self.metrics_df['segment_len_um'] = self.metrics_df['segment_len_um'].fillna(0)\n        return self.metrics_df\n\n    def calculate_speeds(self):\n        \"\"\"\n        Calculate the speed between consecutive frames for each particle in micrometers per second.\n        \"\"\"\n        self.metrics_df[['time_s_prev']] = self.metrics_df.groupby('unique_id')[['time_s']].shift(1)\n        self.metrics_df['delta_time_s'] = self.metrics_df['time_s'] - self.metrics_df['time_s_prev']\n        self.metrics_df['speed_um_s'] = self.metrics_df['segment_len_um'] / self.metrics_df['delta_time_s']\n        # Fill NaN and infinite values with 0\n        self.metrics_df['speed_um_s'] = self.metrics_df['speed_um_s'].replace([np.inf, -np.inf], np.nan).fillna(0)\n        return self.metrics_df\n\n\n\n    def calculate_directions(self):\n        \"\"\"\n        Calculate the direction of motion between consecutive frames for each particle in radians.\n        \"\"\"\n        self.metrics_df['direction_rad'] = np.arctan2(\n            self.metrics_df['y_um'] - self.metrics_df['y_um_prev'],\n            self.metrics_df['x_um'] - self.metrics_df['x_um_prev']\n        )\n        # Fill NaN values with 0\n        self.metrics_df['direction_rad'] = self.metrics_df['direction_rad'].fillna(0)\n        return self.metrics_df\n\n\n    def calculate_accelerations(self):\n        \"\"\"\n        Calculate the acceleration between consecutive frames for each particle in micrometers per second squared.\n        \"\"\"\n        self.metrics_df[['speed_um_s_prev']] = self.metrics_df.groupby('unique_id')[['speed_um_s']].shift(1)\n        self.metrics_df['acceleration_um_s2'] = (self.metrics_df['speed_um_s'] - self.metrics_df['speed_um_s_prev']) / self.metrics_df['delta_time_s']\n        # Fill NaN and infinite values with 0\n        self.metrics_df['acceleration_um_s2'] = self.metrics_df['acceleration_um_s2'].replace([np.inf, -np.inf], np.nan).fillna(0)\n        return self.metrics_df\n\n    def calculate_jerk(self):\n        \"\"\"\n        Calculate the jerk between consecutive frames for each particle in micrometers per second cubed.\n        \"\"\"\n        self.metrics_df[['acceleration_um_s2_prev']] = self.metrics_df.groupby('unique_id')[['acceleration_um_s2']].shift(1)\n        self.metrics_df['jerk_um_s3'] = (self.metrics_df['acceleration_um_s2'] - self.metrics_df['acceleration_um_s2_prev']) / self.metrics_df['delta_time_s']\n        # Fill NaN and infinite values with 0\n        self.metrics_df['jerk_um_s3'] = self.metrics_df['jerk_um_s3'].replace([np.inf, -np.inf], np.nan).fillna(0)\n        return self.metrics_df\n\n    def calculate_normalized_curvature(self):\n        \"\"\"\n        Calculate the curvature normalized by distance between consecutive frames for each particle.\n        \"\"\"\n        self.metrics_df[['direction_rad_prev']] = self.metrics_df.groupby('unique_id')[['direction_rad']].shift(1)\n        self.metrics_df['normalized_curvature'] = (self.metrics_df['direction_rad'] - self.metrics_df['direction_rad_prev']) / self.metrics_df['segment_len_um']\n        # Fill NaN and infinite values with 0\n        self.metrics_df['normalized_curvature'] = self.metrics_df['normalized_curvature'].replace([np.inf, -np.inf], np.nan).fillna(0)\n        return self.metrics_df\n\n    def calculate_angle_normalized_curvature(self):\n        \"\"\"\n        Calculate the curvature (change in direction) normalized to the range [-pi, pi] between consecutive frames for each particle.\n        \"\"\"\n        self.metrics_df[['direction_rad_prev']] = self.metrics_df.groupby('unique_id')[['direction_rad']].shift(1)\n        self.metrics_df['angle_normalized_curvature'] = self.metrics_df['direction_rad'] - self.metrics_df['direction_rad_prev']\n        # Normalize curvature to the range [-pi, pi]\n        self.metrics_df['angle_normalized_curvature'] = (self.metrics_df['angle_normalized_curvature'] + np.pi) % (2 * np.pi) - np.pi\n        # Fill NaN values with 0\n        self.metrics_df['angle_normalized_curvature'] = self.metrics_df['angle_normalized_curvature'].fillna(0)\n        return self.metrics_df\n\n    def calculate_net_displacement(self):\n        \"\"\"\n        Calculate the net displacement from the starting point for each particle in micrometers.\n        \"\"\"\n        self.metrics_df[['x_um_start', 'y_um_start']] = self.metrics_df.groupby('unique_id')[['x_um', 'y_um']].transform('first')\n        self.metrics_df['net_displacement_um'] = np.sqrt(\n            (self.metrics_df['x_um'] - self.metrics_df['x_um_start'])**2 + \n            (self.metrics_df['y_um'] - self.metrics_df['y_um_start'])**2\n        )\n        return self.metrics_df\n\n    def calculate_instantaneous_diffusion_coefficient(self):\n        \"\"\"\n        Calculate the instantaneous diffusion coefficient for each particle in square micrometers per second.\n        \"\"\"\n        self.metrics_df['instant_diff_coeff'] = self.metrics_df['segment_len_um']**2 / (4 * self.metrics_df['delta_time_s'])\n        # Fill NaN and infinite values with 0\n        self.metrics_df['instant_diff_coeff'] = self.metrics_df['instant_diff_coeff'].replace([np.inf, -np.inf], np.nan).fillna(0)\n        return self.metrics_df\n\n    def calculate_instantaneous_velocity(self):\n        \"\"\"\n        Calculate the instantaneous velocity between consecutive frames for each particle in micrometers per second.\n        \"\"\"\n        self.metrics_df['instant_velocity_x_um_s'] = (self.metrics_df['x_um'] - self.metrics_df['x_um_prev']) / self.metrics_df['delta_time_s']\n        self.metrics_df['instant_velocity_y_um_s'] = (self.metrics_df['y_um'] - self.metrics_df['y_um_prev']) / self.metrics_df['delta_time_s']\n        # Fill NaN and infinite values with 0\n        self.metrics_df['instant_velocity_x_um_s'] = self.metrics_df['instant_velocity_x_um_s'].replace([np.inf, -np.inf], np.nan).fillna(0)\n        self.metrics_df['instant_velocity_y_um_s'] = self.metrics_df['instant_velocity_y_um_s'].replace([np.inf, -np.inf], np.nan).fillna(0)\n        return self.metrics_df\n\n    def calculate_default_max_lagtime(self):\n        \"\"\"\n        Calculate the default maximum lag time based on the shortest track in the dataset.\n        \"\"\"\n        min_track_length = self.metrics_df.groupby('unique_id').size().min()\n        default_max_lagtime = min(100, int(min_track_length / 2))  # Example: use half the length of the shortest track or 100, whichever is smaller\n        return default_max_lagtime\n\n    @staticmethod\n    def msd_model(t, D, alpha):\n        return 4 * D * t**alpha\n\n    def get_time_averaged_df(self):\n        \"\"\"\n        Return the DataFrame with time-averaged metrics.\n        \"\"\"\n        return self.time_averaged_df\n\n\n    def calculate_msd(self, max_lagtime):\n        \"\"\"\n        Calculate the time-averaged MSD for each track and aggregate across all tracks.\n        Parameters:\n        - max_lagtime: maximum number of frames to consider for lag times\n        \"\"\"\n        msd_list = []\n\n        for unique_id, track_data in tqdm(self.metrics_df.groupby('unique_id'), desc=\"Calculating MSD\"):\n            n_frames = len(track_data)\n            msd_values = np.zeros(max_lagtime)\n            counts = np.zeros(max_lagtime)\n\n            for lag in range(1, max_lagtime + 1):\n                if lag &lt; n_frames:\n                    displacements = (track_data[['x_um', 'y_um']].iloc[lag:].values - track_data[['x_um', 'y_um']].iloc[:-lag].values) ** 2\n                    squared_displacements = np.sum(displacements, axis=1)\n                    msd_values[lag - 1] = np.mean(squared_displacements)\n                    counts[lag - 1] = len(squared_displacements)\n                else:\n                    break\n\n            avg_msd = np.mean(msd_values)  # Calculate the average MSD for the track (units: \u03bcm\u00b2)\n            msd_list.append(pd.DataFrame({\n                'unique_id': unique_id,\n                'lag_time': np.arange(1, max_lagtime + 1),\n                'msd': msd_values,\n                'count': counts\n            }))\n\n            # Calculate total time in seconds for the track\n            total_time_s = (track_data['time_s'].iloc[-1] - track_data['time_s'].iloc[0])\n\n            # Fit MSD data to determine the type of motion and extract parameters\n            lag_times = np.arange(1, max_lagtime + 1) * (total_time_s / (n_frames - 1))\n            popt, _ = scipy.optimize.curve_fit(self.msd_model, lag_times, msd_values[:max_lagtime])\n            D, alpha = popt[0], popt[1]\n\n            # Classify the type of motion\n            if alpha &lt; 1:\n                motion_class = 'subdiffusive'\n            elif alpha &gt; 1:\n                motion_class = 'superdiffusive'\n            else:\n                motion_class = 'normal'\n\n            # Add track-level summary information to time_averaged_df\n            start_row = track_data.iloc[0]\n            end_row = track_data.iloc[-1]\n            track_summary = pd.DataFrame({\n                'x_um_start': [start_row['x_um']],\n                'y_um_start': [start_row['y_um']],\n                'x_um_end': [end_row['x_um']],\n                'y_um_end': [end_row['y_um']],\n                'particle': [start_row['particle']],\n                'condition': [start_row['condition']],\n                'filename': [start_row['filename']],\n                'file_id': [start_row['file_id']],\n                'unique_id': [unique_id],\n                'avg_msd': [avg_msd],  # Add the average MSD (units: \u03bcm\u00b2)\n                'n_frames': [n_frames],  # Add the number of frames\n                'total_time_s': [total_time_s],  # Add the total time in seconds\n                'Location': [start_row['Location']],  # Add the Location\n                'diffusion_coefficient': [D],  # Add the diffusion coefficient\n                'anomalous_exponent': [alpha],  # Add the anomalous exponent\n                'motion_class': [motion_class],  # Add the motion class\n                # Placeholder for additional metrics\n                # 'additional_metric': None,\n            })\n\n            if self.time_averaged_df.empty:\n                self.time_averaged_df = track_summary\n            else:\n                self.time_averaged_df = pd.concat([self.time_averaged_df, track_summary], ignore_index=True)\n\n        self.msd_df = pd.concat(msd_list).reset_index(drop=True)\n        return self.msd_df\n\n\n\n\n\n\n    def calculate_all_features(self, max_lagtime=None):\n        \"\"\"\n        Calculate all features for the particle tracking data.\n        This method will call all individual feature calculation methods.\n        \"\"\"\n        # Calculate default max lag time if not provided\n        if max_lagtime is None:\n            max_lagtime = self.calculate_default_max_lagtime()\n\n        # Calculate distances between consecutive frames\n        self.calculate_distances()\n\n        # Calculate speeds between consecutive frames\n        self.calculate_speeds()\n\n        # Calculate directions of motion between consecutive frames\n        self.calculate_directions()\n\n        # Calculate accelerations between consecutive frames\n        self.calculate_accelerations()\n\n        # Calculate jerks between consecutive frames\n        self.calculate_jerk()\n\n        # Calculate curvatures between consecutive frames\n        self.calculate_normalized_curvature()\n        self.calculate_angle_normalized_curvature()\n\n        # Calculate net displacement\n        self.calculate_net_displacement()\n\n\n        # Calculate instantaneous diffusion coefficient between consecutive frames\n        self.calculate_instantaneous_diffusion_coefficient()\n\n        # Calculate instantaneous velocity\n        self.calculate_instantaneous_velocity()\n\n        # Calculate MSD for each track and aggregate\n        self.calculate_msd(max_lagtime)\n\n        # Cleanup step to remove temporary columns\n        self.cleanup()\n\n        return self.metrics_df\n\n    def cleanup(self):\n        \"\"\"\n        Cleanup the dataframe by dropping unnecessary columns after all features are calculated.\n        \"\"\"\n        self.metrics_df.drop(columns=[\n            'x_um_prev', 'y_um_prev', 'time_s_prev', 'delta_time_s', \n            'speed_um_s_prev', 'acceleration_um_s2_prev', 'direction_rad_prev',\n            'instant_velocity_x_um_s', 'instant_velocity_y_um_s',\n            ], inplace=True)\n\n    def get_metrics_df(self):\n        \"\"\"\n        Return the dataframe with calculated metrics.\n        \"\"\"\n        return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_accelerations","title":"<code>calculate_accelerations()</code>","text":"<p>Calculate the acceleration between consecutive frames for each particle in micrometers per second squared.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_accelerations(self):\n    \"\"\"\n    Calculate the acceleration between consecutive frames for each particle in micrometers per second squared.\n    \"\"\"\n    self.metrics_df[['speed_um_s_prev']] = self.metrics_df.groupby('unique_id')[['speed_um_s']].shift(1)\n    self.metrics_df['acceleration_um_s2'] = (self.metrics_df['speed_um_s'] - self.metrics_df['speed_um_s_prev']) / self.metrics_df['delta_time_s']\n    # Fill NaN and infinite values with 0\n    self.metrics_df['acceleration_um_s2'] = self.metrics_df['acceleration_um_s2'].replace([np.inf, -np.inf], np.nan).fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_all_features","title":"<code>calculate_all_features(max_lagtime=None)</code>","text":"<p>Calculate all features for the particle tracking data. This method will call all individual feature calculation methods.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_all_features(self, max_lagtime=None):\n    \"\"\"\n    Calculate all features for the particle tracking data.\n    This method will call all individual feature calculation methods.\n    \"\"\"\n    # Calculate default max lag time if not provided\n    if max_lagtime is None:\n        max_lagtime = self.calculate_default_max_lagtime()\n\n    # Calculate distances between consecutive frames\n    self.calculate_distances()\n\n    # Calculate speeds between consecutive frames\n    self.calculate_speeds()\n\n    # Calculate directions of motion between consecutive frames\n    self.calculate_directions()\n\n    # Calculate accelerations between consecutive frames\n    self.calculate_accelerations()\n\n    # Calculate jerks between consecutive frames\n    self.calculate_jerk()\n\n    # Calculate curvatures between consecutive frames\n    self.calculate_normalized_curvature()\n    self.calculate_angle_normalized_curvature()\n\n    # Calculate net displacement\n    self.calculate_net_displacement()\n\n\n    # Calculate instantaneous diffusion coefficient between consecutive frames\n    self.calculate_instantaneous_diffusion_coefficient()\n\n    # Calculate instantaneous velocity\n    self.calculate_instantaneous_velocity()\n\n    # Calculate MSD for each track and aggregate\n    self.calculate_msd(max_lagtime)\n\n    # Cleanup step to remove temporary columns\n    self.cleanup()\n\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_angle_normalized_curvature","title":"<code>calculate_angle_normalized_curvature()</code>","text":"<p>Calculate the curvature (change in direction) normalized to the range [-pi, pi] between consecutive frames for each particle.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_angle_normalized_curvature(self):\n    \"\"\"\n    Calculate the curvature (change in direction) normalized to the range [-pi, pi] between consecutive frames for each particle.\n    \"\"\"\n    self.metrics_df[['direction_rad_prev']] = self.metrics_df.groupby('unique_id')[['direction_rad']].shift(1)\n    self.metrics_df['angle_normalized_curvature'] = self.metrics_df['direction_rad'] - self.metrics_df['direction_rad_prev']\n    # Normalize curvature to the range [-pi, pi]\n    self.metrics_df['angle_normalized_curvature'] = (self.metrics_df['angle_normalized_curvature'] + np.pi) % (2 * np.pi) - np.pi\n    # Fill NaN values with 0\n    self.metrics_df['angle_normalized_curvature'] = self.metrics_df['angle_normalized_curvature'].fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_default_max_lagtime","title":"<code>calculate_default_max_lagtime()</code>","text":"<p>Calculate the default maximum lag time based on the shortest track in the dataset.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_default_max_lagtime(self):\n    \"\"\"\n    Calculate the default maximum lag time based on the shortest track in the dataset.\n    \"\"\"\n    min_track_length = self.metrics_df.groupby('unique_id').size().min()\n    default_max_lagtime = min(100, int(min_track_length / 2))  # Example: use half the length of the shortest track or 100, whichever is smaller\n    return default_max_lagtime\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_directions","title":"<code>calculate_directions()</code>","text":"<p>Calculate the direction of motion between consecutive frames for each particle in radians.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_directions(self):\n    \"\"\"\n    Calculate the direction of motion between consecutive frames for each particle in radians.\n    \"\"\"\n    self.metrics_df['direction_rad'] = np.arctan2(\n        self.metrics_df['y_um'] - self.metrics_df['y_um_prev'],\n        self.metrics_df['x_um'] - self.metrics_df['x_um_prev']\n    )\n    # Fill NaN values with 0\n    self.metrics_df['direction_rad'] = self.metrics_df['direction_rad'].fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_distances","title":"<code>calculate_distances()</code>","text":"<p>Calculate the distances between consecutive frames for each particle in micrometers.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_distances(self):\n    \"\"\"\n    Calculate the distances between consecutive frames for each particle in micrometers.\n    \"\"\"\n    self.metrics_df = self.metrics_df.sort_values(by=['unique_id', 'frame'])\n    self.metrics_df[['x_um_prev', 'y_um_prev']] = self.metrics_df.groupby('unique_id')[['x_um', 'y_um']].shift(1)\n    self.metrics_df['segment_len_um'] = np.sqrt(\n        (self.metrics_df['x_um'] - self.metrics_df['x_um_prev'])**2 + \n        (self.metrics_df['y_um'] - self.metrics_df['y_um_prev'])**2\n    )\n    # Fill NaN values with 0\n    self.metrics_df['segment_len_um'] = self.metrics_df['segment_len_um'].fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_instantaneous_diffusion_coefficient","title":"<code>calculate_instantaneous_diffusion_coefficient()</code>","text":"<p>Calculate the instantaneous diffusion coefficient for each particle in square micrometers per second.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_instantaneous_diffusion_coefficient(self):\n    \"\"\"\n    Calculate the instantaneous diffusion coefficient for each particle in square micrometers per second.\n    \"\"\"\n    self.metrics_df['instant_diff_coeff'] = self.metrics_df['segment_len_um']**2 / (4 * self.metrics_df['delta_time_s'])\n    # Fill NaN and infinite values with 0\n    self.metrics_df['instant_diff_coeff'] = self.metrics_df['instant_diff_coeff'].replace([np.inf, -np.inf], np.nan).fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_instantaneous_velocity","title":"<code>calculate_instantaneous_velocity()</code>","text":"<p>Calculate the instantaneous velocity between consecutive frames for each particle in micrometers per second.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_instantaneous_velocity(self):\n    \"\"\"\n    Calculate the instantaneous velocity between consecutive frames for each particle in micrometers per second.\n    \"\"\"\n    self.metrics_df['instant_velocity_x_um_s'] = (self.metrics_df['x_um'] - self.metrics_df['x_um_prev']) / self.metrics_df['delta_time_s']\n    self.metrics_df['instant_velocity_y_um_s'] = (self.metrics_df['y_um'] - self.metrics_df['y_um_prev']) / self.metrics_df['delta_time_s']\n    # Fill NaN and infinite values with 0\n    self.metrics_df['instant_velocity_x_um_s'] = self.metrics_df['instant_velocity_x_um_s'].replace([np.inf, -np.inf], np.nan).fillna(0)\n    self.metrics_df['instant_velocity_y_um_s'] = self.metrics_df['instant_velocity_y_um_s'].replace([np.inf, -np.inf], np.nan).fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_jerk","title":"<code>calculate_jerk()</code>","text":"<p>Calculate the jerk between consecutive frames for each particle in micrometers per second cubed.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_jerk(self):\n    \"\"\"\n    Calculate the jerk between consecutive frames for each particle in micrometers per second cubed.\n    \"\"\"\n    self.metrics_df[['acceleration_um_s2_prev']] = self.metrics_df.groupby('unique_id')[['acceleration_um_s2']].shift(1)\n    self.metrics_df['jerk_um_s3'] = (self.metrics_df['acceleration_um_s2'] - self.metrics_df['acceleration_um_s2_prev']) / self.metrics_df['delta_time_s']\n    # Fill NaN and infinite values with 0\n    self.metrics_df['jerk_um_s3'] = self.metrics_df['jerk_um_s3'].replace([np.inf, -np.inf], np.nan).fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_msd","title":"<code>calculate_msd(max_lagtime)</code>","text":"<p>Calculate the time-averaged MSD for each track and aggregate across all tracks. Parameters: - max_lagtime: maximum number of frames to consider for lag times</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_msd(self, max_lagtime):\n    \"\"\"\n    Calculate the time-averaged MSD for each track and aggregate across all tracks.\n    Parameters:\n    - max_lagtime: maximum number of frames to consider for lag times\n    \"\"\"\n    msd_list = []\n\n    for unique_id, track_data in tqdm(self.metrics_df.groupby('unique_id'), desc=\"Calculating MSD\"):\n        n_frames = len(track_data)\n        msd_values = np.zeros(max_lagtime)\n        counts = np.zeros(max_lagtime)\n\n        for lag in range(1, max_lagtime + 1):\n            if lag &lt; n_frames:\n                displacements = (track_data[['x_um', 'y_um']].iloc[lag:].values - track_data[['x_um', 'y_um']].iloc[:-lag].values) ** 2\n                squared_displacements = np.sum(displacements, axis=1)\n                msd_values[lag - 1] = np.mean(squared_displacements)\n                counts[lag - 1] = len(squared_displacements)\n            else:\n                break\n\n        avg_msd = np.mean(msd_values)  # Calculate the average MSD for the track (units: \u03bcm\u00b2)\n        msd_list.append(pd.DataFrame({\n            'unique_id': unique_id,\n            'lag_time': np.arange(1, max_lagtime + 1),\n            'msd': msd_values,\n            'count': counts\n        }))\n\n        # Calculate total time in seconds for the track\n        total_time_s = (track_data['time_s'].iloc[-1] - track_data['time_s'].iloc[0])\n\n        # Fit MSD data to determine the type of motion and extract parameters\n        lag_times = np.arange(1, max_lagtime + 1) * (total_time_s / (n_frames - 1))\n        popt, _ = scipy.optimize.curve_fit(self.msd_model, lag_times, msd_values[:max_lagtime])\n        D, alpha = popt[0], popt[1]\n\n        # Classify the type of motion\n        if alpha &lt; 1:\n            motion_class = 'subdiffusive'\n        elif alpha &gt; 1:\n            motion_class = 'superdiffusive'\n        else:\n            motion_class = 'normal'\n\n        # Add track-level summary information to time_averaged_df\n        start_row = track_data.iloc[0]\n        end_row = track_data.iloc[-1]\n        track_summary = pd.DataFrame({\n            'x_um_start': [start_row['x_um']],\n            'y_um_start': [start_row['y_um']],\n            'x_um_end': [end_row['x_um']],\n            'y_um_end': [end_row['y_um']],\n            'particle': [start_row['particle']],\n            'condition': [start_row['condition']],\n            'filename': [start_row['filename']],\n            'file_id': [start_row['file_id']],\n            'unique_id': [unique_id],\n            'avg_msd': [avg_msd],  # Add the average MSD (units: \u03bcm\u00b2)\n            'n_frames': [n_frames],  # Add the number of frames\n            'total_time_s': [total_time_s],  # Add the total time in seconds\n            'Location': [start_row['Location']],  # Add the Location\n            'diffusion_coefficient': [D],  # Add the diffusion coefficient\n            'anomalous_exponent': [alpha],  # Add the anomalous exponent\n            'motion_class': [motion_class],  # Add the motion class\n            # Placeholder for additional metrics\n            # 'additional_metric': None,\n        })\n\n        if self.time_averaged_df.empty:\n            self.time_averaged_df = track_summary\n        else:\n            self.time_averaged_df = pd.concat([self.time_averaged_df, track_summary], ignore_index=True)\n\n    self.msd_df = pd.concat(msd_list).reset_index(drop=True)\n    return self.msd_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_net_displacement","title":"<code>calculate_net_displacement()</code>","text":"<p>Calculate the net displacement from the starting point for each particle in micrometers.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_net_displacement(self):\n    \"\"\"\n    Calculate the net displacement from the starting point for each particle in micrometers.\n    \"\"\"\n    self.metrics_df[['x_um_start', 'y_um_start']] = self.metrics_df.groupby('unique_id')[['x_um', 'y_um']].transform('first')\n    self.metrics_df['net_displacement_um'] = np.sqrt(\n        (self.metrics_df['x_um'] - self.metrics_df['x_um_start'])**2 + \n        (self.metrics_df['y_um'] - self.metrics_df['y_um_start'])**2\n    )\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_normalized_curvature","title":"<code>calculate_normalized_curvature()</code>","text":"<p>Calculate the curvature normalized by distance between consecutive frames for each particle.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_normalized_curvature(self):\n    \"\"\"\n    Calculate the curvature normalized by distance between consecutive frames for each particle.\n    \"\"\"\n    self.metrics_df[['direction_rad_prev']] = self.metrics_df.groupby('unique_id')[['direction_rad']].shift(1)\n    self.metrics_df['normalized_curvature'] = (self.metrics_df['direction_rad'] - self.metrics_df['direction_rad_prev']) / self.metrics_df['segment_len_um']\n    # Fill NaN and infinite values with 0\n    self.metrics_df['normalized_curvature'] = self.metrics_df['normalized_curvature'].replace([np.inf, -np.inf], np.nan).fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.calculate_speeds","title":"<code>calculate_speeds()</code>","text":"<p>Calculate the speed between consecutive frames for each particle in micrometers per second.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def calculate_speeds(self):\n    \"\"\"\n    Calculate the speed between consecutive frames for each particle in micrometers per second.\n    \"\"\"\n    self.metrics_df[['time_s_prev']] = self.metrics_df.groupby('unique_id')[['time_s']].shift(1)\n    self.metrics_df['delta_time_s'] = self.metrics_df['time_s'] - self.metrics_df['time_s_prev']\n    self.metrics_df['speed_um_s'] = self.metrics_df['segment_len_um'] / self.metrics_df['delta_time_s']\n    # Fill NaN and infinite values with 0\n    self.metrics_df['speed_um_s'] = self.metrics_df['speed_um_s'].replace([np.inf, -np.inf], np.nan).fillna(0)\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.cleanup","title":"<code>cleanup()</code>","text":"<p>Cleanup the dataframe by dropping unnecessary columns after all features are calculated.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def cleanup(self):\n    \"\"\"\n    Cleanup the dataframe by dropping unnecessary columns after all features are calculated.\n    \"\"\"\n    self.metrics_df.drop(columns=[\n        'x_um_prev', 'y_um_prev', 'time_s_prev', 'delta_time_s', \n        'speed_um_s_prev', 'acceleration_um_s2_prev', 'direction_rad_prev',\n        'instant_velocity_x_um_s', 'instant_velocity_y_um_s',\n        ], inplace=True)\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.get_metrics_df","title":"<code>get_metrics_df()</code>","text":"<p>Return the dataframe with calculated metrics.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def get_metrics_df(self):\n    \"\"\"\n    Return the dataframe with calculated metrics.\n    \"\"\"\n    return self.metrics_df\n</code></pre>"},{"location":"api/#SPTnano.features.ParticleMetrics.get_time_averaged_df","title":"<code>get_time_averaged_df()</code>","text":"<p>Return the DataFrame with time-averaged metrics.</p> Source code in <code>src/SPTnano/features.py</code> <pre><code>def get_time_averaged_df(self):\n    \"\"\"\n    Return the DataFrame with time-averaged metrics.\n    \"\"\"\n    return self.time_averaged_df\n</code></pre>"},{"location":"api/#SPTnano.helper_scripts","title":"<code>helper_scripts</code>","text":""},{"location":"api/#SPTnano.helper_scripts.add_microns_and_secs","title":"<code>add_microns_and_secs(df, pixelsize_microns, time_between_frames)</code>","text":"<p>Adds columns to the DataFrame with positions in microns and time in seconds</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def add_microns_and_secs(df, pixelsize_microns, time_between_frames):\n    '''Adds columns to the DataFrame with positions in microns and time in seconds'''\n    #space transformations\n    df['x_um'] = df['x'] * pixelsize_microns\n    df['y_um'] = df['y'] * pixelsize_microns\n\n    df['frame_zeroed'] = df.groupby('particle')['frame'].transform(lambda x: x - x.iloc[0])\n    df['time_s'] = df['frame'] * time_between_frames\n    df['time_s_zeroed'] = df.groupby('particle')['time_s'].transform(lambda x: x - x.iloc[0])\n    return df\n</code></pre>"},{"location":"api/#SPTnano.helper_scripts.filter_high_speeds","title":"<code>filter_high_speeds(metrics_df, speed_threshold)</code>","text":"<p>Filter based on speed instead - can be relevant if you have different exposure times and different times between frames</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def filter_high_speeds(metrics_df, speed_threshold):\n    '''\n    Filter based on speed instead - can be relevant if you have different exposure times and different times between frames\n    '''\n\n    # Identify unique_ids with any high speeds\n    high_speed_particles = metrics_df[metrics_df['speed_um_s'] &gt; speed_threshold]['unique_id'].unique()\n\n    # Filter out particles with high speeds\n    metrics_df_filtered = metrics_df[~metrics_df['unique_id'].isin(high_speed_particles)].copy()\n    return metrics_df_filtered\n</code></pre>"},{"location":"api/#SPTnano.helper_scripts.filter_large_jumps","title":"<code>filter_large_jumps(df, threshold)</code>","text":"<p>Filter out entire particles with any frames showing large jumps in micrometers.</p>"},{"location":"api/#SPTnano.helper_scripts.filter_large_jumps--parameters","title":"Parameters","text":"<p>df : DataFrame     DataFrame containing tracking data with a 'segment_len_um' column. threshold : float     Threshold for what constitutes a large jump in micrometers.</p>"},{"location":"api/#SPTnano.helper_scripts.filter_large_jumps--returns","title":"Returns","text":"<p>DataFrame     DataFrame with particles having large jumps filtered out.</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def filter_large_jumps(df, threshold):\n    \"\"\"\n    Filter out entire particles with any frames showing large jumps in micrometers.\n\n    Parameters\n    ----------\n    df : DataFrame\n        DataFrame containing tracking data with a 'segment_len_um' column.\n    threshold : float\n        Threshold for what constitutes a large jump in micrometers.\n\n    Returns\n    -------\n    DataFrame\n        DataFrame with particles having large jumps filtered out.\n    \"\"\"\n    # Identify unique_ids with any large jumps\n    large_jump_particles = df[df['segment_len_um'] &gt; threshold]['unique_id'].unique()\n\n    # Filter out particles with large jumps\n    df_filtered = df[~df['unique_id'].isin(large_jump_particles)].copy()\n    # df_filtered.drop(columns=['x_um_prev', 'y_um_prev', 'segment_len_um'], inplace=True)\n    return df_filtered\n</code></pre>"},{"location":"api/#SPTnano.helper_scripts.filter_stubs","title":"<code>filter_stubs(df, min_time)</code>","text":"<p>Removes tracks that are shorter than 'min_time' by finding the max duration of each time_s_zeroed column and filtering on that Works across exposure times, because it works on converted seconds, not frames</p> Source code in <code>src/SPTnano/helper_scripts.py</code> <pre><code>def filter_stubs(df, min_time):\n\n    '''\n    Removes tracks that are shorter than 'min_time' by finding the max duration of each time_s_zeroed column and filtering on that\n    Works across exposure times, because it works on converted seconds, not frames\n\n    '''\n    # Calculate the duration of each track by grouping by 'particle' and using the 'time_s' column\n    track_durations = df.groupby('unique_id')['time_s_zeroed'].max() \n    # Identify particles with tracks longer than 0.2 seconds\n    valid_particles = track_durations[track_durations &gt;= min_time].index\n    # Filter the dataframe to include only valid particles\n    filtered_df = df[df['unique_id'].isin(valid_particles)]\n\n    return filtered_df\n</code></pre>"},{"location":"api/#SPTnano.visualization","title":"<code>visualization</code>","text":""},{"location":"api/#SPTnano.visualization.batch_plot_trajectories","title":"<code>batch_plot_trajectories(master_folder, traj_df, batch=True, filename=None, colorby='particle', mpp=None, label=False, cmap=None)</code>","text":"<p>Batch plot trajectories for all replicates across several conditions.</p>"},{"location":"api/#SPTnano.visualization.batch_plot_trajectories--parameters","title":"Parameters","text":"<p>master_folder : str     Path to the master folder containing 'data' and 'saved_data' folders. traj_df : DataFrame     The DataFrame containing trajectory data. batch : bool, optional     If True, plots trajectories for all replicates in batch mode.     If False, plots trajectory for the specified filename. filename : str, optional     Filename of interest when batch is False. colorby : str, optional     Color by 'particle' or 'frame'. mpp : float, optional     Microns per pixel. label : bool, optional     Set to True to write particle ID numbers next to trajectories. cmap : colormap, optional     Colormap to use for coloring tracks.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def batch_plot_trajectories(master_folder, traj_df, batch=True, filename=None, colorby='particle', mpp=None, label=False, cmap=None):\n    \"\"\"\n    Batch plot trajectories for all replicates across several conditions.\n\n    Parameters\n    ----------\n    master_folder : str\n        Path to the master folder containing 'data' and 'saved_data' folders.\n    traj_df : DataFrame\n        The DataFrame containing trajectory data.\n    batch : bool, optional\n        If True, plots trajectories for all replicates in batch mode.\n        If False, plots trajectory for the specified filename.\n    filename : str, optional\n        Filename of interest when batch is False.\n    colorby : str, optional\n        Color by 'particle' or 'frame'.\n    mpp : float, optional\n        Microns per pixel.\n    label : bool, optional\n        Set to True to write particle ID numbers next to trajectories.\n    cmap : colormap, optional\n        Colormap to use for coloring tracks.\n    \"\"\"\n    data_folder = os.path.join(master_folder, 'data')\n    vis_folder = os.path.join(master_folder, 'visualization')\n    os.makedirs(vis_folder, exist_ok=True)\n\n    if batch:\n        for condition in os.listdir(data_folder):\n            condition_folder = os.path.join(data_folder, condition)\n            if os.path.isdir(condition_folder):\n                for file in os.listdir(condition_folder):\n                    if file.endswith('.tif'):\n                        filepath = os.path.join(condition_folder, file)\n                        subset_traj_df = traj_df[traj_df['filename'] == file]\n                        if not subset_traj_df.empty:\n                            frames = pims.open(filepath)\n                            frame = frames[0]\n                            fig, ax = plt.subplots()\n                            plot_trajectory(subset_traj_df, colorby=colorby, mpp=mpp, label=label, superimpose=frame, cmap=cmap, ax=ax)\n                            plt.savefig(os.path.join(vis_folder, f'{condition}_{file}.png'))\n                            plt.close(fig)\n    else:\n        if filename is not None:\n            filepath = os.path.join(data_folder, filename)\n            subset_traj_df = traj_df[traj_df['filename'] == filename]\n            if not subset_traj_df.empty:\n                frames = pims.open(filepath)\n                frame = frames[0]\n                fig, ax = plt.subplots()\n                plot_trajectory(subset_traj_df, colorby=colorby, mpp=mpp, label=label, superimpose=frame, cmap=cmap, ax=ax)\n                plt.show()\n        else:\n            print(\"Please provide a filename when batch is set to False.\")\n</code></pre>"},{"location":"api/#SPTnano.visualization.plot_barplots","title":"<code>plot_barplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', meanormedian='mean', talk=False)</code>","text":"<p>Plot bar plots of a specified factor, with bootstrapped confidence intervals.</p>"},{"location":"api/#SPTnano.visualization.plot_barplots--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing the data. factor_col : str, optional     The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'. separate_by : str, optional     Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'. palette : str, optional     Color palette for the plot. Default is 'colorblind'. meanormedian : str, optional     Whether to use mean or median for aggregation. Default is 'mean'. talk : bool, optional     Whether to set the figure size to the original large size or a smaller size. Default is False.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_barplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', meanormedian='mean', talk=False):\n    \"\"\"\n    Plot bar plots of a specified factor, with bootstrapped confidence intervals.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing the data.\n    factor_col : str, optional\n        The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'.\n    separate_by : str, optional\n        Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'.\n    palette : str, optional\n        Color palette for the plot. Default is 'colorblind'.\n    meanormedian : str, optional\n        Whether to use mean or median for aggregation. Default is 'mean'.\n    talk : bool, optional\n        Whether to set the figure size to the original large size or a smaller size. Default is False.\n    \"\"\"\n\n    unique_categories = data_df[separate_by].unique() if separate_by else [None]\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    # Set figure size based on the `talk` parameter\n    if talk:\n        fig_size = (20, 12)\n        font_size = 35\n    else:\n        fig_size = (5, 3)\n        font_size = 14\n\n    fig, ax = plt.subplots(figsize=fig_size)\n    sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5, \"font.size\": font_size, \"axes.titlesize\": font_size, \"axes.labelsize\": font_size, \"xtick.labelsize\": font_size, \"ytick.labelsize\": font_size})\n\n    avg_factors_list = []\n    ci_intervals = []\n\n    for i, category in enumerate(unique_categories):\n        subset = data_df if category is None else data_df[data_df[separate_by] == category]\n\n        if meanormedian == 'mean':\n            avg_factors = subset[factor_col].mean()\n            ci_interval = bootstrap_ci_mean(subset[factor_col], num_samples=1000, alpha=0.05)\n        else:\n            avg_factors = subset[factor_col].median()\n            ci_interval = bootstrap_ci_median(subset[factor_col], num_samples=1000, alpha=0.05)\n\n        avg_factors_list.append(avg_factors)\n        ci_intervals.append(ci_interval)\n\n    categories = unique_categories if separate_by else ['Overall']\n    ax.bar(categories, avg_factors_list, yerr=ci_intervals, color=color_palette, capsize=5, edgecolor='black')\n\n    # Remove 'Condition_' prefix from x tick labels\n    new_labels = [label.replace('Condition_', '') for label in categories]\n    if talk:\n        ax.set_xticklabels(new_labels, fontsize=font_size)\n    else:\n        ax.set_xticklabels(new_labels, fontsize=font_size, rotation=90)\n\n\n    ax.set_ylabel(factor_col, fontsize=font_size)\n    ax.tick_params(axis='both', which='major', labelsize=font_size)\n    plt.tight_layout()\n\n    plt.show()\n</code></pre>"},{"location":"api/#SPTnano.visualization.plot_histograms","title":"<code>plot_histograms(data_df, feature, bins=100, separate=None, xlimit=None, small_multiples=False, palette='colorblind', use_kde=False)</code>","text":"<p>Plot histograms of a specified feature for each category in coltoseparate, with consistent binning.</p>"},{"location":"api/#SPTnano.visualization.plot_histograms--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing track data with the specified feature and optionally a separating column. feature : str     The feature to plot histograms for. bins : int, optional     Number of bins for the histogram. Default is 100. separate : str, optional     Column to separate the data by. If None, all data will be plotted together. Default is None. xlimit : float, optional     Upper limit for the x-axis. Default is None. small_multiples : bool, optional     Whether to plot each category separately as small multiples. Default is False. use_kde : bool, optional     Whether to use KDE plot instead of histogram. Default is False.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_histograms(data_df, feature, bins=100, separate=None, xlimit=None, small_multiples=False, palette='colorblind', use_kde=False):\n    \"\"\"\n    Plot histograms of a specified feature for each category in coltoseparate, with consistent binning.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing track data with the specified feature and optionally a separating column.\n    feature : str\n        The feature to plot histograms for.\n    bins : int, optional\n        Number of bins for the histogram. Default is 100.\n    separate : str, optional\n        Column to separate the data by. If None, all data will be plotted together. Default is None.\n    xlimit : float, optional\n        Upper limit for the x-axis. Default is None.\n    small_multiples : bool, optional\n        Whether to plot each category separately as small multiples. Default is False.\n    use_kde : bool, optional\n        Whether to use KDE plot instead of histogram. Default is False.\n    \"\"\"\n    unique_categories = data_df[separate].unique() if separate else [None]\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    if small_multiples and separate is not None:\n        num_categories = len(unique_categories)\n        fig, axes = plt.subplots(num_categories, 1, figsize=(20, 6 * num_categories), sharex=True)\n\n        if num_categories == 1:\n            axes = [axes]  # To handle the case with only one subplot\n\n        for i, category in enumerate(unique_categories):\n            subset = data_df[data_df[separate] == category]\n            subsetvalues = subset[feature]\n\n            max_value = subsetvalues.max()\n            bin_edges = np.linspace(0, max_value, bins + 1)\n\n            # Plot histogram or KDE\n            if use_kde:\n                sns.kdeplot(subsetvalues, fill=True, ax=axes[i], color=color_palette[i])\n            else:\n                sns.histplot(subsetvalues, bins=bin_edges, kde=True, ax=axes[i], stat=\"percent\", color=color_palette[i])\n\n            axes[i].set_title(f'{category}', fontsize=14)\n\n            mean_value = subsetvalues.mean()\n            median_value = subsetvalues.median()\n            number_of_tracks = len(subset['unique_id'].unique())\n            axes[i].text(0.4, 0.6, f\"Mean: {mean_value:.2f}, Median: {median_value:.2f}, Tracks: {number_of_tracks}\", transform=axes[i].transAxes, fontsize=10)\n\n            if xlimit is not None:\n                axes[i].set_xlim(0, xlimit)\n            else:\n                axes[i].set_xlim(0, max_value)\n\n        plt.xlabel(f'{feature}', fontsize=12)\n        plt.tight_layout()\n        plt.show()\n\n    else:\n        plt.figure(figsize=(20, 12))\n        size = 10\n        multiplier = 2\n        sns.set_context(\"notebook\", rc={\"xtick.labelsize\": size * multiplier, \"ytick.labelsize\": size * multiplier})\n\n        max_value = data_df[feature].max()\n        bin_edges = np.linspace(0, max_value, bins + 1)\n\n        if separate is None:\n            subsetvalues = data_df[feature]\n\n            # Plot histogram or KDE\n            if use_kde:\n                sns.kdeplot(subsetvalues, fill=True, alpha=0.5, color=color_palette[0])\n            else:\n                sns.histplot(subsetvalues, bins=bin_edges, kde=True, alpha=0.5, stat=\"percent\", color=color_palette[0])\n\n            mean_value = subsetvalues.mean()\n            median_value = subsetvalues.median()\n            plt.text(0.4, 0.6, f\"Overall: mean: {mean_value:.2f}, median: {median_value:.2f}\", transform=plt.gca().transAxes, fontsize=10 * multiplier)\n\n        else:\n            for i, category in enumerate(unique_categories):\n                subset = data_df[data_df[separate] == category]\n                subsetvalues = subset[feature]\n\n                # Plot histogram or KDE\n                if use_kde:\n                    sns.kdeplot(subsetvalues, fill=True, label=category, alpha=0.5, color=color_palette[i])\n                else:\n                    sns.histplot(subsetvalues, bins=bin_edges, kde=True, label=category, alpha=0.5, stat=\"percent\", color=color_palette[i])\n\n                mean_value = subsetvalues.mean()\n                median_value = subsetvalues.median()\n                number_of_tracks = len(subset['unique_id'].unique())\n                shift = i * 0.05\n                plt.text(0.4, 0.6 - shift, f\"{category}: mean: {mean_value:.2f} from {number_of_tracks} tracks\", transform=plt.gca().transAxes, fontsize=10 * multiplier)\n\n        plt.xlabel(f'{feature}', fontsize=size * multiplier)\n        plt.ylabel('Percentage', fontsize=size * multiplier)\n        plt.legend(title='', fontsize=size * multiplier)\n        ax = plt.gca()\n        if xlimit is not None:\n            ax.set_xlim(0, xlimit)\n        else:\n            ax.set_xlim(0, max_value)\n        plt.show()\n</code></pre>"},{"location":"api/#SPTnano.visualization.plot_histograms_seconds","title":"<code>plot_histograms_seconds(traj_df, bins=100, coltoseparate='tracker', xlimit=None)</code>","text":"<p>Plot histograms of track lengths in seconds for each tracker, with consistent binning.</p>"},{"location":"api/#SPTnano.visualization.plot_histograms_seconds--parameters","title":"Parameters","text":"<p>traj_df : DataFrame     DataFrame containing track data with columns 'tracker', 'unique_id', 'time_s_zeroed', and 'filename'. bins : int, optional     Number of bins for the histogram. Default is 100. coltoseparate : str, optional     Column to separate the data by. Default is 'tracker'. xlimit : float, optional     Upper limit for the x-axis. Default is None.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_histograms_seconds(traj_df, bins=100, coltoseparate='tracker', xlimit=None):\n    \"\"\"\n    Plot histograms of track lengths in seconds for each tracker, with consistent binning.\n\n    Parameters\n    ----------\n    traj_df : DataFrame\n        DataFrame containing track data with columns 'tracker', 'unique_id', 'time_s_zeroed', and 'filename'.\n    bins : int, optional\n        Number of bins for the histogram. Default is 100.\n    coltoseparate : str, optional\n        Column to separate the data by. Default is 'tracker'.\n    xlimit : float, optional\n        Upper limit for the x-axis. Default is None.\n    \"\"\"\n    plt.figure(figsize=(20, 12))\n    size = 10\n    multiplier = 2\n    sns.set_context(\"notebook\", rc={\"xtick.labelsize\": size*multiplier, \"ytick.labelsize\": size*multiplier})\n\n    max_track_length = traj_df.groupby('unique_id')['time_s_zeroed'].max().max()\n    bin_edges = np.linspace(0, max_track_length, bins + 1)\n\n    for i, tracker in enumerate(traj_df[coltoseparate].unique()):\n        subset = traj_df[traj_df[coltoseparate] == tracker]\n        subsetvalues = subset.groupby('unique_id')['time_s_zeroed'].max()\n\n        # Calculate percentage counts\n        counts, _ = np.histogram(subsetvalues, bins=bin_edges)\n        percentage_counts = (counts / counts.sum()) * 100\n\n        # Plot histogram\n        sns.histplot(subsetvalues, bins=bin_edges, kde=True, label=tracker, alpha=0.5, stat=\"percent\")\n\n        subset_mean = subsetvalues.mean()\n        subset_median = subsetvalues.median()\n        subset_number_of_tracks = len(subset['unique_id'].unique())\n        shift = i * 0.05\n        plt.text(0.4, 0.6 - shift, f\"{tracker}: mean: {subset_mean:.2f} seconds from {subset_number_of_tracks} tracks\", transform=plt.gca().transAxes, fontsize=10 * multiplier)\n\n    plt.xlabel('Track length (seconds)', fontsize=size * multiplier)\n    plt.ylabel('Percentage', fontsize=size * multiplier)\n    plt.legend(title='', fontsize=size * multiplier)\n    ax = plt.gca()\n    if xlimit is not None:\n        ax.set_xlim(0, xlimit)\n    else:\n        ax.set_xlim(0, max_track_length)\n    plt.show()\n</code></pre>"},{"location":"api/#SPTnano.visualization.plot_time_series","title":"<code>plot_time_series(data_df, factor_col='speed_um_s', absolute=True, separate_by='condition', palette='colorblind', meanormedian='mean', multiplot=False, talk=False)</code>","text":"<p>Plot time series of a specified factor, with mean as a thick line and confidence intervals as shaded areas.</p>"},{"location":"api/#SPTnano.visualization.plot_time_series--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing the time series data. factor_col : str, optional     The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'. absolute : bool, optional     Whether to use absolute time values or time zeroed values. Default is True. separate_by : str, optional     Column to separate the data by, for coloring. If None, all data will be plotted together. Default is None. palette : str, optional     Color palette for the plot. Default is 'colorblind'. meanormedian : str, optional     Whether to use mean or median for aggregation. Default is 'mean'. multiplot : bool, optional     Whether to generate separate small multiple plots for each category. Default is False. talk : bool, optional     Whether to set the figure size to the original large size or a smaller size. Default is False.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_time_series(data_df, factor_col='speed_um_s', absolute=True, separate_by='condition', palette='colorblind', meanormedian='mean', multiplot=False, talk=False):\n    \"\"\"\n    Plot time series of a specified factor, with mean as a thick line and confidence intervals as shaded areas.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing the time series data.\n    factor_col : str, optional\n        The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'.\n    absolute : bool, optional\n        Whether to use absolute time values or time zeroed values. Default is True.\n    separate_by : str, optional\n        Column to separate the data by, for coloring. If None, all data will be plotted together. Default is None.\n    palette : str, optional\n        Color palette for the plot. Default is 'colorblind'.\n    meanormedian : str, optional\n        Whether to use mean or median for aggregation. Default is 'mean'.\n    multiplot : bool, optional\n        Whether to generate separate small multiple plots for each category. Default is False.\n    talk : bool, optional\n        Whether to set the figure size to the original large size or a smaller size. Default is False.\n    \"\"\"\n\n    if not absolute:\n        time_col = 'time_s_zeroed'\n        x_label = 'Time zeroed (s)'\n    else:\n        time_col = 'time_s'\n        x_label = 'Time (s)'\n\n    unique_categories = data_df[separate_by].unique() if separate_by else [None]\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    # Set figure size and font size based on the `talk` parameter\n    if talk:\n        fig_size = (40, 12)\n        font_size = 35\n    else:\n        if multiplot and separate_by:\n            fig_size = (10, 5 * len(unique_categories))\n        else:\n            fig_size = (5, 3)\n        font_size = 14\n\n    sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5, \"font.size\": font_size, \"axes.titlesize\": font_size, \"axes.labelsize\": font_size, \"xtick.labelsize\": font_size, \"ytick.labelsize\": font_size})\n\n    if multiplot and separate_by:\n        fig, axes = plt.subplots(len(unique_categories), 1, figsize=fig_size, sharex=True)\n\n        for i, category in enumerate(unique_categories):\n            ax = axes[i] if len(unique_categories) &gt; 1 else axes\n            subset = data_df[data_df[separate_by] == category]\n            times = subset[time_col]\n            factors = subset[factor_col]\n\n            if meanormedian == 'mean':\n                avg_factors = subset.groupby(time_col)[factor_col].mean()\n                ci = subset.groupby(time_col)[factor_col].apply(lambda x: bootstrap_ci_mean(x, num_samples=1000, alpha=0.05))\n            else:\n                avg_factors = subset.groupby(time_col)[factor_col].median()\n                ci = subset.groupby(time_col)[factor_col].apply(lambda x: bootstrap_ci_median(x, num_samples=1000, alpha=0.05))\n\n            color = color_palette[i]\n            label = category\n\n            ax.plot(avg_factors.index, avg_factors.values, label=label, color=color, linewidth=0.5)\n            ax.fill_between(avg_factors.index, avg_factors - ci, avg_factors + ci, color=color, alpha=0.3)\n            ax.set_xlabel(x_label, fontsize=font_size)\n            ax.set_ylabel(factor_col, fontsize=font_size, labelpad=20)\n            ax.legend(title=separate_by, fontsize=font_size, loc='upper left', bbox_to_anchor=(1, 1))\n            ax.set_title(f'Time Series of {factor_col} - {category}', fontsize=font_size)\n\n        plt.tight_layout()\n    else:\n        fig, ax = plt.subplots(figsize=fig_size)\n\n        for i, category in enumerate(unique_categories):\n            subset = data_df if category is None else data_df[data_df[separate_by] == category]\n            times = subset[time_col]\n            factors = subset[factor_col]\n\n            if meanormedian == 'mean':\n                avg_factors = subset.groupby(time_col)[factor_col].mean()\n                ci = subset.groupby(time_col)[factor_col].apply(lambda x: bootstrap_ci_mean(x, num_samples=1000, alpha=0.05))\n            else:\n                avg_factors = subset.groupby(time_col)[factor_col].median()\n                ci = subset.groupby(time_col)[factor_col].apply(lambda x: bootstrap_ci_median(x, num_samples=1000, alpha=0.05))\n\n            color = color_palette[i]\n            label = 'Overall' if category is None else category\n\n            ax.plot(avg_factors.index, avg_factors.values, label=label, color=color, linewidth=0.5)\n            ax.fill_between(avg_factors.index, avg_factors - ci, avg_factors + ci, color=color, alpha=0.3)\n\n        ax.set_xlabel(x_label, fontsize=font_size)\n        ax.set_ylabel(factor_col, fontsize=font_size, labelpad=20)\n        ax.legend(title=separate_by, fontsize=font_size, loc='upper left', bbox_to_anchor=(1, 1))\n        ax.set_title(f'Time Series of {factor_col}', fontsize=font_size)\n        plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout to fit legend\n\n    plt.show()\n</code></pre>"},{"location":"api/#SPTnano.visualization.plot_trajectory","title":"<code>plot_trajectory(traj, colorby='particle', mpp=None, label=False, superimpose=None, cmap=None, ax=None, t_column=None, pos_columns=None, plot_style={}, **kwargs)</code>","text":"<p>Plot traces of trajectories for each particle. Optionally superimpose it on a frame from the video.</p>"},{"location":"api/#SPTnano.visualization.plot_trajectory--parameters","title":"Parameters","text":"<p>traj : DataFrame     The DataFrame should include time and spatial coordinate columns. colorby : {'particle', 'frame'}, optional mpp : float, optional     Microns per pixel. If omitted, the labels will have units of pixels. label : boolean, optional     Set to True to write particle ID numbers next to trajectories. superimpose : ndarray, optional     Background image, default None cmap : colormap, optional     This is only used in colorby='frame' mode. Default = mpl.cm.winter ax : matplotlib axes object, optional     Defaults to current axes t_column : string, optional     DataFrame column name for time coordinate. Default is 'frame'. pos_columns : list of strings, optional     Dataframe column names for spatial coordinates. Default is ['x', 'y']. plot_style : dictionary     Keyword arguments passed through to the <code>Axes.plot(...)</code> command</p>"},{"location":"api/#SPTnano.visualization.plot_trajectory--returns","title":"Returns","text":"<p>Axes object</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_trajectory(traj, colorby='particle', mpp=None, label=False,\n                    superimpose=None, cmap=None, ax=None, t_column=None,\n                    pos_columns=None, plot_style={}, **kwargs):\n    \"\"\"\n    Plot traces of trajectories for each particle.\n    Optionally superimpose it on a frame from the video.\n\n    Parameters\n    ----------\n    traj : DataFrame\n        The DataFrame should include time and spatial coordinate columns.\n    colorby : {'particle', 'frame'}, optional\n    mpp : float, optional\n        Microns per pixel. If omitted, the labels will have units of pixels.\n    label : boolean, optional\n        Set to True to write particle ID numbers next to trajectories.\n    superimpose : ndarray, optional\n        Background image, default None\n    cmap : colormap, optional\n        This is only used in colorby='frame' mode. Default = mpl.cm.winter\n    ax : matplotlib axes object, optional\n        Defaults to current axes\n    t_column : string, optional\n        DataFrame column name for time coordinate. Default is 'frame'.\n    pos_columns : list of strings, optional\n        Dataframe column names for spatial coordinates. Default is ['x', 'y'].\n    plot_style : dictionary\n        Keyword arguments passed through to the `Axes.plot(...)` command\n\n    Returns\n    -------\n    Axes object\n    \"\"\"\n    if cmap is None:\n        cmap = plt.cm.winter\n    if t_column is None:\n        t_column = 'frame'\n    if pos_columns is None:\n        pos_columns = ['x', 'y']\n    if len(traj) == 0:\n        raise ValueError(\"DataFrame of trajectories is empty.\")\n\n    _plot_style = dict(linewidth=1)\n    _plot_style.update(**plot_style)\n\n    if ax is None:\n        ax = plt.gca()\n\n    # Axes labels\n    if mpp is None:\n        ax.set_xlabel(f'{pos_columns[0]} [px]')\n        ax.set_ylabel(f'{pos_columns[1]} [px]')\n        mpp = 1.  # for computations of image extent below\n    else:\n        ax.set_xlabel(f'{pos_columns[0]} [\u03bcm]')\n        ax.set_ylabel(f'{pos_columns[1]} [\u03bcm]')\n\n    # Background image\n    if superimpose is not None:\n        ax.imshow(superimpose, cmap=plt.cm.gray,\n                  origin='lower', interpolation='nearest',\n                  vmin=kwargs.get('vmin'), vmax=kwargs.get('vmax'))\n        ax.set_xlim(-0.5 * mpp, (superimpose.shape[1] - 0.5) * mpp)\n        ax.set_ylim(-0.5 * mpp, (superimpose.shape[0] - 0.5) * mpp)\n\n    # Trajectories\n    if colorby == 'particle':\n        # Unstack particles into columns.\n        unstacked = traj.set_index(['particle', t_column])[pos_columns].unstack()\n        for i, trajectory in unstacked.iterrows():\n            ax.plot(mpp * trajectory[pos_columns[0]], mpp * trajectory[pos_columns[1]], **_plot_style)\n    elif colorby == 'frame':\n        # Read http://www.scipy.org/Cookbook/Matplotlib/MulticoloredLine\n        x = traj.set_index([t_column, 'particle'])[pos_columns[0]].unstack()\n        y = traj.set_index([t_column, 'particle'])[pos_columns[1]].unstack()\n        color_numbers = traj[t_column].values / float(traj[t_column].max())\n        for particle in x:\n            points = np.array([x[particle].values, y[particle].values]).T.reshape(-1, 1, 2)\n            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n            lc = LineCollection(segments, cmap=cmap)\n            lc.set_array(color_numbers)\n            ax.add_collection(lc)\n            ax.set_xlim(x.apply(np.min).min(), x.apply(np.max).max())\n            ax.set_ylim(y.apply(np.min).min(), y.apply(np.max).max())\n\n    if label:\n        unstacked = traj.set_index([t_column, 'particle'])[pos_columns].unstack()\n        first_frame = int(traj[t_column].min())\n        coords = unstacked.fillna(method='backfill').stack().loc[first_frame]\n        for particle_id, coord in coords.iterrows():\n            ax.text(*coord.tolist(), s=\"%d\" % particle_id,\n                    horizontalalignment='center',\n                    verticalalignment='center')\n\n    ax.invert_yaxis()\n    return ax\n</code></pre>"},{"location":"api/#SPTnano.visualization.plot_violinplots","title":"<code>plot_violinplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', talk=False)</code>","text":"<p>Plot violin plots of a specified factor, with data separated by categories.</p>"},{"location":"api/#SPTnano.visualization.plot_violinplots--parameters","title":"Parameters","text":"<p>data_df : DataFrame     DataFrame containing the data. factor_col : str, optional     The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'. separate_by : str, optional     Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'. palette : str, optional     Color palette for the plot. Default is 'colorblind'. talk : bool, optional     Whether to set the figure size to the original large size or a smaller size. Default is False.</p> Source code in <code>src/SPTnano/visualization.py</code> <pre><code>def plot_violinplots(data_df, factor_col='speed_um_s', separate_by='condition', palette='colorblind', talk=False):\n    \"\"\"\n    Plot violin plots of a specified factor, with data separated by categories.\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        DataFrame containing the data.\n    factor_col : str, optional\n        The column representing the factor to be plotted on the y-axis. Default is 'speed_um_s'.\n    separate_by : str, optional\n        Column to separate the data by, for coloring. If None, all data will be plotted together. Default is 'condition'.\n    palette : str, optional\n        Color palette for the plot. Default is 'colorblind'.\n    talk : bool, optional\n        Whether to set the figure size to the original large size or a smaller size. Default is False.\n    \"\"\"\n\n    unique_categories = data_df[separate_by].unique() if separate_by else [None]\n    color_palette = sns.color_palette(palette, len(unique_categories))\n\n    # Set figure size based on the `talk` parameter\n    if talk:\n        fig_size = (20, 12)\n        font_size = 35\n    else:\n        fig_size = (5, 3)\n        font_size = 14\n\n    fig, ax = plt.subplots(figsize=fig_size)\n    sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5, \"font.size\": font_size, \"axes.titlesize\": font_size, \"axes.labelsize\": font_size, \"xtick.labelsize\": font_size, \"ytick.labelsize\": font_size})\n\n    # Plot violin plot\n    sns.violinplot(x=separate_by, y=factor_col, hue=separate_by, data=data_df, palette=color_palette, ax=ax, legend=False, alpha=0.79)\n\n    # Remove 'Condition_' prefix from x tick labels\n    new_labels = [label.replace('Condition_', '') for label in unique_categories]\n    ax.set_xticks(range(len(new_labels)))\n    ax.set_xticklabels(new_labels, fontsize=font_size)\n\n    ax.set_ylabel(factor_col, fontsize=font_size, labelpad=20)\n    ax.set_xlabel(None)\n    ax.tick_params(axis='both', which='major', labelsize=font_size)\n    plt.tight_layout()\n\n    plt.show()\n</code></pre>"}]}